<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Api和SQL | zxj</title><meta name="keywords" content="Flink"><meta name="author" content="Xiangjie"><meta name="copyright" content="Xiangjie"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="整体介绍前言 Flink有三层API，越顶层越抽象（方便），但不灵活 什么是Table API和Flink SQLFlink 本身是批流统一的处理框架，所以 Table API 和 SQL，就是批流统一的上层处理 API。 目前功能尚未完善，处于活跃的开发阶段。  Table API 是一套内嵌在 Java 和 Scala 语言中的查询 API，它允许我们以非常直观的方式， 组合来自一些关系运算符">
<meta property="og:type" content="article">
<meta property="og:title" content="Api和SQL">
<meta property="og:url" content="https://awslzhang.top/2021/01/10/Api%E5%92%8CSQL/index.html">
<meta property="og:site_name" content="zxj">
<meta property="og:description" content="整体介绍前言 Flink有三层API，越顶层越抽象（方便），但不灵活 什么是Table API和Flink SQLFlink 本身是批流统一的处理框架，所以 Table API 和 SQL，就是批流统一的上层处理 API。 目前功能尚未完善，处于活跃的开发阶段。  Table API 是一套内嵌在 Java 和 Scala 语言中的查询 API，它允许我们以非常直观的方式， 组合来自一些关系运算符">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://flink.apache.org/img/flink-header-logo.svg">
<meta property="article:published_time" content="2021-01-10T03:51:40.000Z">
<meta property="article:modified_time" content="2021-02-02T13:36:45.848Z">
<meta property="article:author" content="Xiangjie">
<meta property="article:tag" content="Flink">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://flink.apache.org/img/flink-header-logo.svg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://awslzhang.top/2021/01/10/Api%E5%92%8CSQL/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: Xiangjie","link":"链接: ","source":"来源: zxj","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}

// https://stackoverflow.com/questions/16839698/jquery-getscript-alternative-in-native-javascript
const getScript = url => new Promise((resolve, reject) => {
  const script = document.createElement('script')
  script.src = url
  script.async = true
  script.onerror = reject
  script.onload = script.onreadystatechange = function() {
    const loadState = this.readyState
    if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
    script.onload = script.onreadystatechange = null
    resolve()
  }
  document.head.appendChild(script)
})</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2021-02-02 21:36:45'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
   if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }
  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified
    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }const asideStatus = saveToLocal.get('aside-status')
if (asideStatus !== undefined) {
   if (asideStatus === 'hide') {
     document.documentElement.classList.add('hide-aside')
   } else {
     document.documentElement.classList.remove('hide-aside')
   }
}})()</script><meta name="generator" content="Hexo 5.0.2"><link rel="alternate" href="/atom.xml" title="zxj" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/null" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">85</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">43</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">25</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 目录</span></a></div></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://flink.apache.org/img/flink-header-logo.svg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">zxj</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 目录</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Api和SQL</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-01-10T03:51:40.000Z" title="发表于 2021-01-10 11:51:40">2021-01-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-02-02T13:36:45.848Z" title="更新于 2021-02-02 21:36:45">2021-02-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Flink/">Flink</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">13.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>53分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="整体介绍"><a href="#整体介绍" class="headerlink" title="整体介绍"></a>整体介绍</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20210110115741903.png" alt="image-20210110115741903"></p>
<p>Flink有三层API，越顶层越抽象（方便），但不灵活</p>
<h2 id="什么是Table-API和Flink-SQL"><a href="#什么是Table-API和Flink-SQL" class="headerlink" title="什么是Table API和Flink SQL"></a>什么是Table API和Flink SQL</h2><p>Flink 本身是批流统一的处理框架，所以 Table API 和 SQL，就是<strong>批流统一的上层处理 API</strong>。 目前功能尚未完善，处于活跃的开发阶段。 </p>
<p>Table API 是一套内嵌在 Java 和 Scala 语言中的查询 API，它允许我们以非常直观的方式， 组合来自一些关系运算符的查询（比如 select、filter 和 join）。而对于 Flink SQL，就是直接可 以在代码中写 SQL，来实现一些查询（Query）操作。Flink 的 SQL 支持，基于实现了 SQL 标 准的 Apache Calcite（Apache 开源 SQL 解析工具）。</p>
<p> 无论输入是批输入还是流式输入，在这两套 API 中，指定的查询都具有相同的语义，得 到相同的结果。</p>
<blockquote>
<p>Flink SQL在1.10版本才稳定下来，1.11版本</p>
</blockquote>
<h2 id="需要引入的依赖"><a href="#需要引入的依赖" class="headerlink" title="需要引入的依赖"></a>需要引入的依赖</h2><p>Table API和SQL需要引入的依赖有两个：planner和bridge。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>flink-table-planner：planner计划器，是table API最主要的部分，提供了运行时环境和生成程序执行计划的planner；</li>
<li>flink-table-api-scala-bridge：bridge桥接器，主要负责table API和 DataStream/DataSet API的连接支持，按照语言分java和scala。</li>
</ul>
<blockquote>
<p>planner里包含bridge包。</p>
<p>而Flink有两种版本的planner</p>
</blockquote>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20210110130012436.png" alt="image-20210110130012436"></p>
<h2 id="不同版本的Planner🔺"><a href="#不同版本的Planner🔺" class="headerlink" title="不同版本的Planner🔺"></a>不同版本的Planner🔺</h2><p>而Flink有两种版本的planner：</p>
<ol>
<li>planner</li>
<li>planner-blink</li>
</ol>
<blockquote>
<p>planner和planner-blink的架构都不相同，planner-blink真正的实现了流批统一。</p>
</blockquote>
<p>必须选择一种使用，两者不兼容。</p>
<ol>
<li>批流统一：<strong>Blink将批处理作业，视为流式处理的特殊情况。</strong>所以，blink不支持表和DataSet之间的转换，批处理作业将不转换为DataSet应用程序，而是跟流处理一样，转换为DataStream程序来处理。</li>
<li>因为批流统一，Blink planner也不支持BatchTableSource，而使用有界的StreamTableSource代替。</li>
<li>Blink planner只支持全新的目录，不支持已弃用的ExternalCatalog。</li>
<li>旧planner和Blink planner的FilterableTableSource实现不兼容。旧的planner会把PlannerExpressions下推到filterableTableSource中，而blink planner则会把Expressions下推。</li>
<li>基于字符串的键值配置选项仅适用于Blink planner。</li>
<li>PlannerConfig在两个planner中的实现不同</li>
<li>Blink planner会将多个sink优化在一个DAG中（仅在TableEnvironment上受支持，而在StreamTableEnvironment上不受支持）。而旧planner的优化总是将每一个sink放在一个新的DAG中，其中所有DAG彼此独立。</li>
<li>旧的planner不支持目录统计，而Blink planner支持。</li>
</ol>
<h1 id="API调用"><a href="#API调用" class="headerlink" title="API调用"></a>API调用</h1><h2 id="表环境创建🔺"><a href="#表环境创建🔺" class="headerlink" title="表环境创建🔺"></a>表环境创建🔺</h2><p>通过下面使用不同的planner创建表环境救恩那个理解1，2点的不同。（真正实现流批统一）</p>
<h3 id="默认🔺"><a href="#默认🔺" class="headerlink" title="默认🔺"></a>默认🔺</h3><p>1.10默认planner，1.11以及以后默认blink。</p>
<p>创建表环境最简单的方式，就是基于流处理执行环境，调create方法直接创建：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"><span class="keyword">val</span> tableEnv: <span class="type">StreamTableEnvironment</span> = <span class="type">StreamTableEnvironment</span>.create(env)</span><br></pre></td></tr></table></figure>

<p>此时创建出来的是流处理的环境</p>
<blockquote>
<p>我使用的版本是1.12.0（flink）</p>
</blockquote>
<h3 id="planner"><a href="#planner" class="headerlink" title="planner"></a>planner</h3><p>主要基于<code>def create(executionEnvironment : org.apache.flink.streaming.api.scala.StreamExecutionEnvironment, settings : org.apache.flink.table.api.EnvironmentSettings) : org.apache.flink.table.api.bridge.scala.StreamTableEnvironment</code>方法</p>
<h4 id="批处理环境"><a href="#批处理环境" class="headerlink" title="批处理环境"></a>批处理环境</h4><p>基于老版本的批处理，因为老版本不是流批统一，所以创建批处理环境不得使用<code>StreamTableEnvironment</code>。而且它的参数也不能是流处理的环境<code>StreamExecutionEnvironment</code>，得是批处理的环境：<code>ExecutionEnvironment</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TableEnvCreate</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> batchEnv: <span class="type">ExecutionEnvironment</span> = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> batchTableEnv: <span class="type">BatchTableEnvironment</span> = <span class="type">BatchTableEnvironment</span>.create(batchEnv)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="流处理环境"><a href="#流处理环境" class="headerlink" title="流处理环境"></a>流处理环境</h4><p>基于老版本的流处理</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TableEnvCreate</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    </span><br><span class="line">   </span><br><span class="line">    <span class="keyword">val</span> settings: <span class="type">EnvironmentSettings</span> = <span class="type">EnvironmentSettings</span>.newInstance()</span><br><span class="line">      .useOldPlanner()</span><br><span class="line">      .inStreamingMode()</span><br><span class="line">      .build()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> environment: <span class="type">StreamTableEnvironment</span> = <span class="type">StreamTableEnvironment</span>.create(env, settings)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="blink🔺"><a href="#blink🔺" class="headerlink" title="blink🔺"></a>blink🔺</h3><h4 id="批处理环境-1"><a href="#批处理环境-1" class="headerlink" title="批处理环境"></a>批处理环境</h4><p>因为Flink的blink planner是流批统一的，所以它不需要使用<code>BatchTableEnvironment</code>来创建环境，它只需要通过统一的环境<code>TableEnvironment</code>来创建，只需要将参数声明为流处理还是批处理即可</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TableEnvCreate</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> settings: <span class="type">EnvironmentSettings</span> = <span class="type">EnvironmentSettings</span>.newInstance()</span><br><span class="line">      .useBlinkPlanner()</span><br><span class="line">      .inBatchMode()</span><br><span class="line">      .build()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> environment: <span class="type">TableEnvironment</span> = <span class="type">TableEnvironment</span>.create(settings)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="流处理环境-1"><a href="#流处理环境-1" class="headerlink" title="流处理环境"></a>流处理环境</h4><p>基于新版本的流处理</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TableEnvCreate</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> settings: <span class="type">EnvironmentSettings</span> = <span class="type">EnvironmentSettings</span>.newInstance()</span><br><span class="line">      .useBlinkPlanner()</span><br><span class="line">      .inStreamingMode()</span><br><span class="line">      .build()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> environment: <span class="type">StreamTableEnvironment</span> = <span class="type">StreamTableEnvironment</span>.create(env, settings)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="基本程序结构"><a href="#基本程序结构" class="headerlink" title="基本程序结构"></a>基本程序结构</h2><p>Table API 和 SQL 的程序结构，与流式处理的程序结构类似；也可以近似地认为有这么几步：首先创建执行环境，然后定义source、transform和sink。</p>
<p>具体操作流程如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> tableEnv = ...  <span class="comment">// 创建表的执行环境</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建一张表，用于读取数据</span></span><br><span class="line">tableEnv.connect(...).createTemporaryTable(<span class="string">&quot;inputTable&quot;</span>)</span><br><span class="line"><span class="comment">// 注册一张表，用于把计算结果输出</span></span><br><span class="line">tableEnv.connect(...).createTemporaryTable(<span class="string">&quot;outputTable&quot;</span>)</span><br><span class="line"><span class="comment">// 通过 Table API 查询算子，得到一张结果表</span></span><br><span class="line"><span class="keyword">val</span> result = tableEnv.from(<span class="string">&quot;inputTable&quot;</span>).select(...)</span><br><span class="line"><span class="comment">// 通过 SQL查询语句，得到一张结果表</span></span><br><span class="line"><span class="keyword">val</span> sqlResult  = tableEnv.sqlQuery(<span class="string">&quot;SELECT ... FROM inputTable ...&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将结果表写入输出表中</span></span><br><span class="line">result.insertInto(<span class="string">&quot;outputTable&quot;</span>)</span><br></pre></td></tr></table></figure>

<ol>
<li>创建表执行环境</li>
<li>创建一张表，用于读取数据</li>
<li>注册一张表，用于把计算结果输出</li>
<li>通过 Table API 查询算子，得到一张结果表 || 通过 SQL查询语句，得到一张结果表</li>
<li>将结果表写入输出表中</li>
</ol>
<blockquote>
<p>如果使用Table API的话，必须得通过env创建一个Table类型的对象，通过其才能执行API操作</p>
<p>如果使用SQL对象的话，必须通过env创建<code>createTemporaryTable</code>临时表到环境中，才能使用SQL开始计算</p>
</blockquote>
<h2 id="在-Catalog-中注册表"><a href="#在-Catalog-中注册表" class="headerlink" title="在 Catalog 中注册表"></a>在 Catalog 中注册表</h2><h3 id="表（Table）的概念"><a href="#表（Table）的概念" class="headerlink" title="表（Table）的概念"></a>表（Table）的概念</h3><p>TableEnvironment 可以注册目录 Catalog，并可以基于 Catalog 注册表。它会维护一个 Catalog-Table 表之间的 map。</p>
<p>表（Table）是由一个“标识符”来指定的，由 3 部分组成：<strong>Catalog 名、数据库（database） 名和对象名（表名）</strong>。如果没有指定目录或数据库，就使用当前的默认值。</p>
<p>表可以是常规的（Table，表），或者虚拟的（View，视图）。</p>
<ul>
<li>常规表（Table）一般可以 用来描述外部数据，比如文件、数据库表或消息队列的数据，也可以直接从 DataStream 转 换而来。（用于Source、Sink的表）</li>
<li>视图可以从现有的表中创建，通常是 table API 或者 SQL 查询的一个结果（<code>createTemporaryTable</code>）</li>
</ul>
<h3 id="创建表🔺"><a href="#创建表🔺" class="headerlink" title="创建表🔺"></a>创建表🔺</h3><h4 id="外部系统创建"><a href="#外部系统创建" class="headerlink" title="外部系统创建"></a>外部系统创建</h4><p>现在1.12方法<code>connect</code>好像已经被废弃，但是还能用。</p>
<p>源码中非常清晰的告诉我们应该使用CREATE TABLE DDL来构建SOURCE TABLE。所以，我们优先使用DDL方式来构建TABLE。而且，官方的各种API开始都是以DDL来构建了。</p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20210110142836238.png" alt="image-20210110142836238"></p>
<p>具体使用方法请查看官方：<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/filesystem.html">Apache Flink 1.12 Documentation: FileSystem SQL Connector</a></p>
<hr>
<p><del>直接调用 tableEnv.connect()就可以，里面参数要传 入一个 ConnectorDescriptor，也就是 connector 描述器。对于文件系统的 connector 而言，flink 内部已经提供了，就叫做 FileSystem()。并调用<code>createTemporaryTable</code>方法，在Catalog注册表</del></p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20210110141247564.png" alt="image-20210110141247564"></p>
<h5 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h5><p><strong>举例，读取本地文件系统CSV格式文件</strong></p>
<p>需要额外导入包</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-csv<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.example.table</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">Expressions</span>.$</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">Table</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala.<span class="type">StreamTableEnvironment</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FromFileSystemByDdl</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> tableEnv: <span class="type">StreamTableEnvironment</span> = <span class="type">StreamTableEnvironment</span>.create(env)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileSystemSql: <span class="type">String</span> = <span class="string">&quot;CREATE TABLE fs_table &quot;</span> +</span><br><span class="line">      <span class="string">&quot;(id STRING,&quot;</span> +</span><br><span class="line">      <span class="string">&quot; ts STRING,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;  temp DOUBLE) &quot;</span> +</span><br><span class="line">      <span class="string">&quot; WITH (&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;connector&#x27;=&#x27;filesystem&#x27;,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;path&#x27;=&#x27;D:\\Projects\\Flink-Table\\src\\main\\resources\\sensor.txt&#x27;,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;format&#x27;=&#x27;csv&#x27;)&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//    可以使用 TableEnvironment 中的 executeSql() 方法执行 CREATE 语句。 若 CREATE 操作执行成功，executeSql() 方法返回 ‘OK’，否则会抛出异常。</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//    以下的例子展示了如何在 TableEnvironment 中执行一个 CREATE 语句。</span></span><br><span class="line">    tableEnv.executeSql(fileSystemSql)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Table对象没有方法toAppendStream的， 只有通过导入隐式转换的包才可以实现此功能</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.table.api.bridge.scala.tableConversions</span><br><span class="line">    <span class="comment">// SQL形式</span></span><br><span class="line">    tableEnv.sqlQuery(<span class="string">&quot;select * from fs_table&quot;</span>).toAppendStream[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">Double</span>)].print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Table API形式</span></span><br><span class="line">    <span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.from(<span class="string">&quot;fs_table&quot;</span>)</span><br><span class="line">    table.select($(<span class="string">&quot;id&quot;</span>), $(<span class="string">&quot;ts&quot;</span>), $(<span class="string">&quot;temp&quot;</span>)).toAppendStream[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">Double</span>)].print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>需要注意的点</strong></p>
<ol>
<li>使用 TableEnvironment 中的 executeSql() 方法执行 CREATE 语句。 若 CREATE 操作执行成功，executeSql() 方法返回 ‘OK’，否则会抛出异常。</li>
<li>在计算Table输出时，<strong>Table没有打印到控制台的方法，必须转换为流才能输出</strong></li>
<li>Table对象没有方法toAppendStream的， 只有通过导入隐式转换的包才可以实现此功能</li>
<li>Table API已经不知道参数为String的方法，需要通过<code>($(&quot;id&quot;)</code>形式使用</li>
</ol>
</blockquote>
<h4 id="DataStream转换"><a href="#DataStream转换" class="headerlink" title="DataStream转换"></a>DataStream转换</h4><p>一般都通过DATa Stream转换而来：</p>
<p>包不要到导错</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">DataStream</span>, <span class="type">StreamExecutionEnvironment</span>, createTypeInformation&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">Expressions</span>.$</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">Table</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FromDataStreamByFileSystem</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> tableEnv: <span class="type">StreamTableEnvironment</span> = <span class="type">StreamTableEnvironment</span>.create(env)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取流，本地文件</span></span><br><span class="line">    <span class="keyword">val</span> inputStream: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>)] = env.readTextFile(<span class="string">&quot;D:\\Projects\\Flink-Table\\src\\main\\resources\\sensor.txt&quot;</span>, <span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">      .map(r =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> strings: <span class="type">Array</span>[<span class="type">String</span>] = r.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">        (strings(<span class="number">0</span>), strings(<span class="number">1</span>), strings(<span class="number">2</span>))</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 流转换为表</span></span><br><span class="line">    <span class="keyword">val</span> inputTable: <span class="type">Table</span> = tableEnv.fromDataStream(inputStream, $(<span class="string">&quot;id&quot;</span>), $(<span class="string">&quot;ts&quot;</span>), $(<span class="string">&quot;temp&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// sql,必须在环境中创建了表之后才能用</span></span><br><span class="line">    tableEnv.createTemporaryView(<span class="string">&quot;myTable&quot;</span>, inputTable)</span><br><span class="line">    tableEnv.sqlQuery(<span class="string">&quot;select * from myTable&quot;</span>)</span><br><span class="line">        .toAppendStream[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>)].print(<span class="string">&quot;sql&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// table api</span></span><br><span class="line">    inputTable.select($(<span class="string">&quot;id&quot;</span>), $(<span class="string">&quot;ts&quot;</span>), $(<span class="string">&quot;temp&quot;</span>)).toAppendStream[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>)].print(<span class="string">&quot;tableApi&quot;</span>)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><em>数据类型和Schema的对应关系</em></p>
<p>基于位置：</p>
<p><code> tableEnv.fromDataStream(inputStream, $(&quot;id&quot;), $(&quot;ts&quot;), $(&quot;temp&quot;))</code></p>
<h3 id="创建临时视图"><a href="#创建临时视图" class="headerlink" title="创建临时视图"></a>创建临时视图</h3><p><strong>基于DataStream</strong></p>
<p><code>def createTemporaryView[T](path : scala.Predef.String, dataStream : org.apache.flink.streaming.api.scala.DataStream[T]) : scala.Unit</code></p>
<p><strong>基于表</strong></p>
<p><code>void createTemporaryView(String path, Table view);</code></p>
<h3 id="输出表🔺"><a href="#输出表🔺" class="headerlink" title="输出表🔺"></a>输出表🔺</h3><p>对于绑定外部系统的两个表进行一下操作即可输出：</p>
<p><code>inputTable.executeInsert(&quot;table_other&quot;)</code></p>
<p>但是建议以下面的流程输出：</p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20210110161619800.png" alt="image-20210110161619800"></p>
<h1 id="更新模式🔺"><a href="#更新模式🔺" class="headerlink" title="更新模式🔺"></a>更新模式🔺</h1><p>在流处理过程中，表的处理并不像传统定义的那样简单。由上面的描述可知Table对象并没有输出到控制台的方法，它只能通过转换为流才可以输出到控制台。</p>
<p>对于流式查询（Streaming Queries），需要声明如何在（动态）表和外部连接器之间执行转换。与外部系统交换的消息类型，由更新模式（update mode）指定。</p>
<p><strong>Flink Table API中的更新模式有以下三种：</strong></p>
<ol>
<li>追加模式（Append Mode）：在追加模式下，表（动态表）和外部连接器只交换插入（Insert）消息。即，table的一条信息发送后就不会更变</li>
<li>撤回模式（Retract Mode）：在撤回模式下，表和外部连接器交换的是：添加（Add）和撤回（Retract）消息。即：一般聚合操作时（未开窗时）是一条记录来了就聚合一次，当再次来一个已经聚合过的数据时就会对以前的数据进行更新，而它是通过撤回之前的结果和插入最新的结果实现最新的聚合结果的。</li>
<li>Upsert（更新插入）模式：在Upsert模式下，动态表和外部连接器交换Upsert和Delete消息。</li>
</ol>
<p><em>Upsert模式</em></p>
<p>这个模式需要一个唯一的key，通过这个key可以传递更新消息。为了正确应用消息，外部连接器需要知道这个唯一key的属性</p>
<p>即：一般聚合操作时（未开窗时）是一条记录来了就聚合一次，当再次来一个已经聚合过的数据时就会对以前的数据进行更新，而它是通过key唯一确定是更新还是插入的；还能删除（Delete）编码为Delete信息。</p>
<hr>
<p>各个模式支持的外部系统：</p>
<ol>
<li>追加模式（Append Mode）：控制台、文件、DB、Redis、ElasticSearch</li>
<li>撤回模式（Retract Mode）：控制台、DB、Redis、ElasticSearch</li>
<li>Upsert（更新插入）模式：控制台、DB、Redis、ElasticSearch</li>
</ol>
<h2 id="将表转为DataStream"><a href="#将表转为DataStream" class="headerlink" title="将表转为DataStream"></a>将表转为DataStream</h2><p>此时就需要根据Table的操作来选择不同的更新模式</p>
<p>Table API中表到DataStream有两种模式：</p>
<ol>
<li>追加模式（Append Mode）</li>
<li>撤回模式（Retract Mode）<ol>
<li>得到的数据会增加一个Boolean类型的标识位（返回的第一个字段），用它来表示到底是新增的数据（Insert），还是被删除的数据（老数据，Delete）。</li>
</ol>
</li>
</ol>
<blockquote>
<p>在什么情况下需要使用撤回模式（Retract Mode）</p>
<p>一般聚合操作时（未开窗时）是一条记录来了就聚合一次，当再次来一个已经聚合过的数据时就会对以前的数据进行更新</p>
<p>所以撤回模式（Retract Mode）是通过撤回之前的结果和插入最新的结果实现最新的聚合结果的。</p>
<p>而此时追加模式（Append Mode）只能添加不能更改，所以不适合没有开窗的聚合情况的</p>
<p>但是为什么要制定没有开窗？</p>
<p><font color="red">因为没有开窗是来一条聚合一条的，而开窗是来一条聚合一条，但是开创结束后才开始写入流，所以写入的流的数据也不会变，所以这时候支持追加模式（Append Mode）</font></p>
</blockquote>
<p>例子证明：</p>
<ol>
<li>没有聚合操作的使用追加模式（Append Mode）转为<code>toAppendStream</code></li>
<li>有聚合操作的使用追加模式（Append Mode）转为<code>toAppendStream</code>，错误</li>
<li>有聚合操作的使用撤回模式（Retract Mode）转为<code>toRetractStream</code>，正确</li>
</ol>
<p><strong>1.</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FromDataStreamByFileSystem</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> tableEnv: <span class="type">StreamTableEnvironment</span> = <span class="type">StreamTableEnvironment</span>.create(env)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取流，本地文件</span></span><br><span class="line">    <span class="keyword">val</span> inputStream: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>)] = env.readTextFile(<span class="string">&quot;D:\\Projects\\Flink-Table\\src\\main\\resources\\sensor.txt&quot;</span>, <span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">      .map(r =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> strings: <span class="type">Array</span>[<span class="type">String</span>] = r.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">        (strings(<span class="number">0</span>), strings(<span class="number">1</span>), strings(<span class="number">2</span>))</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 流转换为表</span></span><br><span class="line">    <span class="keyword">val</span> inputTable: <span class="type">Table</span> = tableEnv.fromDataStream(inputStream, $(<span class="string">&quot;id&quot;</span>), $(<span class="string">&quot;ts&quot;</span>), $(<span class="string">&quot;temp&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// sql,必须在环境中创建了表之后才能用</span></span><br><span class="line">    tableEnv.createTemporaryView(<span class="string">&quot;myTable&quot;</span>, inputTable)</span><br><span class="line">    tableEnv.sqlQuery(<span class="string">&quot;select * from myTable&quot;</span>)</span><br><span class="line">        .toAppendStream[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>)].print(<span class="string">&quot;sql&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// table api</span></span><br><span class="line">    inputTable.select($(<span class="string">&quot;id&quot;</span>), $(<span class="string">&quot;ts&quot;</span>), $(<span class="string">&quot;temp&quot;</span>)).toAppendStream[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>)].print(<span class="string">&quot;tableApi&quot;</span>)</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>2.</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CountExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> tableEnv: <span class="type">StreamTableEnvironment</span> = <span class="type">StreamTableEnvironment</span>.create(env)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileSystemSql: <span class="type">String</span> = <span class="string">&quot;CREATE TABLE fs_table &quot;</span> +</span><br><span class="line">      <span class="string">&quot;(id STRING,&quot;</span> +</span><br><span class="line">      <span class="string">&quot; ts STRING,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;  temp DOUBLE) &quot;</span> +</span><br><span class="line">      <span class="string">&quot; WITH (&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;connector&#x27;=&#x27;filesystem&#x27;,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;path&#x27;=&#x27;D:\\Projects\\Flink-Table\\src\\main\\resources\\sensor.txt&#x27;,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;format&#x27;=&#x27;csv&#x27;)&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//    可以使用 TableEnvironment 中的 executeSql() 方法执行 CREATE 语句。 若 CREATE 操作执行成功，executeSql() 方法返回 ‘OK’，否则会抛出异常。</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">//    以下的例子展示了如何在 TableEnvironment 中执行一个 CREATE 语句。</span></span><br><span class="line">    tableEnv.executeSql(fileSystemSql)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Table对象没有方法toAppendStream的， 只有通过导入隐式转换的包才可以实现此功能</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.table.api.bridge.scala.tableConversions</span><br><span class="line">    <span class="comment">// SQL形式</span></span><br><span class="line">    tableEnv.sqlQuery(<span class="string">&quot;select id,avg(temp) from fs_table group by id&quot;</span>).toAppendStream[(<span class="type">String</span>,</span><br><span class="line">      <span class="type">Double</span>)]</span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20210110163624481.png" alt="image-20210110163624481"></p>
<blockquote>
<p>意思是，<code>toAppendStream</code>不支持update，所以得使用<code>toRetractStream</code></p>
</blockquote>
<p><strong>3.</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CountExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> tableEnv: <span class="type">StreamTableEnvironment</span> = <span class="type">StreamTableEnvironment</span>.create(env)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileSystemSql: <span class="type">String</span> = <span class="string">&quot;CREATE TABLE fs_table &quot;</span> +</span><br><span class="line">      <span class="string">&quot;(id STRING,&quot;</span> +</span><br><span class="line">      <span class="string">&quot; ts STRING,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;  temp DOUBLE) &quot;</span> +</span><br><span class="line">      <span class="string">&quot; WITH (&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;connector&#x27;=&#x27;filesystem&#x27;,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;path&#x27;=&#x27;D:\\Projects\\Flink-Table\\src\\main\\resources\\sensor.txt&#x27;,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;format&#x27;=&#x27;csv&#x27;)&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//    可以使用 TableEnvironment 中的 executeSql() 方法执行 CREATE 语句。 若 CREATE 操作执行成功，executeSql() 方法返回 ‘OK’，否则会抛出异常。</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">//    以下的例子展示了如何在 TableEnvironment 中执行一个 CREATE 语句。</span></span><br><span class="line">    tableEnv.executeSql(fileSystemSql)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Table对象没有方法toAppendStream的， 只有通过导入隐式转换的包才可以实现此功能</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.table.api.bridge.scala.tableConversions</span><br><span class="line">    <span class="comment">// SQL形式</span></span><br><span class="line">    tableEnv.sqlQuery(<span class="string">&quot;select id,avg(temp) from fs_table group by id&quot;</span>).toRetractStream[(<span class="type">String</span>,</span><br><span class="line">      <span class="type">Double</span>)]</span><br><span class="line">      .print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20210110163733258.png" alt="image-20210110163733258"></p>
<p>true和false代表是本分组的聚合是第一次来还是第n次来</p>
<ol>
<li>第一个红框代表id为sensor1的数据第一个聚合</li>
<li>第二个红框代表id为sensor1的数据第二次聚合前撤回之前的结果</li>
<li>第二个红框下面的第一个数据代表id为sensor1的数据第二次聚合后的更新结果</li>
</ol>
<h1 id="流处理中的特殊概念🔺"><a href="#流处理中的特殊概念🔺" class="headerlink" title="流处理中的特殊概念🔺"></a>流处理中的特殊概念🔺</h1><p>Table API和SQL，本质上还是基于关系型表的操作方式；而关系型表、关系代数，以及SQL本身，一般是有界的，更适合批处理的场景。这就导致在进行流处理的过程中，理解会稍微复杂一些，需要引入一些特殊概念。</p>
<h2 id="流处理和关系代数（表，及-SQL）的区别"><a href="#流处理和关系代数（表，及-SQL）的区别" class="headerlink" title="流处理和关系代数（表，及 SQL）的区别"></a>流处理和关系代数（表，及 SQL）的区别</h2><p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20210110164925069.png" alt="image-20210110164925069"></p>
<p>可以看到，其实关系代数（主要就是指关系型数据库中的表）和 SQL，主要就是针对批 处理的，这和流处理有天生的隔阂。</p>
<h2 id="动态表（Dynamic-Tables）"><a href="#动态表（Dynamic-Tables）" class="headerlink" title="动态表（Dynamic Tables）"></a>动态表（Dynamic Tables）</h2><p>因为流处理面对的数据，是连续不断的，这和我们熟悉的关系型数据库中保存的“表” 完全不同。所以，如果我们把流数据转换成 Table，然后执行类似于 table 的 select 操作，结 果就不是一成不变的，而是随着新数据的到来，会不停更新。 </p>
<p>我们可以随着新数据的到来，不停地在之前的基础上更新结果。这样得到的表，在 Flink Table API 概念里，就叫做<strong>“动态表”（Dynamic Tables）</strong>。</p>
<p>动态表是 Flink 对流数据的 Table API 和 SQL 支持的核心概念。与表示批处理数据的静态 表不同，动态表是随时间变化的。动态表可以像静态的批处理表一样进行查询，查询一个动 态表会产生持续查询（Continuous Query）。连续查询永远不会终止，并会生成另一个动态表。 查询（Query）会不断更新其动态结果表，以反映其动态输入表上的更改。</p>
<h2 id="流式持续查询的过程"><a href="#流式持续查询的过程" class="headerlink" title="流式持续查询的过程"></a>流式持续查询的过程</h2><p>下图显示了流、动态表和连续查询的关系：</p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20210110165032290.png" alt="image-20210110165032290"></p>
<p>流式持续查询的过程为： </p>
<ol>
<li>流被转换为动态表。</li>
<li>对动态表计算连续查询，生成新的动态表。</li>
<li>生成的动态表被转换回流</li>
</ol>
<h3 id="将流转换成表（Table）"><a href="#将流转换成表（Table）" class="headerlink" title="将流转换成表（Table）"></a>将流转换成表（Table）</h3><p>为了处理带有关系查询的流，必须先将其转换为表。 </p>
<p>从概念上讲，流的每个数据记录，都被解释为对结果表的插入（Insert）修改。因为流 式持续不断的，而且之前的输出结果无法改变。本质上，我们其实是从一个、只有插入操作 的 changelog（更新日志）流，来构建一个表。 </p>
<p>为了更好地说明动态表和持续查询的概念，我们来举一个具体的例子。 比如，我们现在的输入数据，就是用户在网站上的访问行为，数据类型（Schema）如 下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line"> user: VARCHAR, <span class="comment">// 用户名</span></span><br><span class="line"> cTime: TIMESTAMP, <span class="comment">// 访问某个 URL 的时间戳</span></span><br><span class="line"> url: VARCHAR <span class="comment">// 用户访问的 URL</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>下图显示了如何将访问 URL 事件流，或者叫点击事件流（左侧）转换为表（右侧）。</p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20210110165202118.png" alt="image-20210110165202118"></p>
<p>随着插入更多的访问事件流记录，生成的表将不断增长。</p>
<h3 id="持续查询（Continuous-Query）"><a href="#持续查询（Continuous-Query）" class="headerlink" title="持续查询（Continuous Query）"></a>持续查询（Continuous Query）</h3><p>持续查询，会在动态表上做计算处理，并作为结果生成新的动态表。与批处理查询不同， 连续查询从不终止，并根据输入表上的更新更新其结果表。 </p>
<p>在任何时间点，连续查询的结果在语义上，等同于在输入表的快照上，以批处理模式执 行的同一查询的结果。</p>
<p> 在下面的示例中，我们展示了对点击事件流中的一个持续查询。</p>
<p> 这个 Query 很简单，是一个分组聚合做 count 统计的查询。它将用户字段上的 clicks 表 分组，并统计访问的 url 数。图中显示了随着时间的推移，当 clicks 表被其他行更新时如何 计算查询。</p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20210110165244001.png" alt="image-20210110165244001"></p>
<blockquote>
<p>输入的表和计算后的表都是动态表</p>
</blockquote>
<h3 id="将动态表转换成流"><a href="#将动态表转换成流" class="headerlink" title="将动态表转换成流"></a>将动态表转换成流</h3><p>与常规的数据库表一样，动态表可以通过插入（Insert）、更新（Update）和删除（Delete） 更改，进行持续的修改。将动态表转换为流或将其写入外部系统时，需要对这些更改进行编 码。</p>
<p>Flink 的 Table API 和 SQL 支持三种方式对动态表的更改进行编码：</p>
<p><strong>仅追加（Append-only）流</strong></p>
<p>仅通过插入（Insert）更改，来修改的动态表，可以直接转换为“仅追加”流。这个流 中发出的数据，就是动态表中新增的每一行。</p>
<p><strong>撤回（Retract）流</strong></p>
<p>Retract 流是包含两类消息的流，添加（Add）消息和撤回（Retract）消息。</p>
<p>动态表通过将 INSERT 编码为 add 消息、DELETE 编码为 retract 消息、UPDATE 编码为被 更改行（前一行）的 retract 消息和更新后行（新行）的 add 消息，转换为 retract 流。</p>
<p>下图显示了将动态表转换为 Retract 流的过程。</p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20210110165516501.png" alt="image-20210110165516501"></p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20210110165545721.png" alt="image-20210110165545721"></p>
<p><strong>Upsert（更新插入）流</strong></p>
<p>Upsert 流包含两种类型的消息：Upsert 消息和 delete 消息。转换为 upsert 流的动态表， 需要有唯一的键（key）。</p>
<p>通过将 INSERT 和 UPDATE 更改编码为 upsert 消息，将 DELETE 更改编码为 DELETE 消息， 就可以将具有唯一键（Unique Key）的动态表转换为流。</p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20210110165603473.png" alt="image-20210110165603473"></p>
<p>这些概念我们之前都已提到过。<font color="red">需要注意的是，在代码里将动态表转换为 DataStream 时，仅支持 Append 和 Retract 流。</font>而向外部系统输出动态表的 TableSink 接口，则可以有不 同的实现，<strong>比如之前我们讲到的 ES，就可以有 Upsert 模式</strong></p>
<h2 id="时间特性🔺"><a href="#时间特性🔺" class="headerlink" title="时间特性🔺"></a>时间特性🔺</h2><p>基于时间的操作（比如 Table API 和 SQL 中窗口操作），需要定义相关的时间语义和时间 数据来源的信息。所以，Table 可以提供一个逻辑上的时间字段，用于在表处理程序中，指 示时间和访问相应的时间戳。 </p>
<p>时间属性，可以是每个表 schema 的一部分。一旦定义了时间属性，它就可以作为一个 字段引用，并且可以在基于时间的操作中使用。</p>
<p> 时间属性的行为类似于常规时间戳，可以访问，并且进行计算。</p>
<h3 id="处理时间（Processing-Time）"><a href="#处理时间（Processing-Time）" class="headerlink" title="处理时间（Processing Time）"></a>处理时间（Processing Time）</h3><p>处理时间语义下，允许表处理程序根据机器的本地时间生成结果。它是时间的最简单概 念。它既不需要提取时间戳，也不需要生成 watermark。</p>
<p> 定义处理时间属性有三种方法：</p>
<ol>
<li>在 DataStream 转化时直接指定；</li>
<li><del>在定义 Table Schema 时指定；已经废弃connect方法</del></li>
<li>在创建表的 DDL 中指定</li>
</ol>
<h4 id="DataStream-转化成-Table-时指定"><a href="#DataStream-转化成-Table-时指定" class="headerlink" title="DataStream 转化成 Table 时指定"></a>DataStream 转化成 Table 时指定</h4><p>由 DataStream 转换成表时，可以在后面指定字段名来定义 Schema。在定义 Schema 期 间，可以用.proctime，定义处理时间字段。</p>
<p> 注意，这个 proctime 属性只能通过<strong>附加逻辑字段</strong>，来扩展物理 schema。因此，只能在 schema 定义的末尾定义它。</p>
<blockquote>
<p>即：proctime 属性不能使用已有的字段</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ProcessTimeForDataStream</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> tableEnv: <span class="type">StreamTableEnvironment</span> = <span class="type">StreamTableEnvironment</span>.create(env)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取流，本地文件</span></span><br><span class="line">    <span class="keyword">val</span> inputStream: <span class="type">DataStream</span>[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>)] = env.readTextFile(<span class="string">&quot;D:\\Projects\\Flink-Table\\src\\main\\resources\\sensor.txt&quot;</span>, <span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">      .map(r =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> strings: <span class="type">Array</span>[<span class="type">String</span>] = r.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">        (strings(<span class="number">0</span>), strings(<span class="number">1</span>), strings(<span class="number">2</span>))</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 流转换为表</span></span><br><span class="line">    <span class="keyword">val</span> inputTable: <span class="type">Table</span> = tableEnv.fromDataStream(inputStream, $(<span class="string">&quot;id&quot;</span>), $(<span class="string">&quot;ts&quot;</span>), $(<span class="string">&quot;temp&quot;</span>), $(<span class="string">&quot;pt&quot;</span>).proctime())</span><br><span class="line"></span><br><span class="line">    inputTable.toAppendStream[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>, <span class="type">Timestamp</span>)].print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="创建表的-DDL-中指定"><a href="#创建表的-DDL-中指定" class="headerlink" title="创建表的 DDL 中指定"></a>创建表的 DDL 中指定</h4><p>在创建表的 DDL 中，增加一个字段并指定成 proctime，也可以指定当前的时间字段。</p>
<p>代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ProcessTimeForDdl</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> tableEnv: <span class="type">StreamTableEnvironment</span> = <span class="type">StreamTableEnvironment</span>.create(env)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileSystemSql: <span class="type">String</span> = <span class="string">&quot;CREATE TABLE fs_table &quot;</span> +</span><br><span class="line">      <span class="string">&quot;(id STRING,&quot;</span> +</span><br><span class="line">      <span class="string">&quot; ts STRING,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;  temp DOUBLE, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; pt AS PROCTIME()) &quot;</span> +</span><br><span class="line">      <span class="string">&quot; WITH (&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;connector&#x27;=&#x27;filesystem&#x27;,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;path&#x27;=&#x27;D:\\Projects\\Flink-Table\\src\\main\\resources\\sensor.txt&#x27;,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;format&#x27;=&#x27;csv&#x27;)&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//    可以使用 TableEnvironment 中的 executeSql() 方法执行 CREATE 语句。 若 CREATE 操作执行成功，executeSql() 方法返回 ‘OK’，否则会抛出异常。</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">//    以下的例子展示了如何在 TableEnvironment 中执行一个 CREATE 语句。</span></span><br><span class="line">    tableEnv.executeSql(fileSystemSql)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Table对象没有方法toAppendStream的， 只有通过导入隐式转换的包才可以实现此功能</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.table.api.bridge.scala.tableConversions</span><br><span class="line">    <span class="comment">// SQL形式</span></span><br><span class="line">    tableEnv.sqlQuery(<span class="string">&quot;select * from fs_table&quot;</span>).toAppendStream[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">Double</span>, <span class="type">Timestamp</span>)]</span><br><span class="line">      .print()</span><br><span class="line">    </span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>直接在定义表字段的时候，多加一列即可：<code>pt AS PROCTIME()</code></p>
<blockquote>
<p>注意：运行这段 DDL，必须使用 Blink Planner。</p>
</blockquote>
<h3 id="事件时间（Event-Time）"><a href="#事件时间（Event-Time）" class="headerlink" title="事件时间（Event Time）"></a>事件时间（Event Time）</h3><p>事件时间语义，允许表处理程序根据每个记录中包含的时间生成结果。这样即使在有乱 序事件或者延迟事件时，也可以获得正确的结果。 </p>
<p>为了处理无序事件，并区分流中的准时和迟到事件；Flink 需要从事件数据中，提取时 间戳，并用来推进事件时间的进展（watermark）。</p>
<h4 id="DataStream-转化成-Table-时指定-1"><a href="#DataStream-转化成-Table-时指定-1" class="headerlink" title="DataStream 转化成 Table 时指定"></a>DataStream 转化成 Table 时指定</h4><p>在声明DataSteam时直接指定WaterMark和时间戳。然后在转换为Table时，添加新的字段或者覆盖已有字段</p>
<p>在DataStream转换成Table，schema的定义期间，使用.rowtime可以定义事件时间属性。 <strong>注意，必须在转换的数据流中分配时间戳和 watermark。</strong> </p>
<p>在将数据流转换为表时，有两种定义时间属性的方法。根据指定的<code>.rowtime </code>字段名是否 存在于数据流的架构中，timestamp 字段可以： </p>
<ul>
<li><p>作为新字段追加到 schema </p>
</li>
<li><p>替换现有字段</p>
</li>
</ul>
<p>在这两种情况下，定义的事件时间戳字段，都将保存 DataStream 中事件时间戳的值。</p>
<p>代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">EventTimeForDataStream</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">// 默认事件时间，不指定</span></span><br><span class="line">    println(env.getStreamTimeCharacteristic)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> tableEnv: <span class="type">StreamTableEnvironment</span> = <span class="type">StreamTableEnvironment</span>.create(env)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取流，本地文件</span></span><br><span class="line">    <span class="keyword">val</span> inputStream: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = env.addSource(<span class="keyword">new</span> <span class="type">SensorSource</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置水位线</span></span><br><span class="line"><span class="comment">//    val watermark: DataStream[SensorReading] = inputStream.assignAscendingTimestamps(_.timestamp)</span></span><br><span class="line">    <span class="keyword">val</span> value: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = inputStream.assignTimestampsAndWatermarks(<span class="type">WatermarkStrategy</span>.forMonotonousTimestamps()</span><br><span class="line">      .withTimestampAssigner(<span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[<span class="type">SensorReading</span>] &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(t: <span class="type">SensorReading</span>, l: <span class="type">Long</span>): <span class="type">Long</span> = t.timestamp</span><br><span class="line">      &#125;))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 流转换为表</span></span><br><span class="line">    <span class="keyword">val</span> inputTable: <span class="type">Table</span> = tableEnv</span><br><span class="line">      .fromDataStream(value,</span><br><span class="line">        $<span class="string">&quot;id&quot;</span>, $<span class="string">&quot;temperature&quot;</span>, $<span class="string">&quot;timestamp&quot;</span>, $<span class="string">&quot;rt&quot;</span>.rowtime())</span><br><span class="line"></span><br><span class="line">    inputTable.printSchema()</span><br><span class="line">    inputTable.toAppendStream[<span class="type">Row</span>].print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20210111201410740.png" alt="image-20210111201410740"></p>
<p>在我们更改了Flink1.12版本后，设置时间时间方法被废弃，原因是，Flink1.12版本默认是事件时间，如果想要金庸的话请使用<code>org.apache.flink.api.common.ExecutionConfig#setAutoWatermarkInterval(long</code></p>
<hr>
<p>同时发现设置水位线的方法也过期了，这是最新的说法。<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/event_timestamps_watermarks.html">Apache Flink 1.12 Documentation: 生成 Watermark</a></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> setting: <span class="type">WatermarkStrategy</span>[(<span class="type">String</span>, <span class="type">Long</span>, <span class="type">String</span>)] = <span class="type">WatermarkStrategy</span></span><br><span class="line">.forBoundedOutOfOrderness[(<span class="type">String</span>, <span class="type">Long</span>, <span class="type">String</span>)](<span class="type">Duration</span>.ofSeconds(<span class="number">20</span>))</span><br><span class="line">.withTimestampAssigner(<span class="keyword">new</span> <span class="type">SerializableTimestampAssigner</span>[(<span class="type">String</span>, <span class="type">Long</span>, <span class="type">String</span>)] &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">extractTimestamp</span></span>(element: (<span class="type">String</span>, <span class="type">Long</span>, <span class="type">String</span>), recordTimestamp: <span class="type">Long</span>): <span class="type">Long</span> = element._2 * <span class="number">1000</span></span><br><span class="line">&#125;)</span><br><span class="line">inputStream.assignTimestampsAndWatermarks(setting)</span><br></pre></td></tr></table></figure>

<p><strong>重要：</strong></p>
<p>如果编译时提示以下信息：<code>Static methods in interface require -target:jvm-1.8</code></p>
<p>则可以通过修改pom文件中的scala Compiler参数来修改此问题，加入<code>-target:jvm-1.8</code>：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Scala Compiler --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.alchim31.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">args</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">arg</span>&gt;</span>-nobootcp<span class="tag">&lt;/<span class="name">arg</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">arg</span>&gt;</span>-target:jvm-1.8<span class="tag">&lt;/<span class="name">arg</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">args</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>又或者在</p>
<p>In Intellij IDEA (15 CE) add this scalac compiler option:</p>
<p><em>Build, Execution, Deployment</em> -&gt; <em>Compiler</em> -&gt; <em>Scala Compiler</em> -&gt; <em>Your Project</em></p>
<p><em>Additional compiler options</em>: <code>-target:jvm-1.8</code> (was empty).</p>
<h4 id="定义-Table-Schema-时指定"><a href="#定义-Table-Schema-时指定" class="headerlink" title="定义 Table Schema 时指定"></a><del>定义 Table Schema 时指定</del></h4><p><del>这种方</del>法只要在connect后定义 Schema 的时候，将事件时间字段，并指定成 rowtime 就可以了。connect已废弃</p>
<h4 id="创建表的-DDL-中指定-1"><a href="#创建表的-DDL-中指定-1" class="headerlink" title="创建表的 DDL 中指定"></a>创建表的 DDL 中指定</h4><p>事件时间属性，是使用 CREATE TABLE DDL 中的 WARDMARK 语句定义的。watermark 语 句，定义现有事件时间字段上的 watermark 生成表达式，该表达式将事件时间字段标记为事 件时间属性。</p>
<p><a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sql/create.html#watermark">Apache Flink 1.12 Documentation: CREATE 语句</a></p>
<blockquote>
<p><code>WATERMARK</code> 定义了表的事件时间属性，其形式为 <code>WATERMARK FOR rowtime_column_name AS watermark_strategy_expression</code> 。</p>
<p><code>rowtime_column_name</code> 把一个现有的列定义为一个为表标记事件时间的属性。该列的类型必须为 <code>TIMESTAMP(3)</code>，且是 schema 中的顶层列，它也可以是一个计算列。</p>
<p><code>watermark_strategy_expression</code> 定义了 watermark 的生成策略。它允许使用包括计算列在内的任意非查询表达式来计算 watermark ；表达式的返回类型必须是 <code>TIMESTAMP(3)</code>，表示了从 Epoch 以来的经过的时间。 返回的 watermark 只有当其不为空且其值大于之前发出的本地 watermark 时才会被发出（以保证 watermark 递增）。每条记录的 watermark 生成表达式计算都会由框架完成。 框架会定期发出所生成的最大的 watermark ，如果当前 watermark 仍然与前一个 watermark 相同、为空、或返回的 watermark 的值小于最后一个发出的 watermark ，则新的 watermark 不会被发出。 Watermark 根据 <a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/deployment/config.html#pipeline-auto-watermark-interval"><code>pipeline.auto-watermark-interval</code></a> 中所配置的间隔发出。 若 watermark 的间隔是 <code>0ms</code> ，那么每条记录都会产生一个 watermark，且 watermark 会在不为空并大于上一个发出的 watermark 时发出。</p>
<p>使用事件时间语义时，表必须包含事件时间属性和 watermark 策略。</p>
<p>Flink 提供了几种常用的 watermark 策略。</p>
<ul>
<li><p>严格递增时间戳： <code>WATERMARK FOR rowtime_column AS rowtime_column</code>。</p>
<p>发出到目前为止已观察到的最大时间戳的 watermark ，时间戳大于最大时间戳的行被认为没有迟到。</p>
</li>
<li><p>递增时间戳： <code>WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL &#39;0.001&#39; SECOND</code>。</p>
<p>发出到目前为止已观察到的最大时间戳减 1 的 watermark ，时间戳大于或等于最大时间戳的行被认为没有迟到。</p>
</li>
<li><p>有界乱序时间戳： <code>WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL &#39;string&#39; timeUnit</code>。</p>
<p>发出到目前为止已观察到的最大时间戳减去指定延迟的 watermark ，例如， <code>WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL &#39;5&#39; SECOND</code> 是一个 5 秒延迟的 watermark 策略。</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> Orders (</span><br><span class="line">    <span class="string">`user`</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">    product <span class="keyword">STRING</span>,</span><br><span class="line">    order_time <span class="built_in">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">    WATERMARK <span class="keyword">FOR</span> order_time <span class="keyword">AS</span> order_time - <span class="built_in">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> ( . . . );</span><br></pre></td></tr></table></figure>

<p>注意点：被作为时间戳的字段需要是<code>TIMESTAMP</code>类型，且在声明WATERMARK时的字段也需要是<code>TIMESTAMP</code></p>
</blockquote>
<p>例如：读取本地文件信息时直接指定Watermark，代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.example.table</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.eventtime._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.example.source.&#123;<span class="type">SensorReading</span>, <span class="type">SensorSource</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">EventTimeForDdl</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">// 默认事件时间，不指定</span></span><br><span class="line">    println(env.getStreamTimeCharacteristic)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> tableEnv: <span class="type">StreamTableEnvironment</span> = <span class="type">StreamTableEnvironment</span>.create(env)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileSystemSql: <span class="type">String</span> = <span class="string">&quot;CREATE TABLE fs_table &quot;</span> +</span><br><span class="line">      <span class="string">&quot;(id STRING,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;ts BIGINT,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;  temp DOUBLE, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; order_time as  TO_TIMESTAMP( FROM_UNIXTIME(ts) ) ,&quot;</span> +</span><br><span class="line">      <span class="string">&quot; WATERMARK FOR order_time AS order_time - INTERVAL &#x27;5&#x27; SECOND) &quot;</span> +</span><br><span class="line">      <span class="string">&quot; WITH (&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;connector&#x27;=&#x27;filesystem&#x27;,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;path&#x27;=&#x27;D:\\Projects\\Flink-Table\\src\\main\\resources\\sensor.txt&#x27;,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;format&#x27;=&#x27;csv&#x27;)&quot;</span></span><br><span class="line"></span><br><span class="line">    tableEnv.executeSql(fileSystemSql)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.from(<span class="string">&quot;fs_table&quot;</span>)</span><br><span class="line">    table.printSchema()</span><br><span class="line">    table.toAppendStream[<span class="type">Row</span>].print()</span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20210112223123100.png" alt="image-20210112223123100"></p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sensor_1,1547718199,35.8</span><br><span class="line">sensor_6,1547718201,15.4</span><br><span class="line">sensor_7,1547718202,6.7</span><br><span class="line">sensor_10,1547718205,38.1</span><br><span class="line">sensor_1,1547718207,36.3</span><br><span class="line">sensor_1,1547718209,32.8</span><br><span class="line">sensor_1,1547718212,37.1</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意</p>
<p><font color="red">在声明Watermark时需要设置的是时间戳的类型，如果源数据不是时间戳类型，需要新增一个逻辑列，来将其他的时间列转换为时间戳类型，再给设置WaterMark</font></p>
<p>这里 FROM_UNIXTIME 是系统内置的时间函数，用来将一个整数（秒数）转换成 “YYYY-MM-DD hh:mm:ss”格式（默认，也可以作为第二个 String 参数传入）的日期时间 字符串（date time string）；然后再用 TO_TIMESTAMP 将其转换成 Timestamp。</p>
</blockquote>
<h1 id="窗口🔺"><a href="#窗口🔺" class="headerlink" title="窗口🔺"></a>窗口🔺</h1><p>时间语义，要配合窗口操作才能发挥作用。最主要的用途，当然就是开窗口、根据时间 段做计算了。</p>
<p>下面我们就来看看 Table API 和 SQL 中，怎么利用时间字段做窗口操作。 在 Table API 和 SQL 中，主要有两种窗口：</p>
<ol>
<li>Group Windows ：分组窗口</li>
<li>Over Windows：Over窗口，和我们在RDBMS说的开窗函数是一样的</li>
</ol>
<h2 id="分组窗口（Group-Windows）"><a href="#分组窗口（Group-Windows）" class="headerlink" title="分组窗口（Group Windows）"></a>分组窗口（Group Windows）</h2><p>分组窗口（Group Windows）会根据时间或行计数间隔，将行聚合到有限的组（Group） 中，并对每个组的数据执行一次聚合函数。</p>
<p> Table API 中的 Group Windows 都是使用.window（w:GroupWindow）子句定义的，并且 必须由 as 子句指定一个别名。为了按窗口对表进行分组，窗口的别名必须在 group by 子句 中，像常规的分组字段一样引用。</p>
<p>Table API 提供了一组具有特定语义的预定义 Window 类，这些类会被转换为底层 DataStream 或 DataSet 的窗口操作。 </p>
<p>Table API 支持的窗口定义，和我们熟悉的一样，主要也是三种：滚动（Tumbling）、滑 动（Sliding）和会话（Session）。</p>
<blockquote>
<p>注意的是，分组聚合后的Table可以转换为Append流，因为窗口计算完后才会输出，并不会更改已经关闭的窗口的数值</p>
</blockquote>
<h3 id="滚动窗口"><a href="#滚动窗口" class="headerlink" title="滚动窗口"></a>滚动窗口</h3><p><strong><em>Table API</em></strong></p>
<p>滚动窗口（Tumbling windows）要用 Tumble 类来定义，另外还有三个方法：</p>
<ul>
<li>over：定义窗口长度</li>
<li>on：用来分组（按时间间隔）或者排序（按行数）的时间字段</li>
<li>as：别名，必须出现在后面的 groupBy 中</li>
</ul>
<p>使用Table API进行分组时，与DataTream的操作有所不同，他需要先开窗形成一个逻辑字段窗口w，之后使用<code>group by</code>正式开窗杠，此时必须基于窗口逻辑w和其他需要分组的对象来分组。</p>
<p>例如以下代码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">Expressions</span>.lit</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.&#123;$, <span class="type">Table</span>, <span class="type">Tumble</span>, <span class="type">WithOperations</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TumbleWindowApi</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">// 默认事件时间，不指定</span></span><br><span class="line">    println(env.getStreamTimeCharacteristic)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> tableEnv: <span class="type">StreamTableEnvironment</span> = <span class="type">StreamTableEnvironment</span>.create(env)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileSystemSql: <span class="type">String</span> = <span class="string">&quot;CREATE TABLE fs_table &quot;</span> +</span><br><span class="line">      <span class="string">&quot;(id STRING,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;ts BIGINT,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;  temp DOUBLE, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; order_time as  TO_TIMESTAMP( FROM_UNIXTIME(ts) ) ,&quot;</span> +</span><br><span class="line">      <span class="string">&quot; WATERMARK FOR order_time AS order_time - INTERVAL &#x27;5&#x27; SECOND) &quot;</span> +</span><br><span class="line">      <span class="string">&quot; WITH (&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;connector&#x27;=&#x27;filesystem&#x27;,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;path&#x27;=&#x27;D:\\Projects\\Flink-Table\\src\\main\\resources\\sensor.txt&#x27;,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;format&#x27;=&#x27;csv&#x27;)&quot;</span></span><br><span class="line"></span><br><span class="line">    tableEnv.executeSql(fileSystemSql)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.from(<span class="string">&quot;fs_table&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> table1: <span class="type">Table</span> = table.window(<span class="type">Tumble</span>.over(lit(<span class="number">1</span>).hour()).on($(<span class="string">&quot;order_time&quot;</span>)).as($(<span class="string">&quot;w&quot;</span>)))</span><br><span class="line">      .groupBy($(<span class="string">&quot;w&quot;</span>), $(<span class="string">&quot;id&quot;</span>))</span><br><span class="line">      .select($(<span class="string">&quot;id&quot;</span>), $(<span class="string">&quot;temp&quot;</span>).count, $(<span class="string">&quot;w&quot;</span>).`end`())</span><br><span class="line"></span><br><span class="line">    table1.toAppendStream[<span class="type">Row</span>].print()</span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意的是，分组聚合后的Table可以转换为Append流，因为窗口计算完后才会输出，并不会更改已经关闭的窗口的数值.</p>
<hr>
<p><strong><em>SQL</em></strong></p>
<p><a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sql/queries.html#%E5%88%86%E7%BB%84%E7%AA%97%E5%8F%A3">Apache Flink 1.12 Documentation: 查询语句</a></p>
<p>SQL 查询的分组窗口是通过 <code>GROUP BY</code> 子句定义的。类似于使用常规 <code>GROUP BY</code> 语句的查询，窗口分组语句的 <code>GROUP BY</code> 子句中带有一个窗口函数为每个分组计算出一个结果。以下是批处理表和流处理表支持的分组窗口函数：</p>
<table>
<thead>
<tr>
<th align="left">分组窗口函数</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>TUMBLE(time_attr, interval)</code></td>
<td align="left">定义一个滚动窗口。滚动窗口把行分配到有固定持续时间（ <code>interval</code> ）的不重叠的连续窗口。比如，5 分钟的滚动窗口以 5 分钟为间隔对行进行分组。滚动窗口可以定义在事件时间（批处理、流处理）或处理时间（流处理）上。</td>
</tr>
<tr>
<td align="left"><code>HOP(time_attr, interval, interval)</code></td>
<td align="left">定义一个跳跃的时间窗口（在 Table API 中称为滑动窗口）。滑动窗口有一个固定的持续时间（ 第二个 <code>interval</code> 参数 ）以及一个滑动的间隔（第一个 <code>interval</code> 参数 ）。若滑动间隔小于窗口的持续时间，滑动窗口则会出现重叠；因此，行将会被分配到多个窗口中。比如，一个大小为 15 分组的滑动窗口，其滑动间隔为 5 分钟，将会把每一行数据分配到 3 个 15 分钟的窗口中。滑动窗口可以定义在事件时间（批处理、流处理）或处理时间（流处理）上。</td>
</tr>
<tr>
<td align="left"><code>SESSION(time_attr, interval)</code></td>
<td align="left">定义一个会话时间窗口。会话时间窗口没有一个固定的持续时间，但是它们的边界会根据 <code>interval</code> 所定义的不活跃时间所确定；即一个会话时间窗口在定义的间隔时间内没有时间出现，该窗口会被关闭。例如时间窗口的间隔时间是 30 分钟，当其不活跃的时间达到30分钟后，若观测到新的记录，则会启动一个新的会话时间窗口（否则该行数据会被添加到当前的窗口），且若在 30 分钟内没有观测到新纪录，这个窗口将会被关闭。会话时间窗口可以使用事件时间（批处理、流处理）或处理时间（流处理）。</td>
</tr>
</tbody></table>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">Table</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TumbleWindowSql</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">// 默认事件时间，不指定</span></span><br><span class="line">    println(env.getStreamTimeCharacteristic)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> tableEnv: <span class="type">StreamTableEnvironment</span> = <span class="type">StreamTableEnvironment</span>.create(env)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileSystemSql: <span class="type">String</span> = <span class="string">&quot;CREATE TABLE fs_table &quot;</span> +</span><br><span class="line">      <span class="string">&quot;(id STRING,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;ts BIGINT,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;  temp DOUBLE, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; row_time as  TO_TIMESTAMP( FROM_UNIXTIME(ts) ) ,&quot;</span> +</span><br><span class="line">      <span class="string">&quot; WATERMARK FOR row_time AS row_time - INTERVAL &#x27;5&#x27; SECOND) &quot;</span> +</span><br><span class="line">      <span class="string">&quot; WITH (&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;connector&#x27;=&#x27;filesystem&#x27;,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;path&#x27;=&#x27;D:\\Projects\\Flink-Table\\src\\main\\resources\\sensor.txt&#x27;,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;format&#x27;=&#x27;csv&#x27;)&quot;</span></span><br><span class="line"></span><br><span class="line">    tableEnv.executeSql(fileSystemSql)</span><br><span class="line"></span><br><span class="line">    tableEnv.from(<span class="string">&quot;fs_table&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> table1: <span class="type">Table</span> = tableEnv.sqlQuery(</span><br><span class="line">      <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">        |SELECT</span></span><br><span class="line"><span class="string">        |  id,</span></span><br><span class="line"><span class="string">        |  TUMBLE_START(row_time, INTERVAL &#x27;1&#x27; HOUR) as wStart,</span></span><br><span class="line"><span class="string">        |  SUM(temp)</span></span><br><span class="line"><span class="string">        | FROM fs_table</span></span><br><span class="line"><span class="string">        | GROUP BY TUMBLE(row_time, INTERVAL &#x27;1&#x27; HOUR), id</span></span><br><span class="line"><span class="string">    &quot;</span><span class="string">&quot;&quot;</span>.stripMargin)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    table1.toAppendStream[<span class="type">Row</span>].print()</span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h3><p><strong><em>Table API</em></strong></p>
<p>滑动窗口（Sliding windows）要用 Slide 类来定义，另外还有四个方法</p>
<ol>
<li>over：定义窗口长度</li>
<li>every：定义滑动步长</li>
<li> on：用来分组（按时间间隔）或者排序（按行数）的时间字段</li>
<li> as：别名，必须出现在后面的 groupBy 中</li>
</ol>
<p>代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">Expressions</span>.&#123;$, lit&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.&#123;<span class="type">Slide</span>, <span class="type">Table</span>, <span class="type">Tumble</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala.&#123;<span class="type">StreamTableEnvironment</span>, tableConversions&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SlideWindowApi</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">// 默认事件时间，不指定</span></span><br><span class="line">    println(env.getStreamTimeCharacteristic)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> tableEnv: <span class="type">StreamTableEnvironment</span> = <span class="type">StreamTableEnvironment</span>.create(env)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileSystemSql: <span class="type">String</span> = <span class="string">&quot;CREATE TABLE fs_table &quot;</span> +</span><br><span class="line">      <span class="string">&quot;(id STRING,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;ts BIGINT,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;  temp DOUBLE, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; row_time as  TO_TIMESTAMP( FROM_UNIXTIME(ts) ) ,&quot;</span> +</span><br><span class="line">      <span class="string">&quot; WATERMARK FOR row_time AS row_time - INTERVAL &#x27;5&#x27; SECOND) &quot;</span> +</span><br><span class="line">      <span class="string">&quot; WITH (&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;connector&#x27;=&#x27;filesystem&#x27;,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;path&#x27;=&#x27;D:\\Projects\\Flink-Table\\src\\main\\resources\\sensor.txt&#x27;,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;format&#x27;=&#x27;csv&#x27;)&quot;</span></span><br><span class="line"></span><br><span class="line">    tableEnv.executeSql(fileSystemSql)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.from(<span class="string">&quot;fs_table&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// sum0  计算式为空的空  取0</span></span><br><span class="line">    <span class="keyword">val</span> table1: <span class="type">Table</span> = table.window(<span class="type">Slide</span>.over(lit(<span class="number">1</span>).hour()).every(lit(<span class="number">5</span>).minute()).on($(<span class="string">&quot;row_time&quot;</span>)).as(<span class="string">&quot;w&quot;</span>))</span><br><span class="line">      .groupBy($(<span class="string">&quot;w&quot;</span>), $(<span class="string">&quot;id&quot;</span>))</span><br><span class="line">      .select($(<span class="string">&quot;id&quot;</span>), $(<span class="string">&quot;temp&quot;</span>).sum0(), $(<span class="string">&quot;w&quot;</span>).start(), $(<span class="string">&quot;w&quot;</span>).`end`())</span><br><span class="line"></span><br><span class="line">    table1.toAppendStream[<span class="type">Row</span>].print()</span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong><em>SQL</em></strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">Table</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala.&#123;<span class="type">StreamTableEnvironment</span>, tableConversions&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SlideWindowSql</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">// 默认事件时间，不指定</span></span><br><span class="line">    println(env.getStreamTimeCharacteristic)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> tableEnv: <span class="type">StreamTableEnvironment</span> = <span class="type">StreamTableEnvironment</span>.create(env)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileSystemSql: <span class="type">String</span> = <span class="string">&quot;CREATE TABLE fs_table &quot;</span> +</span><br><span class="line">      <span class="string">&quot;(id STRING,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;ts BIGINT,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;  temp DOUBLE, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; row_time as  TO_TIMESTAMP( FROM_UNIXTIME(ts) ) ,&quot;</span> +</span><br><span class="line">      <span class="string">&quot; WATERMARK FOR row_time AS row_time - INTERVAL &#x27;5&#x27; SECOND) &quot;</span> +</span><br><span class="line">      <span class="string">&quot; WITH (&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;connector&#x27;=&#x27;filesystem&#x27;,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;path&#x27;=&#x27;D:\\Projects\\Flink-Table\\src\\main\\resources\\sensor.txt&#x27;,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;format&#x27;=&#x27;csv&#x27;)&quot;</span></span><br><span class="line"></span><br><span class="line">    tableEnv.executeSql(fileSystemSql)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> table1: <span class="type">Table</span> = tableEnv.sqlQuery(</span><br><span class="line">      <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">        |SELECT</span></span><br><span class="line"><span class="string">        |  id,</span></span><br><span class="line"><span class="string">        |  HOP_START(row_time, INTERVAL &#x27;1&#x27; MINUTE, INTERVAL &#x27;1&#x27; HOUR) as wStart,</span></span><br><span class="line"><span class="string">        |  HOP_END(row_time, INTERVAL &#x27;1&#x27; MINUTE, INTERVAL &#x27;1&#x27; HOUR) as wStart,</span></span><br><span class="line"><span class="string">        |  SUM(temp)</span></span><br><span class="line"><span class="string">        | FROM fs_table</span></span><br><span class="line"><span class="string">        | GROUP BY HOP(row_time, INTERVAL &#x27;1&#x27; MINUTE, INTERVAL &#x27;1&#x27; HOUR), id</span></span><br><span class="line"><span class="string">    &quot;</span><span class="string">&quot;&quot;</span>.stripMargin)</span><br><span class="line"></span><br><span class="line">    table1.toAppendStream[<span class="type">Row</span>].print()</span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="会话窗口"><a href="#会话窗口" class="headerlink" title="会话窗口"></a>会话窗口</h3><p>会话窗口（Session windows）要用 Session 类来定义，另外还有三个方法：</p>
<ol>
<li>withGap：会话时间间隔 </li>
<li>on：用来分组（按时间间隔）或者排序（按行数）的时间字段 </li>
<li>as：别名，必须出现在后面的 groupBy 中</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/sql/queries.html#%E5%88%86%E7%BB%84%E7%AA%97%E5%8F%A3">Apache Flink 1.12 Documentation: 查询语句</a></p>
<h2 id="Over-Windows"><a href="#Over-Windows" class="headerlink" title="Over Windows"></a>Over Windows</h2><p>由于 Over 本来就是 SQL 内置支持的语法，所以这在 SQL 中属于基本的聚合操作。</p>
<p>所有 聚合必须在同一窗口上定义，也就是说，必须是相同的分区、排序和范围。目前仅支持在当 前行范围之前的窗口（无边界和有边界）。</p>
<p>注意，ORDER BY 必须在单一的时间属性上指定。</p>
<blockquote>
<p>和常见数据库的开创函数一样，对最后的结果进行指定范围内的开窗，计算后的结果形成新的逻辑列。和分组开窗聚合的区别是并不改变之间的结果行数。</p>
</blockquote>
<p>开窗可以是有界的也可以是无界的：</p>
<ul>
<li>无界：不对行数进行限制</li>
<li>有界：排序好的基础上进行行数的截取，目前只支持到本行，因为是流处理，不可能等下一条结果到了之后才开窗计算<ul>
<li> <code>ROWS BETWEEN 2 PRECEDING AND CURRENT ROW</code></li>
</ul>
</li>
</ul>
<p>代码如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">COUNT</span>(amount) <span class="keyword">OVER</span> (</span><br><span class="line"> <span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">user</span></span><br><span class="line"> <span class="keyword">ORDER</span> <span class="keyword">BY</span> proctime</span><br><span class="line"> <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">2</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>)</span><br><span class="line"><span class="keyword">FROM</span> Orders</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 也可以做多个聚合</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">COUNT</span>(amount) <span class="keyword">OVER</span> w, <span class="keyword">SUM</span>(amount) <span class="keyword">OVER</span> w</span><br><span class="line"><span class="keyword">FROM</span> Orders</span><br><span class="line"><span class="keyword">WINDOW</span> w <span class="keyword">AS</span> (</span><br><span class="line"> <span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">user</span></span><br><span class="line"> <span class="keyword">ORDER</span> <span class="keyword">BY</span> proctime</span><br><span class="line"> <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">2</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>)</span><br></pre></td></tr></table></figure>

<h1 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h1><p>Flink Table 和 SQL 内置了很多 SQL 中支持的函数；如果有无法满足的需要，则可以实 现用户自定义的函数（UDF）来解决。</p>
<h2 id="内置函数"><a href="#内置函数" class="headerlink" title="内置函数"></a>内置函数</h2><p>Flink Table API 和 SQL 为用户提供了一组用于数据转换的内置函数。SQL 中支持的很多 函数，Table API 和 SQL 都已经做了实现，其它还在快速开发扩展中。</p>
<p>以下是一些典型函数的举例，全部的内置函数，可以参考官网介绍。<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/functions/systemFunctions.html">Apache Flink 1.12 Documentation: 系统（内置）函数</a></p>
<p><strong><em>比较函数</em></strong></p>
<p>SQL</p>
<ul>
<li> value1 = value2 </li>
<li>value1 &gt; value2 </li>
</ul>
<p>Table API： </p>
<ul>
<li>ANY1 === ANY2</li>
<li>ANY1 &gt; ANY2</li>
</ul>
<p><strong><em>逻辑函数</em></strong></p>
<p>SQL： </p>
<ul>
<li>boolean1 OR boolean2</li>
<li> boolean IS FALSE </li>
<li>NOT boolean </li>
</ul>
<p>Table API：</p>
<ul>
<li> BOOLEAN1 || BOOLEAN2 </li>
<li>OOLEAN.isFalse </li>
<li>!BOOLEAN</li>
</ul>
<p><strong><em>算术函数</em></strong></p>
<p>SQL： </p>
<ul>
<li><p>numeric1 + numeric2 </p>
</li>
<li><p>POWER(numeric1, numeric2)</p>
<p>Table API：</p>
</li>
<li><p>NUMERIC1 + NUMERIC2 </p>
</li>
<li><p>NUMERIC1.power(NUMERIC2)</p>
</li>
</ul>
<p><strong><em>字符串函数</em></strong></p>
<p>SQL： </p>
<ul>
<li>string1 || string2 </li>
<li>UPPER(string)</li>
<li> CHAR_LENGTH(string) </li>
</ul>
<p>Table API：</p>
<ul>
<li>STRING1 + STRING2 </li>
<li>STRING.upperCase() </li>
<li>STRING.charLength()</li>
</ul>
<p><strong><em>时间函数</em></strong></p>
<p>SQL： </p>
<ul>
<li>DATE string </li>
<li>TIMESTAMP string </li>
<li>CURRENT_TIME </li>
<li>INTERVAL string range </li>
</ul>
<p>Table API： </p>
<ul>
<li>STRING.toDate </li>
<li>STRING.toTimestamp </li>
<li>currentTime() </li>
<li>NUMERIC.days </li>
<li>NUMERIC.minutes</li>
</ul>
<p><strong><em>聚合函数</em></strong></p>
<p>SQL：</p>
<ul>
<li>COUNT(*) </li>
<li>SUM([ ALL | DISTINCT ] expression) </li>
<li>RANK() </li>
<li>ROW_NUMBER() </li>
</ul>
<p>Table API： </p>
<ul>
<li>FIELD.count </li>
<li>FIELD.sum0 </li>
</ul>
<h2 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h2><p><a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/functions/udfs.html">Apache Flink 1.12 Documentation: 自定义函数</a></p>
<p>用户定义函数（User-defined Functions，UDF）是一个重要的特性，因为它们显著地扩 展了查询（Query）的表达能力。一些系统内置函数无法解决的需求，我们可以用 UDF 来自 定义实现。</p>
<p><font color="red">在大多数情况下，用户定义的函数必须先注册，然后才能在查询中使用。</font></p>
<p>不需要专门为 Scala 的 Table API 注册函数。 函数通过调用 registerFunction（）方法在 TableEnvironment 中注册。当用户定义的函数 被注册时，它被插入到 TableEnvironment 的函数目录中，这样 Table API 或 SQL 解析器就可 以识别并正确地解释它。</p>
<p>当前 Flink 有如下几种函数：</p>
<ul>
<li><em>标量函数</em> 将标量值转换成一个新标量值；</li>
<li><em>表值函数</em> 将标量值转换成新的行数据；</li>
<li><em>聚合函数</em> 将多行数据里的标量值转换成一个新标量值；</li>
<li><em>表值聚合函数</em> 将多行数据里的标量值转换成新的行数据；</li>
<li><em>异步表值函数</em> 是异步查询外部数据系统的特殊函数。</li>
</ul>
<h3 id="标量函数（Scalar-Functions）"><a href="#标量函数（Scalar-Functions）" class="headerlink" title="标量函数（Scalar Functions）"></a>标量函数（Scalar Functions）</h3><p>自定义标量函数可以把 0 到多个标量值映射成 1 个标量值，<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/types.html">数据类型</a>里列出的任何数据类型都可作为求值方法的参数和返回值类型。</p>
<p>想要实现自定义标量函数，你需要扩展 <code>org.apache.flink.table.functions</code> 里面的 <code>ScalarFunction</code> 并且实现一个或者多个求值方法。标量函数的行为取决于你写的求值方法。求值方法必须是 <code>public</code> 的，而且名字必须是 <code>eval</code>。</p>
<p>下面的例子展示了如何实现一个求哈希值的函数并在查询里调用它，详情可参考<a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/functions/udfs.html#%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97">开发指南</a>：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala.&#123;<span class="type">StreamTableEnvironment</span>, tableConversions&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.&#123;<span class="type">FieldExpression</span>, call&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.<span class="type">ScalarFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyScalarFunction</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">// 默认事件时间，不指定</span></span><br><span class="line">    println(env.getStreamTimeCharacteristic)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> tableEnv: <span class="type">StreamTableEnvironment</span> = <span class="type">StreamTableEnvironment</span>.create(env)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileSystemSql: <span class="type">String</span> = <span class="string">&quot;CREATE TABLE fs_table &quot;</span> +</span><br><span class="line">      <span class="string">&quot;(id STRING,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;ts BIGINT,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;  temp DOUBLE, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; row_time as  TO_TIMESTAMP( FROM_UNIXTIME(ts) ) ,&quot;</span> +</span><br><span class="line">      <span class="string">&quot; WATERMARK FOR row_time AS row_time - INTERVAL &#x27;5&#x27; SECOND) &quot;</span> +</span><br><span class="line">      <span class="string">&quot; WITH (&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;connector&#x27;=&#x27;filesystem&#x27;,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;path&#x27;=&#x27;D:\\Projects\\Flink-Table\\src\\main\\resources\\sensor.txt&#x27;,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;format&#x27;=&#x27;csv&#x27;)&quot;</span></span><br><span class="line"></span><br><span class="line">    tableEnv.executeSql(fileSystemSql)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. 在 Table API 里不经注册直接“内联”调用函数</span></span><br><span class="line">    tableEnv.from(<span class="string">&quot;fs_table&quot;</span>).select($<span class="string">&quot;id&quot;</span>, call(classOf[<span class="type">HashFunction</span>], $<span class="string">&quot;id&quot;</span>))</span><br><span class="line">      .toAppendStream[<span class="type">Row</span>].print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 注册函数</span></span><br><span class="line">    tableEnv.createTemporarySystemFunction(<span class="string">&quot;HashFunction&quot;</span>, classOf[<span class="type">HashFunction</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.1 在 Table API 里调用注册好的函数</span></span><br><span class="line">    tableEnv.from(<span class="string">&quot;fs_table&quot;</span>).select($<span class="string">&quot;id&quot;</span>, call(<span class="string">&quot;HashFunction&quot;</span>, $<span class="string">&quot;id&quot;</span>))</span><br><span class="line">      .toAppendStream[<span class="type">Row</span>].print()</span><br><span class="line">    <span class="comment">// 2.2 在 SQL 里调用注册好的函数</span></span><br><span class="line">    tableEnv.sqlQuery(<span class="string">&quot;SELECT id, HashFunction(id) FROM fs_table&quot;</span>)</span><br><span class="line">      .toAppendStream[<span class="type">Row</span>].print()</span><br><span class="line">    </span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">HashFunction</span> <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eval</span></span>(a: <span class="type">String</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">      a.hashCode</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="表函数（Table-Functions）"><a href="#表函数（Table-Functions）" class="headerlink" title="表函数（Table Functions）"></a>表函数（Table Functions）</h3><p>跟自定义标量函数一样，自定义表值函数的输入参数也可以是 0 到多个标量。<font color="red">但是跟标量函数只能返回一个值不同的是，它可以返回任意多行。返回的每一行可以包含 1 到多列</font>，如果输出行只包含 1 列，会省略结构化信息并生成标量值，这个标量值在运行阶段会隐式地包装进行里。</p>
<p>要定义一个表值函数，你需要扩展 <code>org.apache.flink.table.functions</code> 下的 <code>TableFunction</code>，可以通过实现多个名为 <code>eval</code> 的方法对求值方法进行重载。像其他函数一样，输入和输出类型也可以通过反射自动提取出来。表值函数返回的表的类型取决于 <code>TableFunction</code> 类的泛型参数 <code>T</code>，不同于标量函数，表值函数的求值方法本身不包含返回类型，而是通过 <code>collect(T)</code> 方法来发送要输出的行。</p>
<p>在 Table API 中，表值函数是通过 <code>.joinLateral(...)</code> 或者 <code>.leftOuterJoinLateral(...)</code> 来使用的。<code>joinLateral</code> 算子会把外表（算子左侧的表）的每一行跟跟表值函数返回的所有行（位于算子右侧）进行 （cross）join。<code>leftOuterJoinLateral</code> 算子也是把外表（算子左侧的表）的每一行跟表值函数返回的所有行（位于算子右侧）进行（cross）join，<strong>并且如果表值函数返回 0 行也会保留外表的这一行。</strong></p>
<blockquote>
<p>即<code>leftOuterJoinLateral</code> ，在表函数返回0行时，原表的此行也会保留，而<code>joinLateral</code>不会</p>
</blockquote>
<p>在 SQL 里面用 <code>JOIN</code> 或者 以 <code>ON TRUE</code> 为条件的 <code>LEFT JOIN</code> 来配合 <code>LATERAL TABLE(&lt;TableFunction&gt;)</code> 的使用。</p>
<blockquote>
<p>LEFT JOIN LATERAL TABLE和 FROM MyTable, LATERAL TABLE和上面一个意思。LEFT JOIN当表函数不返回值时，行会被保留，表函数本该返回的值为null，而 FROM MyTable, LATERAL直接不保留表函数没有返回值行</p>
</blockquote>
<blockquote>
<p>Table API 为什么需要Lateral，SQL为什么需要<code>JOIN</code> 。因为如果你将一列经过表函数形成了多行，那么没有经过此操作的列还是原来的行，为了填充这些列的值，需要和以前的数据进行join</p>
</blockquote>
<p>下面的例子展示了如何实现一个分隔函数并在查询里调用它，将id字段以_分割，输出每个被分割的值和长度：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.example.table.udf</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.annotation.&#123;<span class="type">DataTypeHint</span>, <span class="type">FunctionHint</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.<span class="type">TableFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyTableFunction</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">// 默认事件时间，不指定</span></span><br><span class="line">    println(env.getStreamTimeCharacteristic)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> tableEnv: <span class="type">StreamTableEnvironment</span> = <span class="type">StreamTableEnvironment</span>.create(env)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileSystemSql: <span class="type">String</span> = <span class="string">&quot;CREATE TABLE MyTable &quot;</span> +</span><br><span class="line">      <span class="string">&quot;(id STRING,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;ts BIGINT,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;  temp DOUBLE, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; row_time as  TO_TIMESTAMP( FROM_UNIXTIME(ts) ) ,&quot;</span> +</span><br><span class="line">      <span class="string">&quot; WATERMARK FOR row_time AS row_time - INTERVAL &#x27;5&#x27; SECOND) &quot;</span> +</span><br><span class="line">      <span class="string">&quot; WITH (&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;connector&#x27;=&#x27;filesystem&#x27;,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;path&#x27;=&#x27;D:\\Projects\\Flink-Table\\src\\main\\resources\\sensor.txt&#x27;,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;format&#x27;=&#x27;csv&#x27;)&quot;</span></span><br><span class="line"></span><br><span class="line">    tableEnv.executeSql(fileSystemSql)</span><br><span class="line">    <span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.from(<span class="string">&quot;MyTable&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. 在 Table API 里不经注册直接“内联”调用函数</span></span><br><span class="line"><span class="comment">//    即`leftOuterJoinLateral` ，在表函数返回0行时，原表的此行也会保留，而`joinLateral`不会</span></span><br><span class="line">    table</span><br><span class="line">      .joinLateral(call(classOf[<span class="type">SplitFunction</span>], $<span class="string">&quot;id&quot;</span>))</span><br><span class="line">      .select($<span class="string">&quot;id&quot;</span>, $<span class="string">&quot;word&quot;</span>, $<span class="string">&quot;length&quot;</span>)</span><br><span class="line">      .toAppendStream[<span class="type">Row</span>].print()</span><br><span class="line"></span><br><span class="line">    table</span><br><span class="line">      .leftOuterJoinLateral(call(classOf[<span class="type">SplitFunction</span>], $<span class="string">&quot;myField&quot;</span>))</span><br><span class="line">      .select($<span class="string">&quot;myField&quot;</span>, $<span class="string">&quot;word&quot;</span>, $<span class="string">&quot;length&quot;</span>)</span><br><span class="line"><span class="comment">//</span></span><br><span class="line">    <span class="comment">// 下面的调用方式都需要注册之后</span></span><br><span class="line">    tableEnv.createTemporarySystemFunction(<span class="string">&quot;SplitFunction&quot;</span>, classOf[<span class="type">SplitFunction</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 在 Table API 里调用注册好的函数</span></span><br><span class="line">    table</span><br><span class="line">      .joinLateral(call(<span class="string">&quot;SplitFunction&quot;</span>, $<span class="string">&quot;myField&quot;</span>))</span><br><span class="line">      .select($<span class="string">&quot;myField&quot;</span>, $<span class="string">&quot;word&quot;</span>, $<span class="string">&quot;length&quot;</span>)</span><br><span class="line">    table</span><br><span class="line">      .leftOuterJoinLateral(call(<span class="string">&quot;SplitFunction&quot;</span>, $<span class="string">&quot;myField&quot;</span>))</span><br><span class="line">      .select($<span class="string">&quot;myField&quot;</span>, $<span class="string">&quot;word&quot;</span>, $<span class="string">&quot;length&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//     3.在 SQL 里调用注册好的函数// 在 SQL 里调用注册好的函数</span></span><br><span class="line">    tableEnv.sqlQuery(<span class="string">&quot;SELECT id, word, length &quot;</span> + <span class="string">&quot;FROM MyTable, LATERAL TABLE(SplitFunction&quot;</span> +</span><br><span class="line">      <span class="string">&quot;(id))&quot;</span>).toAppendStream[<span class="type">Row</span>].print()</span><br><span class="line">    tableEnv.sqlQuery(<span class="string">&quot;SELECT id, word, length &quot;</span> + <span class="string">&quot;FROM MyTable &quot;</span> + <span class="string">&quot;LEFT JOIN LATERAL TABLE&quot;</span> +</span><br><span class="line">      <span class="string">&quot;(SplitFunction(id)) ON TRUE&quot;</span>).toAppendStream[<span class="type">Row</span>].print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 函数类的所有求值方法指定同一个输出类型</span></span><br><span class="line">  <span class="meta">@FunctionHint</span>(output = <span class="keyword">new</span> <span class="type">DataTypeHint</span>(<span class="string">&quot;ROW&lt;word STRING, length INT&gt;&quot;</span>))</span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">SplitFunction</span> <span class="keyword">extends</span> <span class="title">TableFunction</span>[<span class="type">Row</span>] </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eval</span></span>(str: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="comment">// use collect(...) to emit a row</span></span><br><span class="line">      str.split(<span class="string">&quot;_&quot;</span>).foreach(s =&gt; collect(<span class="type">Row</span>.of(s, <span class="type">Int</span>.box(s.length))))</span><br><span class="line"><span class="comment">//      if (!str.equalsIgnoreCase(&quot;sensor_6&quot;))&#123;</span></span><br><span class="line"><span class="comment">//        str.split(&quot;_&quot;).foreach(s =&gt; collect(Row.of(s, Int.box(s.length))))</span></span><br><span class="line"><span class="comment">//      &#125;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="聚合函数（Aggregate-Functions）"><a href="#聚合函数（Aggregate-Functions）" class="headerlink" title="聚合函数（Aggregate Functions）"></a>聚合函数（Aggregate Functions）</h3><p>自定义聚合函数（UDAGG）是把一个表（一行或者多行，每行可以有一列或者多列）聚合成一个标量值。</p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20210113212645785.png" alt="image-20210113212645785"></p>
<p>上面的图片展示了一个聚合的例子。假设你有一个关于饮料的表。表里面有三个字段，分别是 <code>id</code>、<code>name</code>、<code>price</code>，表里有 5 行数据。假设你需要找到所有饮料里最贵的饮料的价格，即执行一个 <code>max()</code> 聚合。你需要遍历所有 5 行数据，而结果就只有一个数值。</p>
<p>自定义聚合函数是通过扩展 <code>AggregateFunction</code> 来实现的。<code>AggregateFunction</code> 的工作过程如下。首先，它需要一个 <code>accumulator</code>，它是一个数据结构，存储了聚合的中间结果。通过调用 <code>AggregateFunction</code> 的 <code>createAccumulator()</code> 方法创建一个空的 accumulator。接下来，对于每一行数据，会调用 <code>accumulate()</code> 方法来更新 accumulator。当所有的数据都处理完了之后，通过调用 <code>getValue</code> 方法来计算和返回最终的结果。</p>
<p><strong>下面几个方法是每个 <code>AggregateFunction</code> 必须要实现的：</strong></p>
<ul>
<li><code>createAccumulator()</code></li>
<li><code>accumulate()</code></li>
<li><code>getValue()</code></li>
</ul>
<p>Flink 的类型推导在遇到复杂类型的时候可能会推导出错误的结果，比如那些非基本类型和普通的 POJO 类型的复杂类型。所以跟 <code>ScalarFunction</code> 和 <code>TableFunction</code> 一样，<code>AggregateFunction</code> 也提供了 <code>AggregateFunction#getResultType()</code> 和 <code>AggregateFunction#getAccumulatorType()</code> 来分别指定返回值类型和 accumulator 的类型，两个函数的返回值类型也都是 <code>TypeInformation</code>。</p>
<p>除了上面的方法，还有几个方法可以选择实现。这些方法有些可以让查询更加高效，而有些是在某些特定场景下必须要实现的。例如，<font color="red">如果聚合函数用在会话窗口（当两个会话窗口合并的时候需要 merge 他们的 accumulator）的话，<code>merge()</code> 方法就是必须要实现的。</font></p>
<p><strong><code>AggregateFunction</code> 的以下方法在某些场景下是必须实现的：</strong></p>
<ul>
<li><code>retract()</code> 在 bounded <code>OVER</code> 窗口中是必须实现的。</li>
<li><code>merge()</code> 在许多批式聚合和会话以及滚动窗口聚合中是必须实现的。除此之外，这个方法对于优化也很多帮助。例如，两阶段聚合优化就需要所有的 <code>AggregateFunction</code> 都实现 <code>merge</code> 方法。</li>
<li><code>resetAccumulator()</code> 在许多批式聚合中是必须实现的。</li>
</ul>
<p><code>AggregateFunction</code> 的所有方法都必须是 <code>public</code> 的，不能是 <code>static</code> 的，而且名字必须跟上面写的一样。<code>createAccumulator</code>、<code>getValue</code>、<code>getResultType</code> 以及 <code>getAccumulatorType</code> 这几个函数是在抽象类 <code>AggregateFunction</code> 中定义的，而其他函数都是约定的方法。如果要定义一个聚合函数，你需要扩展 <code>org.apache.flink.table.functions.AggregateFunction</code>，并且实现一个（或者多个）<code>accumulate</code> 方法。<code>accumulate</code> 方法可以重载，每个方法的参数类型不同，并且支持变长参数。</p>
<hr>
<p>自定义实现avg聚合：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.example.table.udf</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.annotation.&#123;<span class="type">DataTypeHint</span>, <span class="type">FunctionHint</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">Expressions</span>.lit</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.&#123;$, <span class="type">Table</span>, <span class="type">Tumble</span>, <span class="type">WithOperations</span>, call&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.<span class="type">AggregateFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyAggregateFunction</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">// 默认事件时间，不指定</span></span><br><span class="line">    println(env.getStreamTimeCharacteristic)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> tableEnv: <span class="type">StreamTableEnvironment</span> = <span class="type">StreamTableEnvironment</span>.create(env)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fileSystemSql: <span class="type">String</span> = <span class="string">&quot;CREATE TABLE MyTable &quot;</span> +</span><br><span class="line">      <span class="string">&quot;(id STRING,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;ts BIGINT,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;  temp DOUBLE, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; row_time as  TO_TIMESTAMP( FROM_UNIXTIME(ts) ) ,&quot;</span> +</span><br><span class="line">      <span class="string">&quot; WATERMARK FOR row_time AS row_time - INTERVAL &#x27;5&#x27; SECOND) &quot;</span> +</span><br><span class="line">      <span class="string">&quot; WITH (&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;connector&#x27;=&#x27;filesystem&#x27;,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;path&#x27;=&#x27;D:\\Projects\\Flink-Table\\src\\main\\resources\\sensor.txt&#x27;,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;&#x27;format&#x27;=&#x27;csv&#x27;)&quot;</span></span><br><span class="line"></span><br><span class="line">    tableEnv.executeSql(fileSystemSql)</span><br><span class="line">    <span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.from(<span class="string">&quot;MyTable&quot;</span>)</span><br><span class="line"></span><br><span class="line">    tableEnv.createTemporarySystemFunction(<span class="string">&quot;myavg&quot;</span>, classOf[<span class="type">MyAvg</span>])</span><br><span class="line">    table.window(<span class="type">Tumble</span>.over(lit(<span class="number">1</span>).hour()).on($(<span class="string">&quot;row_time&quot;</span>)).as(<span class="string">&quot;w&quot;</span>))</span><br><span class="line">      .groupBy($(<span class="string">&quot;w&quot;</span>), $(<span class="string">&quot;id&quot;</span>))</span><br><span class="line">      .select($(<span class="string">&quot;id&quot;</span>).as(<span class="string">&quot;test&quot;</span>), call(<span class="string">&quot;myavg&quot;</span>, $(<span class="string">&quot;temp&quot;</span>)).as(<span class="string">&quot;avg&quot;</span>))</span><br><span class="line">      .toAppendStream[<span class="type">Row</span>].print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">CountAccumulator</span>(<span class="params">var count: <span class="type">Long</span>, var sum: <span class="type">Double</span></span>) </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyAvg</span> <span class="keyword">extends</span> <span class="title">AggregateFunction</span>[<span class="type">Double</span>, <span class="type">CountAccumulator</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 不加报错</span></span><br><span class="line">    <span class="meta">@FunctionHint</span>(</span><br><span class="line">      input = <span class="type">Array</span>(<span class="keyword">new</span> <span class="type">DataTypeHint</span>(<span class="string">&quot;DOUBLE&quot;</span>)),</span><br><span class="line">      output = <span class="keyword">new</span> <span class="type">DataTypeHint</span>(<span class="string">&quot;DOUBLE&quot;</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accumulate</span></span>(acc: <span class="type">CountAccumulator</span>, param: <span class="type">Double</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      acc.sum = acc.sum + param</span><br><span class="line">      acc.count = acc.count + <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValue</span></span>(accumulator: <span class="type">CountAccumulator</span>): <span class="type">Double</span> = accumulator.sum / accumulator.count</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createAccumulator</span></span>(): <span class="type">CountAccumulator</span> = <span class="type">CountAccumulator</span>(<span class="number">0</span>L, <span class="number">0.0</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="表聚合函数（Table-Aggregate-Functions）"><a href="#表聚合函数（Table-Aggregate-Functions）" class="headerlink" title="表聚合函数（Table Aggregate Functions）"></a>表聚合函数（Table Aggregate Functions）</h3><p>= 表函数 + 聚合函数</p>
<p>略。</p>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">@FunctionHint(output &#x3D; new DataTypeHint(&quot;ROW&lt;word STRING, length INT&gt;&quot;))</span><br></pre></td></tr></table></figure>

<p>挺重要的</p>
<p><a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/table/functions/udfs.html#%E7%B1%BB%E5%9E%8B%E6%8E%A8%E5%AF%BC">Apache Flink 1.12 Documentation: 自定义函数</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Xiangjie</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://awslzhang.top/2021/01/10/Api%E5%92%8CSQL/">https://awslzhang.top/2021/01/10/Api%E5%92%8CSQL/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://awslzhang.top" target="_blank">zxj</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Flink/">Flink</a></div><div class="post_share"><div class="social-share" data-image="https://flink.apache.org/img/flink-header-logo.svg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2021/01/02/Flink%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86/"><img class="next-cover" src="https://flink.apache.org/img/flink-header-logo.svg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Flink状态编程和容错机制</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/12/16/Flink学习笔记/" title="Flink学习笔记"><img class="cover" src="https://flink.apache.org/img/flink-header-logo.svg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-16</div><div class="title">Flink学习笔记</div></div></a></div><div><a href="/2021/01/02/Flink状态管理/" title="Flink状态编程和容错机制"><img class="cover" src="https://flink.apache.org/img/flink-header-logo.svg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-01-02</div><div class="title">Flink状态编程和容错机制</div></div></a></div><div><a href="/2020/12/23/Flink时间语义和WaterMark/" title="Flink的时间语义和watermark"><img class="cover" src="https://flink.apache.org/img/flink-header-logo.svg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-23</div><div class="title">Flink的时间语义和watermark</div></div></a></div><div><a href="/2020/12/17/Flink-API/" title="Flink Api学习"><img class="cover" src="https://flink.apache.org/img/flink-header-logo.svg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-17</div><div class="title">Flink Api学习</div></div></a></div></div></div></div><div class="aside_content" id="aside_content"><div class="card-widget card-info"><div class="card-content"><div class="card-info-avatar is-center"><img class="avatar-img" src="/null" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Xiangjie</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">85</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">43</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">25</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/XiangJie-Zhang" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:qluzxj@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div></div><div class="card-widget card-announcement"><div class="card-content"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="card-content"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B4%E4%BD%93%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.</span> <span class="toc-text">整体介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFTable-API%E5%92%8CFlink-SQL"><span class="toc-number">1.2.</span> <span class="toc-text">什么是Table API和Flink SQL</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9C%80%E8%A6%81%E5%BC%95%E5%85%A5%E7%9A%84%E4%BE%9D%E8%B5%96"><span class="toc-number">1.3.</span> <span class="toc-text">需要引入的依赖</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E7%89%88%E6%9C%AC%E7%9A%84Planner%F0%9F%94%BA"><span class="toc-number">1.4.</span> <span class="toc-text">不同版本的Planner🔺</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#API%E8%B0%83%E7%94%A8"><span class="toc-number">2.</span> <span class="toc-text">API调用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A1%A8%E7%8E%AF%E5%A2%83%E5%88%9B%E5%BB%BA%F0%9F%94%BA"><span class="toc-number">2.1.</span> <span class="toc-text">表环境创建🔺</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%BB%98%E8%AE%A4%F0%9F%94%BA"><span class="toc-number">2.1.1.</span> <span class="toc-text">默认🔺</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#planner"><span class="toc-number">2.1.2.</span> <span class="toc-text">planner</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%B9%E5%A4%84%E7%90%86%E7%8E%AF%E5%A2%83"><span class="toc-number">2.1.2.1.</span> <span class="toc-text">批处理环境</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%81%E5%A4%84%E7%90%86%E7%8E%AF%E5%A2%83"><span class="toc-number">2.1.2.2.</span> <span class="toc-text">流处理环境</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#blink%F0%9F%94%BA"><span class="toc-number">2.1.3.</span> <span class="toc-text">blink🔺</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%B9%E5%A4%84%E7%90%86%E7%8E%AF%E5%A2%83-1"><span class="toc-number">2.1.3.1.</span> <span class="toc-text">批处理环境</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%81%E5%A4%84%E7%90%86%E7%8E%AF%E5%A2%83-1"><span class="toc-number">2.1.3.2.</span> <span class="toc-text">流处理环境</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E7%A8%8B%E5%BA%8F%E7%BB%93%E6%9E%84"><span class="toc-number">2.2.</span> <span class="toc-text">基本程序结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9C%A8-Catalog-%E4%B8%AD%E6%B3%A8%E5%86%8C%E8%A1%A8"><span class="toc-number">2.3.</span> <span class="toc-text">在 Catalog 中注册表</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%A8%EF%BC%88Table%EF%BC%89%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-number">2.3.1.</span> <span class="toc-text">表（Table）的概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E8%A1%A8%F0%9F%94%BA"><span class="toc-number">2.3.2.</span> <span class="toc-text">创建表🔺</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%96%E9%83%A8%E7%B3%BB%E7%BB%9F%E5%88%9B%E5%BB%BA"><span class="toc-number">2.3.2.1.</span> <span class="toc-text">外部系统创建</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F"><span class="toc-number">2.3.2.1.1.</span> <span class="toc-text">文件系统</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DataStream%E8%BD%AC%E6%8D%A2"><span class="toc-number">2.3.2.2.</span> <span class="toc-text">DataStream转换</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E4%B8%B4%E6%97%B6%E8%A7%86%E5%9B%BE"><span class="toc-number">2.3.3.</span> <span class="toc-text">创建临时视图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E8%A1%A8%F0%9F%94%BA"><span class="toc-number">2.3.4.</span> <span class="toc-text">输出表🔺</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9B%B4%E6%96%B0%E6%A8%A1%E5%BC%8F%F0%9F%94%BA"><span class="toc-number">3.</span> <span class="toc-text">更新模式🔺</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%86%E8%A1%A8%E8%BD%AC%E4%B8%BADataStream"><span class="toc-number">3.1.</span> <span class="toc-text">将表转为DataStream</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B5%81%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E7%89%B9%E6%AE%8A%E6%A6%82%E5%BF%B5%F0%9F%94%BA"><span class="toc-number">4.</span> <span class="toc-text">流处理中的特殊概念🔺</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%81%E5%A4%84%E7%90%86%E5%92%8C%E5%85%B3%E7%B3%BB%E4%BB%A3%E6%95%B0%EF%BC%88%E8%A1%A8%EF%BC%8C%E5%8F%8A-SQL%EF%BC%89%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">4.1.</span> <span class="toc-text">流处理和关系代数（表，及 SQL）的区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A8%E6%80%81%E8%A1%A8%EF%BC%88Dynamic-Tables%EF%BC%89"><span class="toc-number">4.2.</span> <span class="toc-text">动态表（Dynamic Tables）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%81%E5%BC%8F%E6%8C%81%E7%BB%AD%E6%9F%A5%E8%AF%A2%E7%9A%84%E8%BF%87%E7%A8%8B"><span class="toc-number">4.3.</span> <span class="toc-text">流式持续查询的过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%86%E6%B5%81%E8%BD%AC%E6%8D%A2%E6%88%90%E8%A1%A8%EF%BC%88Table%EF%BC%89"><span class="toc-number">4.3.1.</span> <span class="toc-text">将流转换成表（Table）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%81%E7%BB%AD%E6%9F%A5%E8%AF%A2%EF%BC%88Continuous-Query%EF%BC%89"><span class="toc-number">4.3.2.</span> <span class="toc-text">持续查询（Continuous Query）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%86%E5%8A%A8%E6%80%81%E8%A1%A8%E8%BD%AC%E6%8D%A2%E6%88%90%E6%B5%81"><span class="toc-number">4.3.3.</span> <span class="toc-text">将动态表转换成流</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%B6%E9%97%B4%E7%89%B9%E6%80%A7%F0%9F%94%BA"><span class="toc-number">4.4.</span> <span class="toc-text">时间特性🔺</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E6%97%B6%E9%97%B4%EF%BC%88Processing-Time%EF%BC%89"><span class="toc-number">4.4.1.</span> <span class="toc-text">处理时间（Processing Time）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#DataStream-%E8%BD%AC%E5%8C%96%E6%88%90-Table-%E6%97%B6%E6%8C%87%E5%AE%9A"><span class="toc-number">4.4.1.1.</span> <span class="toc-text">DataStream 转化成 Table 时指定</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E8%A1%A8%E7%9A%84-DDL-%E4%B8%AD%E6%8C%87%E5%AE%9A"><span class="toc-number">4.4.1.2.</span> <span class="toc-text">创建表的 DDL 中指定</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4%EF%BC%88Event-Time%EF%BC%89"><span class="toc-number">4.4.2.</span> <span class="toc-text">事件时间（Event Time）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#DataStream-%E8%BD%AC%E5%8C%96%E6%88%90-Table-%E6%97%B6%E6%8C%87%E5%AE%9A-1"><span class="toc-number">4.4.2.1.</span> <span class="toc-text">DataStream 转化成 Table 时指定</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89-Table-Schema-%E6%97%B6%E6%8C%87%E5%AE%9A"><span class="toc-number">4.4.2.2.</span> <span class="toc-text">定义 Table Schema 时指定</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E8%A1%A8%E7%9A%84-DDL-%E4%B8%AD%E6%8C%87%E5%AE%9A-1"><span class="toc-number">4.4.2.3.</span> <span class="toc-text">创建表的 DDL 中指定</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AA%97%E5%8F%A3%F0%9F%94%BA"><span class="toc-number">5.</span> <span class="toc-text">窗口🔺</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E7%BB%84%E7%AA%97%E5%8F%A3%EF%BC%88Group-Windows%EF%BC%89"><span class="toc-number">5.1.</span> <span class="toc-text">分组窗口（Group Windows）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BB%9A%E5%8A%A8%E7%AA%97%E5%8F%A3"><span class="toc-number">5.1.1.</span> <span class="toc-text">滚动窗口</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3"><span class="toc-number">5.1.2.</span> <span class="toc-text">滑动窗口</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%9A%E8%AF%9D%E7%AA%97%E5%8F%A3"><span class="toc-number">5.1.3.</span> <span class="toc-text">会话窗口</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Over-Windows"><span class="toc-number">5.2.</span> <span class="toc-text">Over Windows</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%87%BD%E6%95%B0"><span class="toc-number">6.</span> <span class="toc-text">函数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0"><span class="toc-number">6.1.</span> <span class="toc-text">内置函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#UDF"><span class="toc-number">6.2.</span> <span class="toc-text">UDF</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%87%E9%87%8F%E5%87%BD%E6%95%B0%EF%BC%88Scalar-Functions%EF%BC%89"><span class="toc-number">6.2.1.</span> <span class="toc-text">标量函数（Scalar Functions）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%A8%E5%87%BD%E6%95%B0%EF%BC%88Table-Functions%EF%BC%89"><span class="toc-number">6.2.2.</span> <span class="toc-text">表函数（Table Functions）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0%EF%BC%88Aggregate-Functions%EF%BC%89"><span class="toc-number">6.2.3.</span> <span class="toc-text">聚合函数（Aggregate Functions）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%A8%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0%EF%BC%88Table-Aggregate-Functions%EF%BC%89"><span class="toc-number">6.2.4.</span> <span class="toc-text">表聚合函数（Table Aggregate Functions）</span></a></li></ol></li></ol></li></ol></div></div></div><div class="card-widget card-recent-post"><div class="card-content"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/01/10/Api%E5%92%8CSQL/" title="Api和SQL"><img src="https://flink.apache.org/img/flink-header-logo.svg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Api和SQL"/></a><div class="content"><a class="title" href="/2021/01/10/Api%E5%92%8CSQL/" title="Api和SQL">Api和SQL</a><time datetime="2021-01-10T03:51:40.000Z" title="发表于 2021-01-10 11:51:40">2021-01-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/01/02/Flink%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86/" title="Flink状态编程和容错机制"><img src="https://flink.apache.org/img/flink-header-logo.svg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Flink状态编程和容错机制"/></a><div class="content"><a class="title" href="/2021/01/02/Flink%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86/" title="Flink状态编程和容错机制">Flink状态编程和容错机制</a><time datetime="2021-01-02T10:06:02.000Z" title="发表于 2021-01-02 18:06:02">2021-01-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/23/Flink%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89%E5%92%8CWaterMark/" title="Flink的时间语义和watermark"><img src="https://flink.apache.org/img/flink-header-logo.svg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Flink的时间语义和watermark"/></a><div class="content"><a class="title" href="/2020/12/23/Flink%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89%E5%92%8CWaterMark/" title="Flink的时间语义和watermark">Flink的时间语义和watermark</a><time datetime="2020-12-23T10:59:07.000Z" title="发表于 2020-12-23 18:59:07">2020-12-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/17/Flink-API/" title="Flink Api学习"><img src="https://flink.apache.org/img/flink-header-logo.svg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Flink Api学习"/></a><div class="content"><a class="title" href="/2020/12/17/Flink-API/" title="Flink Api学习">Flink Api学习</a><time datetime="2020-12-17T12:36:28.000Z" title="发表于 2020-12-17 20:36:28">2020-12-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/17/ready/" title="ready!"><img src="https://images.pexels.com/photos/924824/pexels-photo-924824.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ready!"/></a><div class="content"><a class="title" href="/2020/12/17/ready/" title="ready!">ready!</a><time datetime="2020-12-16T16:02:06.000Z" title="发表于 2020-12-17 00:02:06">2020-12-17</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Xiangjie</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script></div></body></html>