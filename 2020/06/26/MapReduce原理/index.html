<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>MapReduce原理 | zxj</title><meta name="keywords" content="Hadoop"><meta name="author" content="Xiangjie"><meta name="copyright" content="Xiangjie"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="MapReduce概述MapReduce定义MapReduce是一个分布式运算程序的编程框架，是用户开发基于“Hadoop的数据分析应用”的核心框架。 MapReduce的核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个Hadoop集群上。 MapReduce优缺点优点  易于编程 良好的扩展性(计算资源) 高容错性 适合PB级以上数据的离线处理  缺">
<meta property="og:type" content="article">
<meta property="og:title" content="MapReduce原理">
<meta property="og:url" content="https://awslzhang.top/2020/06/26/MapReduce%E5%8E%9F%E7%90%86/index.html">
<meta property="og:site_name" content="zxj">
<meta property="og:description" content="MapReduce概述MapReduce定义MapReduce是一个分布式运算程序的编程框架，是用户开发基于“Hadoop的数据分析应用”的核心框架。 MapReduce的核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个Hadoop集群上。 MapReduce优缺点优点  易于编程 良好的扩展性(计算资源) 高容错性 适合PB级以上数据的离线处理  缺">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2020/06/27/2xbLZqNQKHGoUu1.jpg">
<meta property="article:published_time" content="2020-06-26T15:39:15.000Z">
<meta property="article:modified_time" content="2021-01-01T05:49:59.958Z">
<meta property="article:author" content="Xiangjie">
<meta property="article:tag" content="Hadoop">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2020/06/27/2xbLZqNQKHGoUu1.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://awslzhang.top/2020/06/26/MapReduce%E5%8E%9F%E7%90%86/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: Xiangjie","link":"链接: ","source":"来源: zxj","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}

// https://stackoverflow.com/questions/16839698/jquery-getscript-alternative-in-native-javascript
const getScript = url => new Promise((resolve, reject) => {
  const script = document.createElement('script')
  script.src = url
  script.async = true
  script.onerror = reject
  script.onload = script.onreadystatechange = function() {
    const loadState = this.readyState
    if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
    script.onload = script.onreadystatechange = null
    resolve()
  }
  document.head.appendChild(script)
})</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2021-01-01 13:49:59'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
   if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }
  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified
    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }const asideStatus = saveToLocal.get('aside-status')
if (asideStatus !== undefined) {
   if (asideStatus === 'hide') {
     document.documentElement.classList.add('hide-aside')
   } else {
     document.documentElement.classList.remove('hide-aside')
   }
}})()</script><meta name="generator" content="Hexo 5.0.2"><link rel="alternate" href="/atom.xml" title="zxj" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/null" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">85</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">43</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">25</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 目录</span></a></div></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://i.loli.net/2020/06/27/2xbLZqNQKHGoUu1.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">zxj</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 目录</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">MapReduce原理</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-06-26T15:39:15.000Z" title="发表于 2020-06-26 23:39:15">2020-06-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-01-01T05:49:59.958Z" title="更新于 2021-01-01 13:49:59">2021-01-01</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>16分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="MapReduce概述"><a href="#MapReduce概述" class="headerlink" title="MapReduce概述"></a>MapReduce概述</h1><h2 id="MapReduce定义"><a href="#MapReduce定义" class="headerlink" title="MapReduce定义"></a>MapReduce定义</h2><p>MapReduce是一个<font color="red">分布式运算程序的编程框架</font>，是用户开发基于“Hadoop的数据分析应用”的核心框架。</p>
<p>MapReduce的核心功能是将<strong>用户编写的业务逻辑代码和自带默认组件</strong>整合成一个完整的<strong>分布式运算程序</strong>，并发运行在一个Hadoop集群上。</p>
<h2 id="MapReduce优缺点"><a href="#MapReduce优缺点" class="headerlink" title="MapReduce优缺点"></a>MapReduce优缺点</h2><p><strong>优点</strong></p>
<ol>
<li>易于编程</li>
<li>良好的扩展性(计算资源)</li>
<li>高容错性</li>
<li>适合PB级以上数据的离线处理</li>
</ol>
<p><strong>缺点(实质：计算速度慢)</strong></p>
<ol>
<li>不擅长实时计算：无法再毫秒/秒级返回结果</li>
<li>不擅长流式计算：流式计算的输入数据是动态的，而MapReduce只能处理静态数据</li>
<li>不擅长DAG(有向图)计算：多个应用程序之间存在依赖关系，后一应用的输入为前一个的输出。这里使用MapReduce会很慢，它每次将结果都写入磁盘</li>
</ol>
<h2 id="MapReudce流程"><a href="#MapReudce流程" class="headerlink" title="MapReudce流程"></a>MapReudce流程</h2><p><img src="https://i.loli.net/2020/07/02/LgUycWVrnBOZpfx.png" alt="image-20200702101011024"></p>
<ol>
<li>分布式的运算程序往往需要分成至少2个阶段。</li>
<li>第一个阶段的MapTask并发实例，完全并行运行，互不相干。(默认：并行度由block数量决定)</li>
<li>第二个阶段的ReduceTask并发实例互不相干，但是他们的数据依赖于上一个阶段的所有MapTask并发实例的部分输出(指定分区)</li>
<li>MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序，串行运行。</li>
</ol>
<h2 id="MapReduce进程"><a href="#MapReduce进程" class="headerlink" title="MapReduce进程"></a>MapReduce进程</h2><p>一个完整的MapReduce程序在分布式运行时有三类实例进程：</p>
<ol>
<li><code>MrAppMaster</code>：负责整个程序的过程调度及状态协调</li>
<li><code>MapTask</code>：负责Map阶段的整个数据处理流程</li>
<li><code>ReduceTask</code>：负责Reduce阶段的整个数据处理流程</li>
</ol>
<h2 id="MapReduce序列化"><a href="#MapReduce序列化" class="headerlink" title="MapReduce序列化"></a>MapReduce序列化</h2><p>MapReduce程序由三部分构成：Map类、Reduce类和驱动类。且数据的类型是Hadoop自身封装的序列化类型。</p>
<p><strong>常用数据序列化类型</strong></p>
<table>
<thead>
<tr>
<th><strong>Java类型</strong></th>
<th><strong>Hadoop Writable类型</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Boolean</td>
<td>BooleanWritable</td>
</tr>
<tr>
<td>Byte</td>
<td>ByteWritable</td>
</tr>
<tr>
<td>Int</td>
<td>IntWritable</td>
</tr>
<tr>
<td>Float</td>
<td>FloatWritable</td>
</tr>
<tr>
<td>Long</td>
<td>LongWritable</td>
</tr>
<tr>
<td>Double</td>
<td>DoubleWritable</td>
</tr>
<tr>
<td>String</td>
<td>Text</td>
</tr>
<tr>
<td>Map</td>
<td>MapWritable</td>
</tr>
<tr>
<td>Array</td>
<td>ArrayWritable</td>
</tr>
</tbody></table>
<h3 id="自定义bean对象实现序列化接口（Writable）"><a href="#自定义bean对象实现序列化接口（Writable）" class="headerlink" title="自定义bean对象实现序列化接口（Writable）"></a>自定义bean对象实现序列化接口（Writable）</h3><p>在企业开发中往往常用的基本序列化类型不能满足所有需求，比如在Hadoop框架内部传递一个bean对象，那么该对象就需要实现序列化接口。</p>
<p>具体实现bean对象序列化步骤如下7步。</p>
<ol>
<li>必须实现Writable接口</li>
<li>反序列化时，需要反射调用空参构造函数，所以必须有空参构造</li>
<li>重写序列化方法</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    out.writeLong(upFlow);</span><br><span class="line">    out.writeLong(downFlow);</span><br><span class="line">    out.writeLong(sumFlow);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>重写反序列化方法</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    upFlow = in.readLong();</span><br><span class="line">    downFlow = in.readLong();</span><br><span class="line">    sumFlow = in.readLong();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>注意反序列化的顺序和序列化的顺序完全一致</li>
<li>要想把结果显示在文件中，需要重写toString()，可用”\t”分开，方便后续用。</li>
<li>如果需要将自定义的bean放在key中传输，则还需要实现<code>Comparable</code>接口，因为MapReduce框中的Shuffle过程要求对key必须能排序。</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean o)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 倒序排列，从大到小</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>.sumFlow &gt; o.getSumFlow() ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="MapReduce编程规范"><a href="#MapReduce编程规范" class="headerlink" title="MapReduce编程规范"></a>MapReduce编程规范</h2><p><strong>Mapper阶段</strong></p>
<ol>
<li>自定义的Mapper继承父类Mapper, 四个参数分别为输入的k-v(InputFormat生成的kv)，map映射完成后输出的k-v；</li>
<li>Mapper的输入数据是KV格式数据(具体看你使用什么<code>InputFormat</code>)</li>
<li>业务逻辑写入<code>map(xxx)</code>方法</li>
<li>Mapper的输出是KV格式</li>
<li><code>map(xxx)</code>方法(MapTask进程)对每一个KV调用一次</li>
</ol>
<p><strong>Reducer阶段</strong></p>
<ol>
<li>自定义的Reducer继承父类Reducer，参数分别为Mapper的输出和最终输出</li>
<li>Reducer的输入数据是KV格式数据</li>
<li>业务逻辑写入<code>reduce(xxx)</code>方法</li>
<li>ReduceTask进程对每一组相同k的kv组调用一次reduce()方法</li>
</ol>
<p><strong>Driver阶段</strong></p>
<p>相当于YARN集群的客户端，用于提交我们整个程序到Yarn集群，提交的是封装了MapReduce程序相关运行参数的job对象</p>
<hr>
<h3 id="代码展示wordcount"><a href="#代码展示wordcount" class="headerlink" title="代码展示wordcount"></a>代码展示wordcount</h3><ol>
<li>MapReduce程序的流程是，首先根据输入目录/文件，然后通过默认的<code>InputFormat(TextInputFormat)</code>或者指定需要的<code>InputFormat</code>将文件处理为k=每行行首偏移量，value=每行的内容</li>
<li>将<code>InputFormat</code>处理后的KV交给Map Task来处理，Map Task声明时的泛型应该依次为<code>InputFormat</code>的输出KV、Map Task输出的KV</li>
<li>将MapTask处理后的KV交给Reduce Task来执行任务，而Reduce Task声明时候的泛型应该依次为Map Task输出的KV，最终输出的KV，Reduce Task的处理函数<code>reduce</code>的方法参数是MapTask的K和组成迭代的V。</li>
<li>将ReduceTask处理后的KV交给默认的<code>OutputFormat(TextOutputFormat)</code>将KV输入文件。</li>
</ol>
<p><strong>Mapper</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * mr程序中不得使用Java的基础类，得使用mr提供的序列化类</span></span><br><span class="line"><span class="comment"> * 编写Map程序:</span></span><br><span class="line"><span class="comment"> * 1. 继承父类Mapper(), 四个参数分别为输入的k-v，map映射完成后输出的k-v；(TextInputFormat默认)输入的k-v是文件每行的偏移量和每行内容</span></span><br><span class="line"><span class="comment"> * 2. 重写父类的map()方法。context是此次Map程序的上下文。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Map</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> IntWritable num = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Text text = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 每一行数据会调用一次map方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException,</span></span><br><span class="line"><span class="function">            InterruptedException </span>&#123;</span><br><span class="line">        String[] s = value.toString().split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (String s1 : s) &#123;</span><br><span class="line">         <span class="comment">//</span></span><br><span class="line">            text.set(s1);</span><br><span class="line">            context.write(text, num);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>Reducer</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * mr程序中不得使用Java的基础类，得使用mr提供的序列化类</span></span><br><span class="line"><span class="comment"> * 编写Reducer程序:</span></span><br><span class="line"><span class="comment"> * 1. 继承父类Reducer(), 四个参数分别为map输出的k-v，reduce输出的k-v；</span></span><br><span class="line"><span class="comment"> * 2. 重写父类的reduce()方法。context是此次reduce程序的上下文。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Reducer</span> <span class="keyword">extends</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">hadoop</span>.<span class="title">mapreduce</span>.<span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> IntWritable intWritable = <span class="keyword">new</span> IntWritable();</span><br><span class="line">    <span class="comment">// 每一组相同的key会调用一次，value是相同的key的value的组合</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        intWritable.set(sum);</span><br><span class="line">        context.write(key, intWritable);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>Driver</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Driver</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException,</span></span><br><span class="line"><span class="function">            InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//1. 获取一个Job实例</span></span><br><span class="line">        Job job = Job.getInstance(<span class="keyword">new</span> Configuration());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 设置我们的类路径（Classpath）</span></span><br><span class="line">        job.setJarByClass(Driver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 设置Mapper和Reducer</span></span><br><span class="line">        job.setMapperClass(Map.class);</span><br><span class="line">        job.setReducerClass(Reducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4. 设置Mapper和Reducer 输出的类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 更改inputFormat/outputFormat</span></span><br><span class="line"><span class="comment">//        job.setOutputFormatClass(SequenceFileOutputFormat.class);</span></span><br><span class="line"><span class="comment">//        job.setInputFormatClass(TextInputFormat.class);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//5. 设置输入输出数据</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="string">&quot;E:\\input&quot;</span>);</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">&quot;E:\\input874&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//6. 提交我们的Job</span></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<hr>
<h3 id="Driver优化写法"><a href="#Driver优化写法" class="headerlink" title="Driver优化写法"></a>Driver优化写法</h3><p>我们可以使用工具类将Driver类切分为main和tool设置，无需直接在main方法中写入过多的配置：</p>
<p><strong>main</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountApplication</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// ToolRunner可以运行MR</span></span><br><span class="line">        ToolRunner.run(<span class="keyword">new</span> WordCountMRTool(), args);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>Tool</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMRTool</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">    <span class="comment">// strings就是main的args参数</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//1. 获取一个Job实例</span></span><br><span class="line">        Job job = Job.getInstance();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 设置我们的类路径（Classpath）</span></span><br><span class="line">        job.setJarByClass(WordCountMRTool.class);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//3. 设置Mapper和Reducer</span></span><br><span class="line">        job.setMapperClass(Map.class);</span><br><span class="line">        job.setReducerClass(Reducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4. 设置Mapper和Reducer 输出的类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//5. 设置输入输出数据</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="string">&quot;E:\\input&quot;</span>);</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">&quot;E:\\input444&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//6. 提交我们的Job</span></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> b ? JobStatus.State.SUCCEEDED.getValue() : JobStatus.State.FAILED.getValue();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setConf</span><span class="params">(Configuration configuration)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Configuration <span class="title">getConf</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>两种代码本质上是一致的，但是第二种看起来比较好。</p>
<h1 id="MapReduce框架原理"><a href="#MapReduce框架原理" class="headerlink" title="MapReduce框架原理"></a>MapReduce框架原理</h1><h2 id="InputFormat"><a href="#InputFormat" class="headerlink" title="InputFormat"></a>InputFormat</h2><h3 id="FileInputFormat切片"><a href="#FileInputFormat切片" class="headerlink" title="FileInputFormat切片"></a>FileInputFormat切片</h3><p><img src="https://i.loli.net/2020/06/27/jg8ymAwIrU4M5uk.png" alt="image-20200627105459017"></p>
<p><strong>切片与MapTask并行度决定机制</strong></p>
<p><strong>问题引出</strong></p>
<p>思考：1G的数据，启动8个MapTask，可以提高集群的并发处理能力。那么1K的数据，也启动8个MapTask，会提高集群性能吗？MapTask并行任务是否越多越好呢？哪些因素影响了MapTask并行度？       </p>
<p>MapTask并行度决定机制：</p>
<p><strong>数据块：</strong>Block是HDFS物理上把数据分成一块一块。</p>
<p><strong>数据切片：</strong>数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。</p>
<p>一个MapTask处理一个切片，默认切片大小=数据块大小</p>
<p><strong>Job提交流程</strong></p>
<p><img src="https://i.loli.net/2020/06/27/35BAYkgES6qimFW.png" alt="image-20200627105835689"></p>
<h4 id="FileInputFormat切片机制"><a href="#FileInputFormat切片机制" class="headerlink" title="FileInputFormat切片机制"></a>FileInputFormat切片机制</h4><ol>
<li>简单的按照文件内容长度进行切分</li>
<li>切片大小，默认等于Block大小</li>
<li>切片时不考虑数据集整体，而是逐个针对每一个文件进行切分</li>
</ol>
<p>FileInputFormat的实现类有：<code>TextInputFormat</code>、<code>KeyValueTextInputFormat</code>、<code>NLineInputFormat</code>、<code>CombineTextInputFormat</code>和自定义InputFormat等。</p>
<p>上述各个子类有的使用了FileInputFormat的切片机制(<code>TextInputFormat</code>、<code>KeyValueTextInputFormat</code>、<code>NLineInputFormat</code>)，特别的<code>CombineTextInputFormat</code>[自定义了一套切片机制](# CombineTextInputFormat切片机制)。</p>
<h4 id="CombineTextInputFormat切片机制"><a href="#CombineTextInputFormat切片机制" class="headerlink" title="CombineTextInputFormat切片机制"></a>CombineTextInputFormat切片机制</h4><p>框架默认的TextInputFormat切片机制是对任务按文件规划切片，<font color="red">不管文件多小，都会是一个单独的切片</font>，都会交给一个MapTask，这样如果有大量小文件，就<font color="red">会产生大量的MapTask</font>，处理效率极其低下。</p>
<p><strong>应用场景</strong></p>
<p>CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理。</p>
<p><strong>虚拟存储切片最大值设置</strong></p>
<p>CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m</p>
<p>注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。</p>
<p><strong>切片机制</strong></p>
<p><img src="https://i.loli.net/2020/06/27/6EzDF5hqc8LIxwU.png" alt="image-20200627114202060"></p>
<p><strong>虚拟存储过程：</strong></p>
<p>将输入目录下所有文件大小，依次和设置的setMaxInputSplitSize值比较，如果不大于设置的最大值，逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块；<font color="red">当剩余数据大小超过设置的最大值且不大于最大值2倍，此时将文件均分成2个虚拟存储块（防止出现太小切片）。</font></p>
<p>例如setMaxInputSplitSize值为4M，输入文件大小为8.02M，则先逻辑上分成一个4M。剩余的大小为4.02M，如果按照4M逻辑划分，就会出现0.02M的小的虚拟存储文件，所以将剩余的4.02M文件切分成（2.01M和2.01M）两个文件。</p>
<p><strong>切片过程：</strong></p>
<p>判断虚拟存储的文件大小是否大于setMaxInputSplitSize值，大于等于则单独形成一个切片。</p>
<p>如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片。</p>
<p><strong>测试举例</strong></p>
<p>有4个小文件大小分别为1.7M、5.1M、3.4M以及6.8M这四个小文件</p>
<p>则虚拟存储之后形成6个文件块，大小分别为：</p>
<p>1.7M，（2.55M、2.55M），3.4M以及（3.4M、3.4M）</p>
<p>最终会形成3个切片，大小分别为：</p>
<p>（1.7+2.55）M，（2.55+3.4）M，（3.4+3.4）M</p>
<h3 id="FileInputFormat的RecordReader生成K-V"><a href="#FileInputFormat的RecordReader生成K-V" class="headerlink" title="FileInputFormat的RecordReader生成K-V"></a>FileInputFormat的RecordReader生成K-V</h3><p>FileInputFormat的实现类有：<code>TextInputFormat</code>、<code>KeyValueTextInputFormat</code>、<code>NLineInputFormat</code>、<code>CombineTextInputFormat</code>和自定义InputFormat等。</p>
<p>上述各个子类有的使用了FileInputFormat的切片机制(<code>TextInputFormat</code>、<code>KeyValueTextInputFormat</code>、<code>NLineInputFormat</code>)，特别的<code>CombineTextInputFormat</code>[自定义了一套切片机制](# CombineTextInputFormat切片机制)。</p>
<h4 id="KeyValueTextInputFormat的RecordReader生成K-V"><a href="#KeyValueTextInputFormat的RecordReader生成K-V" class="headerlink" title="KeyValueTextInputFormat的RecordReader生成K-V"></a>KeyValueTextInputFormat的RecordReader生成K-V</h4><p>每一行均为一条记录，被分割符分割为key，value。可以通过在驱动类中设置分隔符。默认分隔符<code>\t</code>。</p>
<p><img src="https://i.loli.net/2020/06/27/uhMNsVke6iFP8oO.png" alt="image-20200627115321246"></p>
<h4 id="TextInputFormat的RecordReader生成K-V"><a href="#TextInputFormat的RecordReader生成K-V" class="headerlink" title="TextInputFormat的RecordReader生成K-V"></a>TextInputFormat的RecordReader生成K-V</h4><p><font color="red"><code>TextInputFormat</code>是默认的FileInputFormat实现类。按行读取每条记录。</font>建是存储该行在整个文件中的起始字节偏移量，<code>LongWritable</code>类型；值是这行的内容，不包括任何行终止符(换行符和回车符)，<code>Text</code>类型。</p>
<p><img src="https://i.loli.net/2020/06/27/gAshOJRpqN8YM7E.png" alt="image-20200627120055534"></p>
<h4 id="NLineInputFormat的RecordReader生成K-V"><a href="#NLineInputFormat的RecordReader生成K-V" class="headerlink" title="NLineInputFormat的RecordReader生成K-V"></a>NLineInputFormat的RecordReader生成K-V</h4><p>如果使用<code>NLineInputFormat</code>，代表每个map进程处理的<code>InputSpilt不再按照Block块去划分</code>，而是按照<code>NLineInputFormat</code>指定的行数来划分的。即输入文件的总行数/N=切片数，如果不整除，切片数=商+1.</p>
<p><img src="https://i.loli.net/2020/06/27/NFaighH4XodG8tk.png" alt="image-20200627121035864"></p>
<h3 id="自定义InputFormat🔺"><a href="#自定义InputFormat🔺" class="headerlink" title="自定义InputFormat🔺"></a>自定义InputFormat🔺</h3><p>在企业发开中，Hadoop自带的InputFormat类型不能满足所有应用场景，需要自定义来解决实际问题。</p>
<p><strong>步骤：</strong></p>
<ol>
<li>自定义一个类继承<code>FileInputFormat</code></li>
<li>自定义一个类继承<code>RecordReader</code>，实现读取切片形成K-V</li>
<li>如果想改变切片机制，改写<code>WriteSpilts</code>方法</li>
<li>切片机制中需要先判断文件是否支持切分，是否支持切分是这个函数<code>isSpiltable()</code></li>
<li>Driver类中设置使用自定义的InputFormat</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setInputFormatClass(xxxx.class);</span><br></pre></td></tr></table></figure>

<h2 id="MapReduce工作流程"><a href="#MapReduce工作流程" class="headerlink" title="MapReduce工作流程"></a>MapReduce工作流程</h2><p><img src="https://i.loli.net/2020/06/27/qv7MoCncyVRNKzd.png" alt="image-20200627121618614"></p>
<p><img src="https://i.loli.net/2020/06/27/jG2CRhwftTd1Nzv.png" alt="image-20200627121633037"></p>
<p>上面的流程是整个MapReduce最全工作流程，但是Shuffle过程只是从第7步开始到第16步结束，具体Shuffle过程详解，如下：</p>
<ol>
<li>MapTask收集我们的map()方法输出的kv对，放到环形内存缓冲区中(此时会计算分区号)。</li>
<li>从内存缓冲区不断溢出本地磁盘文件，可能会溢出(&gt;80M时)多个文件。此时如果设置了Combiner则排序完成后还会进行Combiner</li>
<li>多个溢出文件会被合并成大的溢出文件，归并排序按照分区号和key值，这样保证分区数据被特定的ReduceTask处理，并且根据Key分组。此时如果设置了Combiner则排序完成后还会进行Combiner</li>
<li>在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序</li>
<li>ReduceTask根据自己的分区号，去每个MapTask机器上取相应的结果分区数据</li>
<li>ReduceTask会取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并（归并排序）</li>
<li>合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程（从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法）</li>
</ol>
<h2 id="Shuffle机制"><a href="#Shuffle机制" class="headerlink" title="Shuffle机制"></a>Shuffle机制</h2><p><strong>Shuffle</strong>就是将每个Map输出的数据进行规整，同同一分区的数据都发送到指定的ReduceTask中去处理。</p>
<p>Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle。如图所示。</p>
<p><img src="https://i.loli.net/2020/06/27/HrkfelLnKDcav4B.png" alt="Snipaste_2020-06-27_12-43-49"></p>
<ol>
<li>MapTask通过<code>context</code>上下文写入时，写到了环形缓冲区。一进入缓冲区就会对KV进行分区，默认分区是<code>HashPartitoner</code>，成为K-V-P</li>
<li>环形缓冲区的数据会对KVP进行快速排序，根据P-K二次排序</li>
<li>环形缓冲区的数据大于80M时，会溢写到磁盘中。如果此时设置了Combiner(实质Reducer)的话，会先执行Combiner后溢写</li>
<li>多个溢写的小文件会进行归并排序形成一个MapTask的最终文件。也是根据P-K来二次排序。如果此时设置了Combiner(实质Reducer)的话，也会执行Combiner，同时进行</li>
<li>最后ReducerTask端会访问所有MapTask的结果文件，取到此ReduceTask需要处理的分区文件</li>
<li>对多个次分区的文件进行归并排序，根据K，因为ReduceTask里的数据全是同一分区的，然后Reduce端就得到了根据可以排好序的数据</li>
<li>然而，我们ReducerTask的处理粒度是k-Itorable<V>，所以ReduceTask还需要对key进行分组。</li>
<li>这里我们可是使用默认的，就是值相同为一组；也可以自定义<code>GroupingComparator</code>设置分组规则。</li>
<li>分组实上就是一个比较器，如果第一个和第二个相同就归为一组，否则不同组。为什么这样就可以？因为我们的Key是已经排好序的，这样key相同的数据必然是连续的。</li>
<li>执行ReduceTask的业务逻辑</li>
<li>多个ReduceTask的结果进行汇总，得到最总数据！</li>
</ol>
<h3 id="Partiton分区"><a href="#Partiton分区" class="headerlink" title="Partiton分区"></a>Partiton分区</h3><p> 前面说到MapTask的并行度由数据的切片数决定。</p>
<p>而ReduceTask的并行度由Partion分区决定，必须通过在Driver驱动类手动设置或者修改默认配置。</p>
<p>为什么说ReduceTask的并行度由Partion分区决定呢？</p>
<p>如果Partion分区器给你分了5个区，而你没有设置ReduceTask的并行度(默认1)，就会报错！当然ReduceTask的并行度可以大约5个区，但是最总结果只有5个区有数据，多余的区不会用到，浪费。所以ReduceTask的并行度=Partion分区器分的区。<font color="red">其中Partion分区器分的区的区号，<strong>必须是从0开始，逐一递增！！</strong></font>因为我们手动设置ReduceTask的并行度为5时，它产生的分区就是0-4。</p>
<p>ps：分区器分了几个区，最终结果就有几个文件有内容。</p>
<h4 id="默认分区器"><a href="#默认分区器" class="headerlink" title="默认分区器"></a>默认分区器</h4><p><img src="https://i.loli.net/2020/06/27/V51zjlWTCtGd3Dh.png" alt="image-20200627130939155"></p>
<h4 id="自定义分区器🔺"><a href="#自定义分区器🔺" class="headerlink" title="自定义分区器🔺"></a>自定义分区器🔺</h4><ol>
<li>自定义类继承<code>Partitioner</code>，重写<code>getPartition()</code></li>
<li>Job驱动中，设置自定义Partitoner；<code>job.setPartitionerClass(xxx.class)</code></li>
<li>自定义<code>Partitioner</code>后，需要根据它的逻辑来设置相应数量的ReduceTask；<code>job.setNumReduceTasks(n)</code></li>
</ol>
<h4 id="分区总结"><a href="#分区总结" class="headerlink" title="分区总结"></a>分区总结</h4><ol>
<li>如果ReduceTask的数量&gt;getPartiton的结果数，则会多产生几个空的输出文件part-r-000xx;</li>
<li>如果1&lt;ReduceTask的数量&lt;getPartiton的结果数，则有一部分分区数据无法安放，会Exception</li>
<li>如果1=ReduceTask的数量，不管多少个分区，最后产生一个ReduceTask，最终一个结果文件</li>
<li>分区号必须从0开始，逐一累加</li>
</ol>
<h3 id="WritableComparable排序"><a href="#WritableComparable排序" class="headerlink" title="WritableComparable排序"></a>WritableComparable排序</h3><p>排序时MapReduce框架中最重要的操作之一。是ReduceTask能<strong>把相同Key分组这个操作的基石</strong>！！</p>
<p>因为ReduceTask的分组不是一个个分的，因为大数据情况下很慢。还可能会爆掉。Hadoop通过三次排序(环形缓冲区时、溢写大文件时、ReduceTask从各个MapTask获取特定分区数据时)，当ReduceTask获取到数据时，数据的顺序是根据key排好的(相同key比相邻)，所以只需要编写一个key的比较器即可，如果比较器返回0，则归为同一组，否则不同组</p>
<p>默认排序是按照字典顺序排序的，如果你的key是一个bean，那么你就必须得在bean中指定bean的排序规则，然后在GroupingComparator中指定分组规则。排序规则必须包含分组规则，而且先按照分组规则排一下序，再排其他。否则就会导致ReduceTask分组效果失败。</p>
<h4 id="排序的分类"><a href="#排序的分类" class="headerlink" title="排序的分类"></a>排序的分类</h4><p><img src="https://i.loli.net/2020/06/27/MuGgZNohnFyT57c.png" alt="image-20200627133002336"></p>
<h4 id="自定义排序WritableComparable🔺"><a href="#自定义排序WritableComparable🔺" class="headerlink" title="自定义排序WritableComparable🔺"></a>自定义排序WritableComparable🔺</h4><p>bean对象做为key传输，需要实现WritableComparable接口重写compareTo方法，就可以实现排序。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean o)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> result;</span><br><span class="line">        </span><br><span class="line">    <span class="comment">// 按照总流量大小，倒序排列</span></span><br><span class="line">    <span class="keyword">if</span> (sumFlow &gt; bean.getSumFlow()) &#123;</span><br><span class="line">        result = -<span class="number">1</span>;</span><br><span class="line">    &#125;<span class="keyword">else</span> <span class="keyword">if</span> (sumFlow &lt; bean.getSumFlow()) &#123;</span><br><span class="line">        result = <span class="number">1</span>;</span><br><span class="line">    &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">        result = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="GroupingComparator分组（辅助排序）"><a href="#GroupingComparator分组（辅助排序）" class="headerlink" title="GroupingComparator分组（辅助排序）"></a>GroupingComparator分组（辅助排序）</h3><p>上面说到bean作为key时候的排序，那么GroupingComparator就是ReduceTask端bean做为key时的分组(排序)</p>
<p>Hadoop通过三次排序(环形缓冲区时、溢写大文件时、ReduceTask从各个MapTask获取特定分区数据时)，当ReduceTask获取到数据时，数据的顺序是根据key排好的(相同key比相邻)，所以只需要编写一个key的比较器即可，如果比较器返回0，则归为同一组，否则不同组</p>
<h4 id="自定义分组🔺"><a href="#自定义分组🔺" class="headerlink" title="自定义分组🔺"></a>自定义分组🔺</h4><ol>
<li>作为key的bean必须实现了<code>WritableComparable</code>接口，并实现了相关方法</li>
<li>编写分组排序类继承<code>WritableComparator</code>，编写key的bean类型的(分组)比较方法</li>
<li>分组排序类额外需要创建一个构造将比较对象的类传给父类🔺</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="title">xxx</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(OrderBean.class, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol start="4">
<li>Driver驱动类中<code>job.setGroupingComparatorClass(xxx.class)</code></li>
</ol>
<h3 id="Combiner"><a href="#Combiner" class="headerlink" title="Combiner"></a>Combiner</h3><p><img src="https://i.loli.net/2020/06/27/5pkt1imEhJ2ZVDv.png" alt="image-20200627133822658"></p>
<h4 id="自定义Combiner"><a href="#自定义Combiner" class="headerlink" title="自定义Combiner"></a>自定义Combiner</h4><ol>
<li>和Reduce一样，继承Reducer，重写Reduce方法</li>
<li>在Job驱动类中设置：<code>job.setCombinerClass(WordcountCombiner.class)</code></li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Xiangjie</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://awslzhang.top/2020/06/26/MapReduce%E5%8E%9F%E7%90%86/">https://awslzhang.top/2020/06/26/MapReduce%E5%8E%9F%E7%90%86/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://awslzhang.top" target="_blank">zxj</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Hadoop/">Hadoop</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/06/27/2xbLZqNQKHGoUu1.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/07/02/Hive%E6%A6%82%E8%BF%B0/"><img class="prev-cover" src="https://i.loli.net/2020/07/02/LEKTZ49OFH5Jo3C.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Hive概述</div></div></a></div><div class="next-post pull-right"><a href="/2020/06/22/HDFS%E6%A6%82%E8%BF%B0/"><img class="next-cover" src="https://i.loli.net/2020/06/22/xIQlAhrgUHqSa27.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">HDFS概述</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/07/15/Oozie调度/" title="Oozie调度"><img class="cover" src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/Oozie-logo.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-15</div><div class="title">Oozie调度</div></div></a></div><div><a href="/2020/07/14/Sqoop的简单使用/" title="Sqoop的简单使用"><img class="cover" src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/sqoop-logo.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-14</div><div class="title">Sqoop的简单使用</div></div></a></div><div><a href="/2020/07/04/Flume基本使用/" title="Flume基本使用"><img class="cover" src="https://i.loli.net/2020/07/04/FjyZ6DdUigWebJn.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-04</div><div class="title">Flume基本使用</div></div></a></div><div><a href="/2020/06/22/HDFS概述/" title="HDFS概述"><img class="cover" src="https://i.loli.net/2020/06/22/xIQlAhrgUHqSa27.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-06-22</div><div class="title">HDFS概述</div></div></a></div><div><a href="/2020/06/22/Hadoop编译安装/" title="Hadoop编译安装"><img class="cover" src="https://i.loli.net/2020/06/22/Wb7kUVafmZSOudC.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-06-22</div><div class="title">Hadoop编译安装</div></div></a></div><div><a href="/2020/07/02/Hive概述/" title="Hive概述"><img class="cover" src="https://i.loli.net/2020/07/02/LEKTZ49OFH5Jo3C.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-02</div><div class="title">Hive概述</div></div></a></div></div></div></div><div class="aside_content" id="aside_content"><div class="card-widget card-info"><div class="card-content"><div class="card-info-avatar is-center"><img class="avatar-img" src="/null" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Xiangjie</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">85</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">43</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">25</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/XiangJie-Zhang" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:qluzxj@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div></div><div class="card-widget card-announcement"><div class="card-content"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="card-content"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#MapReduce%E6%A6%82%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">MapReduce概述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#MapReduce%E5%AE%9A%E4%B9%89"><span class="toc-number">1.1.</span> <span class="toc-text">MapReduce定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MapReduce%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">1.2.</span> <span class="toc-text">MapReduce优缺点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MapReudce%E6%B5%81%E7%A8%8B"><span class="toc-number">1.3.</span> <span class="toc-text">MapReudce流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MapReduce%E8%BF%9B%E7%A8%8B"><span class="toc-number">1.4.</span> <span class="toc-text">MapReduce进程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MapReduce%E5%BA%8F%E5%88%97%E5%8C%96"><span class="toc-number">1.5.</span> <span class="toc-text">MapReduce序列化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89bean%E5%AF%B9%E8%B1%A1%E5%AE%9E%E7%8E%B0%E5%BA%8F%E5%88%97%E5%8C%96%E6%8E%A5%E5%8F%A3%EF%BC%88Writable%EF%BC%89"><span class="toc-number">1.5.1.</span> <span class="toc-text">自定义bean对象实现序列化接口（Writable）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MapReduce%E7%BC%96%E7%A8%8B%E8%A7%84%E8%8C%83"><span class="toc-number">1.6.</span> <span class="toc-text">MapReduce编程规范</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%B1%95%E7%A4%BAwordcount"><span class="toc-number">1.6.1.</span> <span class="toc-text">代码展示wordcount</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Driver%E4%BC%98%E5%8C%96%E5%86%99%E6%B3%95"><span class="toc-number">1.6.2.</span> <span class="toc-text">Driver优化写法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#MapReduce%E6%A1%86%E6%9E%B6%E5%8E%9F%E7%90%86"><span class="toc-number">2.</span> <span class="toc-text">MapReduce框架原理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#InputFormat"><span class="toc-number">2.1.</span> <span class="toc-text">InputFormat</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#FileInputFormat%E5%88%87%E7%89%87"><span class="toc-number">2.1.1.</span> <span class="toc-text">FileInputFormat切片</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#FileInputFormat%E5%88%87%E7%89%87%E6%9C%BA%E5%88%B6"><span class="toc-number">2.1.1.1.</span> <span class="toc-text">FileInputFormat切片机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#CombineTextInputFormat%E5%88%87%E7%89%87%E6%9C%BA%E5%88%B6"><span class="toc-number">2.1.1.2.</span> <span class="toc-text">CombineTextInputFormat切片机制</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#FileInputFormat%E7%9A%84RecordReader%E7%94%9F%E6%88%90K-V"><span class="toc-number">2.1.2.</span> <span class="toc-text">FileInputFormat的RecordReader生成K-V</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#KeyValueTextInputFormat%E7%9A%84RecordReader%E7%94%9F%E6%88%90K-V"><span class="toc-number">2.1.2.1.</span> <span class="toc-text">KeyValueTextInputFormat的RecordReader生成K-V</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#TextInputFormat%E7%9A%84RecordReader%E7%94%9F%E6%88%90K-V"><span class="toc-number">2.1.2.2.</span> <span class="toc-text">TextInputFormat的RecordReader生成K-V</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#NLineInputFormat%E7%9A%84RecordReader%E7%94%9F%E6%88%90K-V"><span class="toc-number">2.1.2.3.</span> <span class="toc-text">NLineInputFormat的RecordReader生成K-V</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89InputFormat%F0%9F%94%BA"><span class="toc-number">2.1.3.</span> <span class="toc-text">自定义InputFormat🔺</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MapReduce%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-number">2.2.</span> <span class="toc-text">MapReduce工作流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Shuffle%E6%9C%BA%E5%88%B6"><span class="toc-number">2.3.</span> <span class="toc-text">Shuffle机制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Partiton%E5%88%86%E5%8C%BA"><span class="toc-number">2.3.1.</span> <span class="toc-text">Partiton分区</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%BB%98%E8%AE%A4%E5%88%86%E5%8C%BA%E5%99%A8"><span class="toc-number">2.3.1.1.</span> <span class="toc-text">默认分区器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%86%E5%8C%BA%E5%99%A8%F0%9F%94%BA"><span class="toc-number">2.3.1.2.</span> <span class="toc-text">自定义分区器🔺</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E6%80%BB%E7%BB%93"><span class="toc-number">2.3.1.3.</span> <span class="toc-text">分区总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#WritableComparable%E6%8E%92%E5%BA%8F"><span class="toc-number">2.3.2.</span> <span class="toc-text">WritableComparable排序</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8E%92%E5%BA%8F%E7%9A%84%E5%88%86%E7%B1%BB"><span class="toc-number">2.3.2.1.</span> <span class="toc-text">排序的分类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8E%92%E5%BA%8FWritableComparable%F0%9F%94%BA"><span class="toc-number">2.3.2.2.</span> <span class="toc-text">自定义排序WritableComparable🔺</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GroupingComparator%E5%88%86%E7%BB%84%EF%BC%88%E8%BE%85%E5%8A%A9%E6%8E%92%E5%BA%8F%EF%BC%89"><span class="toc-number">2.3.3.</span> <span class="toc-text">GroupingComparator分组（辅助排序）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%86%E7%BB%84%F0%9F%94%BA"><span class="toc-number">2.3.3.1.</span> <span class="toc-text">自定义分组🔺</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Combiner"><span class="toc-number">2.3.4.</span> <span class="toc-text">Combiner</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89Combiner"><span class="toc-number">2.3.4.1.</span> <span class="toc-text">自定义Combiner</span></a></li></ol></li></ol></li></ol></li></ol></div></div></div><div class="card-widget card-recent-post"><div class="card-content"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/01/10/Api%E5%92%8CSQL/" title="Api和SQL"><img src="https://flink.apache.org/img/flink-header-logo.svg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Api和SQL"/></a><div class="content"><a class="title" href="/2021/01/10/Api%E5%92%8CSQL/" title="Api和SQL">Api和SQL</a><time datetime="2021-01-10T03:51:40.000Z" title="发表于 2021-01-10 11:51:40">2021-01-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/01/02/Flink%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86/" title="Flink状态编程和容错机制"><img src="https://flink.apache.org/img/flink-header-logo.svg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Flink状态编程和容错机制"/></a><div class="content"><a class="title" href="/2021/01/02/Flink%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86/" title="Flink状态编程和容错机制">Flink状态编程和容错机制</a><time datetime="2021-01-02T10:06:02.000Z" title="发表于 2021-01-02 18:06:02">2021-01-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/23/Flink%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89%E5%92%8CWaterMark/" title="Flink的时间语义和watermark"><img src="https://flink.apache.org/img/flink-header-logo.svg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Flink的时间语义和watermark"/></a><div class="content"><a class="title" href="/2020/12/23/Flink%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89%E5%92%8CWaterMark/" title="Flink的时间语义和watermark">Flink的时间语义和watermark</a><time datetime="2020-12-23T10:59:07.000Z" title="发表于 2020-12-23 18:59:07">2020-12-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/17/Flink-API/" title="Flink Api学习"><img src="https://flink.apache.org/img/flink-header-logo.svg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Flink Api学习"/></a><div class="content"><a class="title" href="/2020/12/17/Flink-API/" title="Flink Api学习">Flink Api学习</a><time datetime="2020-12-17T12:36:28.000Z" title="发表于 2020-12-17 20:36:28">2020-12-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/17/ready/" title="ready!"><img src="https://images.pexels.com/photos/924824/pexels-photo-924824.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ready!"/></a><div class="content"><a class="title" href="/2020/12/17/ready/" title="ready!">ready!</a><time datetime="2020-12-16T16:02:06.000Z" title="发表于 2020-12-17 00:02:06">2020-12-17</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Xiangjie</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script></div></body></html>