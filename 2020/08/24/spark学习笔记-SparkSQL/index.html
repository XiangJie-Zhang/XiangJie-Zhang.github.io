<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>spark学习笔记-SparkSQL | zxj</title><meta name="keywords" content="Spark"><meta name="author" content="Xiangjie"><meta name="copyright" content="Xiangjie"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="Spark SQL概述什么是Spark SQLpark SQL是Spark用来处理结构化数据的一个模块，它提供了2个编程抽象：DataFrame和DataSet，并且作为分布式SQL查询引擎的作用。 将Spark SQL转换成RDD，然后提交到集群执行，执行效率非常快！ Spark SQL的特点 易整合 统一的数据访问方式 兼容Hive 标准的数据连接  什么是DataFrame与RDD类似，Da">
<meta property="og:type" content="article">
<meta property="og:title" content="spark学习笔记-SparkSQL">
<meta property="og:url" content="https://awslzhang.top/2020/08/24/spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-SparkSQL/index.html">
<meta property="og:site_name" content="zxj">
<meta property="og:description" content="Spark SQL概述什么是Spark SQLpark SQL是Spark用来处理结构化数据的一个模块，它提供了2个编程抽象：DataFrame和DataSet，并且作为分布式SQL查询引擎的作用。 将Spark SQL转换成RDD，然后提交到集群执行，执行效率非常快！ Spark SQL的特点 易整合 统一的数据访问方式 兼容Hive 标准的数据连接  什么是DataFrame与RDD类似，Da">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://spark.apache.org/images/spark-logo-trademark.png">
<meta property="article:published_time" content="2020-08-24T13:10:34.000Z">
<meta property="article:modified_time" content="2021-01-01T05:50:00.043Z">
<meta property="article:author" content="Xiangjie">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://spark.apache.org/images/spark-logo-trademark.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://awslzhang.top/2020/08/24/spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-SparkSQL/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: Xiangjie","link":"链接: ","source":"来源: zxj","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}

// https://stackoverflow.com/questions/16839698/jquery-getscript-alternative-in-native-javascript
const getScript = url => new Promise((resolve, reject) => {
  const script = document.createElement('script')
  script.src = url
  script.async = true
  script.onerror = reject
  script.onload = script.onreadystatechange = function() {
    const loadState = this.readyState
    if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
    script.onload = script.onreadystatechange = null
    resolve()
  }
  document.head.appendChild(script)
})</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2021-01-01 13:50:00'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
   if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }
  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified
    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }const asideStatus = saveToLocal.get('aside-status')
if (asideStatus !== undefined) {
   if (asideStatus === 'hide') {
     document.documentElement.classList.add('hide-aside')
   } else {
     document.documentElement.classList.remove('hide-aside')
   }
}})()</script><meta name="generator" content="Hexo 5.0.2"><link rel="alternate" href="/atom.xml" title="zxj" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/null" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">85</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">43</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">25</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 目录</span></a></div></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://spark.apache.org/images/spark-logo-trademark.png)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">zxj</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 目录</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">spark学习笔记-SparkSQL</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-08-24T13:10:34.000Z" title="发表于 2020-08-24 21:10:34">2020-08-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-01-01T05:50:00.043Z" title="更新于 2021-01-01 13:50:00">2021-01-01</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Spark/">Spark</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>23分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Spark-SQL概述"><a href="#Spark-SQL概述" class="headerlink" title="Spark SQL概述"></a>Spark SQL概述</h1><h2 id="什么是Spark-SQL"><a href="#什么是Spark-SQL" class="headerlink" title="什么是Spark SQL"></a>什么是Spark SQL</h2><p>park SQL是Spark用来处理结构化数据的一个模块，它提供了2个编程抽象：<font color="red">DataFrame和DataSet</font>，并且作为分布式SQL查询引擎的作用。</p>
<p>将Spark SQL转换成RDD，然后提交到集群执行，执行效率非常快！</p>
<h2 id="Spark-SQL的特点"><a href="#Spark-SQL的特点" class="headerlink" title="Spark SQL的特点"></a>Spark SQL的特点</h2><ul>
<li>易整合</li>
<li>统一的数据访问方式</li>
<li>兼容Hive</li>
<li>标准的数据连接</li>
</ul>
<h2 id="什么是DataFrame"><a href="#什么是DataFrame" class="headerlink" title="什么是DataFrame"></a>什么是DataFrame</h2><p>与RDD类似，DataFrame也是一个分布式数据容器。然而DataFrame更像传统数据库的二维表格，除了数据以外，<strong>还记录数据的结构信息，即schema</strong>。同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从API易用性的角度上看，DataFrame API提供的是一套高层的关系操作，比函数式的RDD API要更加友好，门槛更低。</p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200824211402757.png" alt="image-20200824211402757"></p>
<p>上图直观地体现了DataFrame和RDD的区别。左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame是为数据提供了Schema的视图。可以把它当做数据库中的一张表来对待，DataFrame也是懒执行的。性能上比RDD要高，主要原因：</p>
<p><font color="red">优化的执行计划：查询计划通过Spark catalyst optimiser进行优化。</font></p>
<p><font color="red">但是DataFrame只存储了结构信息，即在每一行中，只能通过索引来访问元素，并不能通过类型访问。因为DataFrame不保存类型信息。</font></p>
<h2 id="什么是DataSet"><a href="#什么是DataSet" class="headerlink" title="什么是DataSet"></a>什么是DataSet</h2><ol>
<li>是Dataframe API的一个扩展，是Spark最新的数据抽象。</li>
<li>用户友好的API风格，既具有类型安全检查也具有Dataframe的查询优化特性。</li>
<li>Dataset支持编解码器，当需要访问非堆上的数据时可以避免反序列化整个对象，提高了效率。</li>
<li><font color="red">样例类被用来在Dataset中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet中的字段名称。</font></li>
<li>Dataframe是Dataset的特列，DataFrame=Dataset[Row] ，所以可以通过as方法将Dataframe转换为Dataset。Row是一个类型，跟Car、Person这些的类型一样，所有的表结构信息我都用Row来表示。</li>
<li><font color="red"><strong>DataSet是强类型的</strong>。比如可以有Dataset[Car]，Dataset[Person].</font></li>
<li><font color="red">DataFrame只是知道字段，但是不知道字段的类型，所以在执行这些操作的时候是没办法在编译的时候检查是否类型失败的，比如你可以对一个String进行减法操作，在执行的时候才报错</font>，而DataSet不仅仅知道字段，<strong>而且知道字段类型，所以有更严格的错误检查。就跟JSON对象和类对象之间的类比</strong>。</li>
</ol>
<h2 id="RDD、DataFrame、DataSet🔺"><a href="#RDD、DataFrame、DataSet🔺" class="headerlink" title="RDD、DataFrame、DataSet🔺"></a>RDD、DataFrame、DataSet🔺</h2><p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200824213514197.png" alt="image-20200824213514197"></p>
<ul>
<li>RDD：没有结构和类型</li>
<li>DataFrame：有结构没类型。所以由DF变为RDD时，RDD存储的是ROW对象，此时只能通过索引获取值，因为它不存储类型</li>
<li>DataSet：有结构和类型。所以DS变RDD时，RDD存储的是确定的类，可以通过类访问确定类型的属性</li>
</ul>
<p>由于上述三者的关系，我们可以确定：</p>
<ol>
<li>RDD转换DF时，需要说明结构</li>
<li>RDD转换DS时，需要说明结构和类型。(类同时包含结构和类型)</li>
<li>DF转RDD时，直接转换，RDD存储ROW</li>
<li>DS转RDD时，直接转换，RDD存储具体的类</li>
</ol>
<h1 id="SparkSQL编程"><a href="#SparkSQL编程" class="headerlink" title="SparkSQL编程"></a>SparkSQL编程</h1><h2 id="SparkSession新的起始点"><a href="#SparkSession新的起始点" class="headerlink" title="SparkSession新的起始点"></a>SparkSession新的起始点</h2><p>在老的版本中，SparkSQL提供两种SQL查询起始点：一个叫<code>SQLContext</code>，用于Spark自己提供的SQL查询；一个叫<code>HiveContext</code>，用于连接Hive的查询。</p>
<p>SparkSession是Spark最新的SQL查询起始点，实质上是SQLContext和HiveContext的组合，所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。<font color="red">**<code>SparkSession</code>内部封装了<code>sparkContext</code>，所以计算实际上是由sparkContext完成的。**</font></p>
<h2 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h2><p>DataFrame只会知道数据的整体结构，当你查询一个列名<code>name</code>时<strong>，此时它并不知道它的类型，如果你进行操作编译时不会出错，当运行时才有可能出错。</strong></p>
<h3 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h3><p>在Spark SQL中<code>SparkSession</code>是创建<code>DataFrame</code>和执行SQL<strong>的入口</strong>，创建DataFrame有三种方式：</p>
<ul>
<li>通过Spark的数据源进行创建；</li>
<li>从一个存在的RDD进行转换；</li>
<li>还可以从Hive Table进行查询返回。</li>
</ul>
<h4 id="Spark数据源创建"><a href="#Spark数据源创建" class="headerlink" title="Spark数据源创建"></a><strong>Spark数据源创建</strong></h4><p>查看Spark数据源进行创建的文件格式</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.read.</span><br><span class="line">csv   format   jdbc   json   load   option   options   orc   parquet   schema   table   text   textFile</span><br></pre></td></tr></table></figure>

<p><strong>读取json文件创建DataFrame</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;file:///home/hadoop/test.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">scala&gt; df.show</span><br><span class="line">+---+--------+</span><br><span class="line">|age|    name|</span><br><span class="line">+---+--------+</span><br><span class="line">| <span class="number">20</span>|zhangsan|</span><br><span class="line">| <span class="number">40</span>|    lisi|</span><br><span class="line">+---+--------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>

<h4 id="RDD创建"><a href="#RDD创建" class="headerlink" title="RDD创建"></a>RDD创建</h4><p><a href="#RDD%E8%BD%ACDataFrame">点击跳转</a></p>
<h3 id="SQL风格语法🔺"><a href="#SQL风格语法🔺" class="headerlink" title="SQL风格语法🔺"></a>SQL风格语法🔺</h3><p>如果想以SQL形式读取数据，就必须有表，上面我们创建的对象<code>df</code>并不是表明，我们需要通过这个对象来创建一个临时视图<font color="red">，既然是视图，必然只可以查询。</font>然后就可以通过视图名来编写SQL来查询了！！</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.create</span><br><span class="line">createGlobalTempView   createOrReplaceTempView   createTempView</span><br><span class="line"></span><br><span class="line">scala&gt; df.createTempView(<span class="string">&quot;table&quot;</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;select * from table where age=40&quot;</span>).show</span><br><span class="line">+---+----+</span><br><span class="line">|age|name|</span><br><span class="line">+---+----+</span><br><span class="line">| <span class="number">40</span>|lisi|</span><br><span class="line">+---+----+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>

<p><font color="red"><strong>注意</strong>：临时表是Session范围内的，Session退出后，表就失效了。</font>如果想应用范围内有效，可以使用全局表。注意使用全局表时需要全路径访问，如：global_temp.people</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.newSession.sql(<span class="string">&quot;select * from table where age=40&quot;</span>).show <span class="comment">// 错误</span></span><br><span class="line">scala&gt; df.createGlobalTempView(<span class="string">&quot;tmp&quot;</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; spark.newSession.sql(<span class="string">&quot;select * from global_temp.tmp&quot;</span>).show</span><br><span class="line">+---+--------+</span><br><span class="line">|age|    name|</span><br><span class="line">+---+--------+</span><br><span class="line">| <span class="number">20</span>|zhangsan|</span><br><span class="line">| <span class="number">40</span>|    lisi|</span><br><span class="line">+---+--------+</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>如果使用全局表的话，无论哪个Session环境都必须使用<code>global_temp.xxx</code>形式访问。</p>
<h3 id="DSL风格语法"><a href="#DSL风格语法" class="headerlink" title="DSL风格语法"></a>DSL风格语法</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.select(<span class="string">&quot;name&quot;</span>).show</span><br><span class="line">+--------+</span><br><span class="line">|    name|</span><br><span class="line">+--------+</span><br><span class="line">|zhangsan|</span><br><span class="line">|    lisi|</span><br><span class="line">+--------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; df.select($<span class="string">&quot;age&quot;</span>+<span class="number">1</span>).show</span><br><span class="line">+---------+</span><br><span class="line">|(age + <span class="number">1</span>)|</span><br><span class="line">+---------+</span><br><span class="line">|       <span class="number">21</span>|</span><br><span class="line">|       <span class="number">41</span>|</span><br><span class="line">+---------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li><code>select</code>：只查看指定列数据</li>
<li><code>select($&quot;&quot;)</code>：以此列为变量进行其他计算</li>
</ul>
<h3 id="RDD转DataFrame"><a href="#RDD转DataFrame" class="headerlink" title="RDD转DataFrame"></a>RDD转DataFrame</h3><p>注意：如果需要RDD与DF或者DS之间操作，<font color="red">那么都需要引入<code> import spark.implicits._</code> <strong>【spark不是包名，而是sparkSession对象的名称】</strong></font></p>
<p><strong>前置条件：<font color="red">导入隐式转换并创建一个RDD</font></strong></p>
<p><strong>1. 通过手动指定结构</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">20</span>] at makeRDD at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = rdd.toDF(<span class="string">&quot;id&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [id: int]</span><br><span class="line"></span><br><span class="line">scala&gt; df.show</span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">|  <span class="number">1</span>|</span><br><span class="line">|  <span class="number">2</span>|</span><br><span class="line">|  <span class="number">3</span>|</span><br><span class="line">|  <span class="number">4</span>|</span><br><span class="line">|  <span class="number">5</span>|</span><br><span class="line">|  <span class="number">6</span>|</span><br><span class="line">|  <span class="number">7</span>|</span><br><span class="line">|  <span class="number">8</span>|</span><br><span class="line">|  <span class="number">9</span>|</span><br><span class="line">| <span class="number">10</span>|</span><br><span class="line">+---+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>

<p><strong>2. 通过样例类指定结构，类同时包含结构和类型</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>,<span class="number">20</span>), (<span class="string">&quot;lisi&quot;</span>,<span class="number">20</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">24</span>] at makeRDD at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">user</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">defined</span> <span class="title">class</span> <span class="title">user</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">val</span> <span class="title">mapRdd</span> </span>= rdd.map(x=&gt;&#123;user(x._1, x._2)&#125;)</span><br><span class="line">mapRdd: org.apache.spark.rdd.<span class="type">RDD</span>[user] = <span class="type">MapPartitionsRDD</span>[<span class="number">25</span>] at map at &lt;console&gt;:<span class="number">31</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = mapRdd.toDF</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, age: int]</span><br><span class="line"></span><br><span class="line">scala&gt; df.show</span><br><span class="line">+--------+---+</span><br><span class="line">|    name|age|</span><br><span class="line">+--------+---+</span><br><span class="line">|zhangsan| <span class="number">20</span>|</span><br><span class="line">|    lisi| <span class="number">20</span>|</span><br><span class="line">+--------+---+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>

<h3 id="DataFrame转RDD"><a href="#DataFrame转RDD" class="headerlink" title="DataFrame转RDD"></a>DataFrame转RDD</h3><p>DataFrame：有结构没类型。所以由DF变为RDD时，RDD存储的是ROW对象，此时只能通过索引获取值，因为它不存储类型</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> dfToRDD = df.rdd</span><br><span class="line">dfToRDD: org.apache.spark.rdd.<span class="type">RDD</span>[org.apache.spark.sql.<span class="type">Row</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">35</span>] at rdd at &lt;console&gt;:<span class="number">35</span></span><br><span class="line"></span><br><span class="line">scala&gt; dfToRDD.collect</span><br><span class="line">res11: <span class="type">Array</span>[org.apache.spark.sql.<span class="type">Row</span>] = <span class="type">Array</span>([zhangsan,<span class="number">20</span>], [lisi,<span class="number">20</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>请注意<code>Row</code>对象，它只能索引访问元素，而且它不知道字段的类型</p>
<h2 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h2><h3 id="创建-1"><a href="#创建-1" class="headerlink" title="创建"></a>创建</h3><p><font color="red">因为<code>DataSet</code>创建时同时需要结构和类型，这时就需要使用到样例类了，因为类既有结构又有类型。</font>通过类创建DataSet很方便。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">defined</span> <span class="title">class</span> <span class="title">Person</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span>  <span class="title">val</span> <span class="title">caseClassDS</span> </span>= <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">&quot;Andy&quot;</span>, <span class="number">32</span>)).toDS()</span><br><span class="line">caseClassDS: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Person</span>] = [name: string, age: bigint]</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>

<h3 id="RDD转DataSet"><a href="#RDD转DataSet" class="headerlink" title="RDD转DataSet"></a>RDD转DataSet</h3><p>SparkSQL能够自动将包含有<code>case</code>类的RDD转换成DataFrame，<code>case</code>类定义了table的结构，<font color="red"><strong>case类属性通过反射变成了表的列名。</strong></font></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>,<span class="number">20</span>), (<span class="string">&quot;lisi&quot;</span>,<span class="number">20</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">24</span>] at makeRDD at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">user</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">defined</span> <span class="title">class</span> <span class="title">user</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">val</span> <span class="title">mapRdd</span> </span>= rdd.map(x=&gt;&#123;user(x._1, x._2)&#125;)</span><br><span class="line">mapRdd: org.apache.spark.rdd.<span class="type">RDD</span>[user] = <span class="type">MapPartitionsRDD</span>[<span class="number">25</span>] at map at &lt;console&gt;:<span class="number">31</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> ds = mapRdd.toDS</span><br></pre></td></tr></table></figure>

<h3 id="DataSet转RDD"><a href="#DataSet转RDD" class="headerlink" title="DataSet转RDD"></a>DataSet转RDD</h3><p>调用rdd方法即可。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">defined</span> <span class="title">class</span> <span class="title">Person</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span>  <span class="title">val</span> <span class="title">caseClassDS</span> </span>= <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">&quot;Andy&quot;</span>, <span class="number">32</span>)).toDS()</span><br><span class="line">caseClassDS: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Person</span>] = [name: string, age: bigint]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = caseClassDS.rdd</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Person</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">39</span>] at rdd at &lt;console&gt;:<span class="number">30</span></span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>

<h2 id="DataSet与DataFrame的互操作"><a href="#DataSet与DataFrame的互操作" class="headerlink" title="DataSet与DataFrame的互操作"></a>DataSet与DataFrame的互操作</h2><ol>
<li>DataFrame与DataSet相比的话少了类型，所以将DF的结构添加类型即可，可以通过<font color="red"><strong>case类属性通过反射变成了表的列名。</strong></font>主要操作<code>df.as[类]</code></li>
<li>DataSet与DataFrame相比的话，啥也不缺，直接转换即可：<code>ds.toDF</code></li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">defined</span> <span class="title">class</span> <span class="title">Person</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span>  <span class="title">val</span> <span class="title">caseClassDS</span> </span>= <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">&quot;Andy&quot;</span>, <span class="number">32</span>)).toDS()</span><br><span class="line">caseClassDS: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Person</span>] = [name: string, age: bigint]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = caseClassDS.toDF</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, age: bigint]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> ds = df.as[<span class="type">Person</span>]</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Person</span>] = [name: string, age: bigint]</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>

<h1 id="Idea创建Spark-SQL程序"><a href="#Idea创建Spark-SQL程序" class="headerlink" title="Idea创建Spark SQL程序"></a>Idea创建Spark SQL程序</h1><p>IDEA中程序的打包和运行方式都和<code>SparkCore</code>类似，需要注意<code>Spark SQL</code>程序需要引入以下包，<font color="red"><code>SparkCore</code>的包中是没有<code>SparkSQL</code>的开发环境的</font></p>
<p><strong>Maven依赖中需要添加新的依赖项：</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>程序环境三部曲</strong></p>
<ol>
<li>环境配置对象<code>sparkConfig</code></li>
<li>SparkSQL环境<code>sparkSession</code></li>
<li>DF与DS的相互隐式转换导入<code>import spark.implicits._</code></li>
</ol>
<p>注意：如果需要RDD与DF或者DS之间操作，<font color="red">那么都需要引入<code> import spark.implicits._</code> <strong>【spark不是包名，而是sparkSession对象的名称】</strong></font></p>
<p><strong>程序如下</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkSql</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 创建配置对象</span></span><br><span class="line">    <span class="keyword">val</span> config: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;sql&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建SparkSQL环境</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(config).getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 隐式转换</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// SparkSession包含了SparkContext</span></span><br><span class="line">    <span class="keyword">val</span> initRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = spark.sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>, <span class="number">20</span>), (<span class="string">&quot;zhang&quot;</span>, <span class="number">30</span>)))</span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = initRDD.map(x =&gt; person(x._1, x._2)).toDF()</span><br><span class="line">    <span class="keyword">val</span> ds: <span class="type">Dataset</span>[person] = initRDD.map(x =&gt; person(x._1, x._2)).toDS()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Row</span>] = df.rdd</span><br><span class="line">    <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[person] = ds.rdd</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">person</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br></pre></td></tr></table></figure>



<p><font color="red"></font></p>
<h1 id="用户自定义函数"><a href="#用户自定义函数" class="headerlink" title="用户自定义函数"></a>用户自定义函数</h1><h2 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h2><p>一对一。</p>
<p>很简单，直接采用匿名函数即可。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkSql</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 创建配置对象</span></span><br><span class="line">    <span class="keyword">val</span> config: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;udf&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建SparkSQL环境</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(config).getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 隐式转换</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// SparkSession包含了SparkContext</span></span><br><span class="line">    <span class="keyword">val</span> initRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = spark.sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>, <span class="number">20</span>), (<span class="string">&quot;zhang&quot;</span>, <span class="number">30</span>)))</span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = initRDD.map(x =&gt; person(x._1, x._2)).toDF()</span><br><span class="line">    <span class="keyword">val</span> ds: <span class="type">Dataset</span>[person] = initRDD.map(x =&gt; person(x._1, x._2)).toDS()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// --------df与ds使用方法一致 --------------</span></span><br><span class="line">    ds.createTempView(<span class="string">&quot;ds_table&quot;</span>)</span><br><span class="line">    spark.udf.register(<span class="string">&quot;addName&quot;</span>, (x:<span class="type">String</span>)=&gt; <span class="string">&quot;Name:&quot;</span>+x)</span><br><span class="line">    spark.sql(<span class="string">&quot;select addName(name) from ds_table&quot;</span>).show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">person</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br></pre></td></tr></table></figure>

<h2 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a>UDAF</h2><p>用户自定义聚合函数</p>
<p><code>强类型的Dataset</code>和<code>弱类型的DataFrame</code>都提供了相关的聚合函数， 如 count()，countDistinct()，avg()，max()，min()。除此之外，用户可以设定<strong>自己的自定义聚合函数</strong>。</p>
<h3 id="弱类型自定义函数"><a href="#弱类型自定义函数" class="headerlink" title="弱类型自定义函数"></a>弱类型自定义函数</h3><p>弱类型用户自定义聚合函数：通过继承<code>UserDefinedAggregateFunction</code>来实现用户自定义聚合函数。下面展示一个求平均工资的自定义聚合函数。</p>
<p>缺点：需要自己手动输入各种类型，在编写逻辑时只能通过索引拿到值，必须和设置类型时顺序一致，否则运行时后出现问题。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkSql</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 创建配置对象</span></span><br><span class="line">    <span class="keyword">val</span> config: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;udf&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建SparkSQL环境</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(config).getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 隐式转换</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// SparkSession包含了SparkContext</span></span><br><span class="line">    <span class="keyword">val</span> initRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = spark.sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>, <span class="number">20</span>), (<span class="string">&quot;zhang&quot;</span>, <span class="number">30</span>)))</span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = initRDD.map(x =&gt; person(x._1, x._2)).toDF()</span><br><span class="line">    <span class="keyword">val</span> ds: <span class="type">Dataset</span>[person] = initRDD.map(x =&gt; person(x._1, x._2)).toDS()</span><br><span class="line"></span><br><span class="line">    ds.createTempView(<span class="string">&quot;ds_table&quot;</span>)</span><br><span class="line">    <span class="comment">// 注册聚合函数</span></span><br><span class="line">    <span class="keyword">val</span> avg = <span class="keyword">new</span> myAvg</span><br><span class="line">    spark.udf.register(<span class="string">&quot;myavg&quot;</span>, avg)</span><br><span class="line">    <span class="comment">// 弱类型可以通过sql或者DSL两种模式查询，通过SQL因为它的输入只是一个值而已</span></span><br><span class="line">    spark.sql(<span class="string">&quot;select myavg(age) from ds_table&quot;</span>).show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">myAvg</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 输入值的类型</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">StructType</span>().add(<span class="string">&quot;age&quot;</span>, <span class="type">LongType</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 用户自定义处理函数内部使用的变量的类型</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="comment">// 记住顺序，后面的操作全是有序的</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">StructType</span>().add(<span class="string">&quot;sum&quot;</span>, <span class="type">LongType</span>).add(<span class="string">&quot;count&quot;</span>, <span class="type">LongType</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 返回的类型</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 对于相同的输入是否一直返回相同的输出；函数的稳定性</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 看到buffer就是工作区，函数内部变量的初始化</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer(<span class="number">0</span>) = <span class="number">0</span>L <span class="comment">// sum</span></span><br><span class="line">    buffer(<span class="number">1</span>) = <span class="number">0</span>L <span class="comment">// count</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// buffer是工作区，input是函数的输入。这个函数就是每次来新的输入值时，函数内部变量的变化</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer(<span class="number">0</span>) = buffer.getLong(<span class="number">0</span>) + input.getLong(<span class="number">0</span>) <span class="comment">// sum</span></span><br><span class="line">    buffer(<span class="number">1</span>) = buffer.getLong(<span class="number">1</span>) + <span class="number">1</span> <span class="comment">// count</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 多个Executor执行后，形成多个buffer  最后合并到一起</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer1(<span class="number">0</span>) = buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>)</span><br><span class="line">    buffer1(<span class="number">1</span>) = buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 最后的结果</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">    buffer.getLong(<span class="number">0</span>).toDouble / buffer.getLong(<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p> <font color="red"><strong>弱类型可以通过sql或者DSL两种模式查询，通过SQL因为它的输入只是一个值而已</strong></font></p>
<h3 id="强类型自定义函数"><a href="#强类型自定义函数" class="headerlink" title="强类型自定义函数"></a>强类型自定义函数</h3><p>强类型用户自定义聚合函数：通过继承<code>Aggregator</code>来实现强类型自定义聚合函数。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkSql</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 创建配置对象</span></span><br><span class="line">    <span class="keyword">val</span> config: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;udf&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建SparkSQL环境</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(config).getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 隐式转换</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">// SparkSession包含了SparkContext</span></span><br><span class="line">    <span class="keyword">val</span> initRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = spark.sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>, <span class="number">20</span>), (<span class="string">&quot;zhang&quot;</span>, <span class="number">30</span>)))</span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = initRDD.map(x =&gt; person(x._1, x._2)).toDF()</span><br><span class="line">    <span class="keyword">val</span> ds: <span class="type">Dataset</span>[person] = initRDD.map(x =&gt; person(x._1, x._2)).toDS()</span><br><span class="line"></span><br><span class="line">    ds.createTempView(<span class="string">&quot;ds_table&quot;</span>)</span><br><span class="line">    <span class="comment">// 注册聚合函数</span></span><br><span class="line">    <span class="keyword">val</span> avg = <span class="keyword">new</span> <span class="type">StrongAvg</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 但是不能使用SQL形式，因为我们强类型输入的值是一个类，用sql编写如何传送给函数结构加类型，那么通过有结构和类型的只有DS</span></span><br><span class="line">    <span class="comment">// 所以我们需要使用DS的DSL方式查询</span></span><br><span class="line">    <span class="comment">// 将聚合函数转换为查询列</span></span><br><span class="line">    <span class="keyword">val</span> value: <span class="type">TypedColumn</span>[person, <span class="type">Double</span>] = avg.toColumn.name(<span class="string">&quot;avg&quot;</span>)</span><br><span class="line"></span><br><span class="line">    ds.select(value).show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="comment">// !!!因为样例类属性默认val，需要手动设置为var</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">dealwith</span>(<span class="params">var sum: <span class="type">Long</span>, var count: <span class="type">Long</span></span>) </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">myAvg</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 输入值的类型</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">StructType</span>().add(<span class="string">&quot;age&quot;</span>, <span class="type">LongType</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 用户自定义处理函数内部使用的变量的类型</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="comment">// 记住顺序，后面的操作全是有序的</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">StructType</span>().add(<span class="string">&quot;sum&quot;</span>, <span class="type">LongType</span>).add(<span class="string">&quot;count&quot;</span>, <span class="type">LongType</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 返回的类型</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 对于相同的输入是否一直返回相同的输出；函数的稳定性</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 看到buffer就是工作区，函数内部变量的初始化</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer(<span class="number">0</span>) = <span class="number">0</span>L <span class="comment">// sum</span></span><br><span class="line">    buffer(<span class="number">1</span>) = <span class="number">0</span>L <span class="comment">// count</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// buffer是工作区，input是函数的输入。这个函数就是每次来新的输入值时，函数内部变量的变化</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer(<span class="number">0</span>) = buffer.getLong(<span class="number">0</span>) + input.getLong(<span class="number">0</span>) <span class="comment">// sum</span></span><br><span class="line">    buffer(<span class="number">1</span>) = buffer.getLong(<span class="number">1</span>) + <span class="number">1</span> <span class="comment">// count</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 多个Executor执行后，形成多个buffer  最后合并到一起</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer1(<span class="number">0</span>) = buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>)</span><br><span class="line">    buffer1(<span class="number">1</span>) = buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 最后的结果</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">    buffer.getLong(<span class="number">0</span>).toDouble / buffer.getLong(<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Aggregator[-IN, BUF, OUT]</span></span><br><span class="line"><span class="comment">// 泛型分别为输入类型，处理类型，输出类型。因为强类型所以需要指定类型，这样就能直接通过对象访问到每个值，不是通过索引</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StrongAvg</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[person, dealwith, <span class="type">Double</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 初始值，因为返回dealwith，所以是处理逻辑类的初始化</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: dealwith = dealwith(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 这个函数就是每次来新的输入值时，函数内部变量的变化</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(b: dealwith, a: person): dealwith = &#123;</span><br><span class="line">    b.sum = b.sum + a.age</span><br><span class="line">    b.count = b.count + <span class="number">1</span></span><br><span class="line">    b</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 多个Executor执行后，形成多个dealwith  最后合并到一起</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: dealwith, b2: dealwith): dealwith = &#123;</span><br><span class="line">    b1.sum = b1.sum + b2.sum</span><br><span class="line">    b1.count = b1.count + b2.count</span><br><span class="line">    b1</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(reduction: dealwith): <span class="type">Double</span> = &#123;</span><br><span class="line">    reduction.sum.toDouble / reduction.count</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 转码对象设置，固定套路，第三方使用 Encoders.product!!!</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[dealwith] = <span class="type">Encoders</span>.product</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 转码对象设置</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Double</span>] = <span class="type">Encoders</span>.scalaDouble</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h1 id="Spark-SQL数据源"><a href="#Spark-SQL数据源" class="headerlink" title="Spark SQL数据源"></a>Spark SQL数据源</h1><h2 id="通用加载-保存方法"><a href="#通用加载-保存方法" class="headerlink" title="通用加载/保存方法"></a>通用加载/保存方法</h2><h3 id="手动指定选项"><a href="#手动指定选项" class="headerlink" title="手动指定选项"></a>手动指定选项</h3><p>park SQL的DataFrame接口支持多种数据源的操作。一个DataFrame可以进行RDDs方式的操作，也可以被注册为临时表。<strong>把DataFrame注册为临时表之后，就可以对该DataFrame执行SQL查询。</strong>\</p>
<p>Spark SQL的<font color="red"><strong>默认数据源为Parquet格式</strong></font>。数据源为Parquet文件时，Spark SQL可以方便的执行所有的操作。修改配置项<code>spark.sql.sources.default</code>，可修改默认数据源格式。</p>
<p><font color="red">当数据源格式不是<code>parquet</code>格式文件时，需要手动指定数据源的格式</font>。数据源格式需要指定全名（例如：org.apache.spark.sql.parquet），如果数据源格式为内置格式，则只需要指定简称定json, parquet, jdbc, orc, libsvm, csv, text来指定数据的格式。</p>
<p><strong>两种方式：</strong></p>
<ol>
<li>可以通过SparkSession提供的read.load方法用于通用加载数据，使用write和save保存数据</li>
<li>read.xxx()</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//    spark.read.csv()</span></span><br><span class="line"><span class="comment">//    spark.read.json()</span></span><br><span class="line">    </span><br><span class="line">    spark.read.format(<span class="string">&quot;csv&quot;</span>).load()</span><br><span class="line">    spark.read.format(<span class="string">&quot;json&quot;</span>).load()</span><br></pre></td></tr></table></figure>

<h3 id="文件保存选项"><a href="#文件保存选项" class="headerlink" title="文件保存选项"></a>文件保存选项</h3><p>可以采用<code>SaveMode</code>执行存储操作，<code>SaveMode</code>定义了对数据的处理模式。需要注意的是，这些保存模式不使用任何锁定，不是原子操作。此外，当使用<code>Overwrite</code>方式执行时，在输出新数据之前原数据就已经被删除。SaveMode详细介绍如下表：</p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200826201904989.png" alt="image-20200826201904989"></p>
<h2 id="JSON文件"><a href="#JSON文件" class="headerlink" title="JSON文件"></a>JSON文件</h2><p>Spark SQL 能够自动推测 JSON数据集的结构，并将它加载为一个Dataset[Row]. 可以通过SparkSession.read.json()去加载一个 一个JSON 文件。</p>
<p><font color="red"><strong>注意：这个JSON文件不是一个传统的JSON文件，每一行都得是一个JSON串。</strong></font></p>
<p><code>spark.read.format(&quot;json&quot;).load()</code></p>
<h2 id="Parquet文件"><a href="#Parquet文件" class="headerlink" title="Parquet文件"></a>Parquet文件</h2><p><code>Parquet</code>是一种流行的列式存储格式，可以高效地存储具有嵌套字段的记录。Parquet格式经常在Hadoop生态圈中被使用，它也支持Spark SQL的全部数据类型。<font color="red">Spark SQL 提供了直接读取和存储 Parquet 格式文件的方法。 (默认)</font></p>
<p><code>spark.read.load(xxx)</code>，不用指定类型，默认<code>Parquet</code></p>
<p><code>peopleDF.write.mode.parquet(&quot;hdfs://hadoop102:9000/people.parquet&quot;)</code></p>
<h2 id="JDBC"><a href="#JDBC" class="headerlink" title="JDBC"></a>JDBC</h2><p>Spark SQL可以通过JDBC从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame一系列的计算后，还可以将数据再写回关系型数据库中。</p>
<p><font color="red">注意:需要将相关的数据库驱动放到spark的类路径下。如果使用Spark-Shell的话</font></p>
<h3 id="读方式一"><a href="#读方式一" class="headerlink" title="读方式一"></a>读方式一</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> jdbcDF = spark.read</span><br><span class="line">.format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql://hadoop102:3306/rdd&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;rddtable&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;000000&quot;</span>)</span><br><span class="line">.load()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="读方式二"><a href="#读方式二" class="headerlink" title="读方式二"></a>读方式二</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> connectionProperties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">connectionProperties.put(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">connectionProperties.put(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;000000&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> jdbcDF2 = spark.read.jdbc(<span class="string">&quot;jdbc:mysql://hadoop102:3306/rdd&quot;</span>, <span class="string">&quot;rddtable&quot;</span>,connectionProperties)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="写方式一"><a href="#写方式一" class="headerlink" title="写方式一"></a>写方式一</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">jdbcDF.write.mode(xxx)</span><br><span class="line">.format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql://hadoop102:3306/rdd&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;dftable&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;000000&quot;</span>)</span><br><span class="line">.save()</span><br></pre></td></tr></table></figure>

<h3 id="写方式二"><a href="#写方式二" class="headerlink" title="写方式二"></a>写方式二</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jdbcDF2.write.jdbc(<span class="string">&quot;jdbc:mysql://hadoop102:3306/rdd&quot;</span>, <span class="string">&quot;db&quot;</span>, connectionProperties)</span><br></pre></td></tr></table></figure>

<h2 id="Hive数据仓库"><a href="#Hive数据仓库" class="headerlink" title="Hive数据仓库"></a>Hive数据仓库</h2><p>Apache Hive是Hadoop上的SQL引擎，<strong>Spark SQL编译时可以包含Hive支持，也可以不包含</strong>。包含Hive支持的Spark SQL可以<strong>支持Hive表访问、UDF(用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等</strong>。需要强调的一点是，如果要在Spark SQL中包含Hive的库，并不需要事先安装Hive。一般来说，最好还是在编译Spark SQL时引入Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持。 </p>
<p><font color="red">若要把Spark SQL连接到一个部署好的Hive上，你必须把hive-site.xml复制到 Spark的配置文件目录中($SPARK_HOME/conf)</font>。即使没有部署好Hive，Spark SQL也可以运行。<font color="red"> 需要注意的是，如果你没有部署好Hive，Spark SQL会在当前的工作目录中创建出自己的Hive 元数据仓库，叫作 metastore_db。</font>此外，如果你尝试使用 HiveQL 中的 CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的 hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。</p>
<h3 id="内嵌的Hive应用"><a href="#内嵌的Hive应用" class="headerlink" title="内嵌的Hive应用"></a>内嵌的Hive应用</h3><p>如果要使用内嵌的Hive，什么都不用做，直接用就可以了。 </p>
<p>可以通过添加参数初次指定数据仓库地址：–conf </p>
<p><font color="red"><strong>注意：</strong>如果你使用的是内部的Hive，在Spark2.0之后，spark.sql.warehouse.dir用于指定数据仓库的地址，如果你需要是用HDFS作为路径，那么需要将core-site.xml和hdfs-site.xml 加入到Spark conf目录，否则只会创建master节点上的warehouse目录，查询时会出现文件找不到的问题，这是需要使用HDFS，则需要将metastore删除，重启集群。</font></p>
<h3 id="外部的Hive应用"><a href="#外部的Hive应用" class="headerlink" title="外部的Hive应用"></a>外部的Hive应用</h3><p>如果想连接外部已经部署好的Hive，需要通过以下几个步骤。</p>
<ol>
<li>将Hive中的hive-site.xml拷贝或者软连接到Spark安装目录下的conf目录下。</li>
<li>打开spark shell，注意带上访问Hive元数据库的JDBC客户端</li>
</ol>
<p><code>bin/spark-shell --jars mysql-connector-java-5.1.27-bin.jar</code></p>
<p>又或者将此jar包放入spark的依赖目录</p>
<h3 id="Spark-SQL-CLI"><a href="#Spark-SQL-CLI" class="headerlink" title="Spark SQL CLI"></a>Spark SQL CLI</h3><p>Spark SQL CLI可以很方便的在本地运行Hive元数据服务以及从命令行执行查询任务。在Spark目录下执行如下命令启动Spark SQL CLI：</p>
<p><code>./bin/spark-sql</code></p>
<h3 id="代码中使用Hive"><a href="#代码中使用Hive" class="headerlink" title="代码中使用Hive"></a>代码中使用Hive</h3><p><strong>pom</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-hive --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-exec --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>创建SparkSession时需要添加hive支持</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> warehouseLocation: <span class="type">String</span> = <span class="keyword">new</span> <span class="type">File</span>(<span class="string">&quot;spark-warehouse&quot;</span>).getAbsolutePath</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">.builder()</span><br><span class="line">.appName(<span class="string">&quot;Spark Hive Example&quot;</span>)</span><br><span class="line">.config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, warehouseLocation)</span><br><span class="line">.enableHiveSupport() <span class="comment">// 添加hive支持</span></span><br><span class="line">.getOrCreate()</span><br></pre></td></tr></table></figure>

<p><font color="red"><strong>注意：</strong>蓝色部分为使用内置Hive需要指定一个Hive仓库地址。若使用的是外部Hive，则需要<strong>将hive-site.xml添加到ClassPath下</strong>。</font></p>
<p><font color="red"></font></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Xiangjie</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://awslzhang.top/2020/08/24/spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-SparkSQL/">https://awslzhang.top/2020/08/24/spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-SparkSQL/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://awslzhang.top" target="_blank">zxj</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><div class="post_share"><div class="social-share" data-image="https://spark.apache.org/images/spark-logo-trademark.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/08/26/spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-SparkStreaming/"><img class="prev-cover" src="https://spark.apache.org/images/spark-logo-trademark.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">spark学习笔记-SparkStreaming</div></div></a></div><div class="next-post pull-right"><a href="/2020/08/23/spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F%E5%8F%8A%E7%B4%AF%E5%8A%A0%E5%99%A8/"><img class="next-cover" src="https://spark.apache.org/images/spark-logo-trademark.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">spark学习笔记-广播变量及累加器</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/08/23/spark学习笔记-广播变量及累加器/" title="spark学习笔记-广播变量及累加器"><img class="cover" src="https://spark.apache.org/images/spark-logo-trademark.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-23</div><div class="title">spark学习笔记-广播变量及累加器</div></div></a></div><div><a href="/2020/08/26/spark学习笔记-SparkStreaming/" title="spark学习笔记-SparkStreaming"><img class="cover" src="https://spark.apache.org/images/spark-logo-trademark.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-26</div><div class="title">spark学习笔记-SparkStreaming</div></div></a></div><div><a href="/2020/08/19/spark学习笔记-简单了解/" title="spark学习笔记-简单了解"><img class="cover" src="https://spark.apache.org/images/spark-logo-trademark.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-19</div><div class="title">spark学习笔记-简单了解</div></div></a></div><div><a href="/2020/08/29/spark学习笔记-SparkCore解析/" title="spark学习笔记-SparkCore解析"><img class="cover" src="https://spark.apache.org/images/spark-logo-trademark.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-29</div><div class="title">spark学习笔记-SparkCore解析</div></div></a></div><div><a href="/2020/08/22/spark学习笔记-RDD/" title="spark学习笔记-RDD"><img class="cover" src="https://spark.apache.org/images/spark-logo-trademark.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-22</div><div class="title">spark学习笔记-RDD</div></div></a></div></div></div></div><div class="aside_content" id="aside_content"><div class="card-widget card-info"><div class="card-content"><div class="card-info-avatar is-center"><img class="avatar-img" src="/null" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Xiangjie</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">85</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">43</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">25</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/XiangJie-Zhang" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:qluzxj@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div></div><div class="card-widget card-announcement"><div class="card-content"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="card-content"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark-SQL%E6%A6%82%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">Spark SQL概述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFSpark-SQL"><span class="toc-number">1.1.</span> <span class="toc-text">什么是Spark SQL</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-SQL%E7%9A%84%E7%89%B9%E7%82%B9"><span class="toc-number">1.2.</span> <span class="toc-text">Spark SQL的特点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFDataFrame"><span class="toc-number">1.3.</span> <span class="toc-text">什么是DataFrame</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFDataSet"><span class="toc-number">1.4.</span> <span class="toc-text">什么是DataSet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD%E3%80%81DataFrame%E3%80%81DataSet%F0%9F%94%BA"><span class="toc-number">1.5.</span> <span class="toc-text">RDD、DataFrame、DataSet🔺</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#SparkSQL%E7%BC%96%E7%A8%8B"><span class="toc-number">2.</span> <span class="toc-text">SparkSQL编程</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#SparkSession%E6%96%B0%E7%9A%84%E8%B5%B7%E5%A7%8B%E7%82%B9"><span class="toc-number">2.1.</span> <span class="toc-text">SparkSession新的起始点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataFrame"><span class="toc-number">2.2.</span> <span class="toc-text">DataFrame</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA"><span class="toc-number">2.2.1.</span> <span class="toc-text">创建</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Spark%E6%95%B0%E6%8D%AE%E6%BA%90%E5%88%9B%E5%BB%BA"><span class="toc-number">2.2.1.1.</span> <span class="toc-text">Spark数据源创建</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#RDD%E5%88%9B%E5%BB%BA"><span class="toc-number">2.2.1.2.</span> <span class="toc-text">RDD创建</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SQL%E9%A3%8E%E6%A0%BC%E8%AF%AD%E6%B3%95%F0%9F%94%BA"><span class="toc-number">2.2.2.</span> <span class="toc-text">SQL风格语法🔺</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DSL%E9%A3%8E%E6%A0%BC%E8%AF%AD%E6%B3%95"><span class="toc-number">2.2.3.</span> <span class="toc-text">DSL风格语法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD%E8%BD%ACDataFrame"><span class="toc-number">2.2.4.</span> <span class="toc-text">RDD转DataFrame</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DataFrame%E8%BD%ACRDD"><span class="toc-number">2.2.5.</span> <span class="toc-text">DataFrame转RDD</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataSet"><span class="toc-number">2.3.</span> <span class="toc-text">DataSet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-1"><span class="toc-number">2.3.1.</span> <span class="toc-text">创建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD%E8%BD%ACDataSet"><span class="toc-number">2.3.2.</span> <span class="toc-text">RDD转DataSet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DataSet%E8%BD%ACRDD"><span class="toc-number">2.3.3.</span> <span class="toc-text">DataSet转RDD</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataSet%E4%B8%8EDataFrame%E7%9A%84%E4%BA%92%E6%93%8D%E4%BD%9C"><span class="toc-number">2.4.</span> <span class="toc-text">DataSet与DataFrame的互操作</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Idea%E5%88%9B%E5%BB%BASpark-SQL%E7%A8%8B%E5%BA%8F"><span class="toc-number">3.</span> <span class="toc-text">Idea创建Spark SQL程序</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%A8%E6%88%B7%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0"><span class="toc-number">4.</span> <span class="toc-text">用户自定义函数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#UDF"><span class="toc-number">4.1.</span> <span class="toc-text">UDF</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#UDAF"><span class="toc-number">4.2.</span> <span class="toc-text">UDAF</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%B1%E7%B1%BB%E5%9E%8B%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0"><span class="toc-number">4.2.1.</span> <span class="toc-text">弱类型自定义函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%BA%E7%B1%BB%E5%9E%8B%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0"><span class="toc-number">4.2.2.</span> <span class="toc-text">强类型自定义函数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark-SQL%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="toc-number">5.</span> <span class="toc-text">Spark SQL数据源</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%9A%E7%94%A8%E5%8A%A0%E8%BD%BD-%E4%BF%9D%E5%AD%98%E6%96%B9%E6%B3%95"><span class="toc-number">5.1.</span> <span class="toc-text">通用加载&#x2F;保存方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E6%8C%87%E5%AE%9A%E9%80%89%E9%A1%B9"><span class="toc-number">5.1.1.</span> <span class="toc-text">手动指定选项</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E4%BF%9D%E5%AD%98%E9%80%89%E9%A1%B9"><span class="toc-number">5.1.2.</span> <span class="toc-text">文件保存选项</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#JSON%E6%96%87%E4%BB%B6"><span class="toc-number">5.2.</span> <span class="toc-text">JSON文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Parquet%E6%96%87%E4%BB%B6"><span class="toc-number">5.3.</span> <span class="toc-text">Parquet文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#JDBC"><span class="toc-number">5.4.</span> <span class="toc-text">JDBC</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E6%96%B9%E5%BC%8F%E4%B8%80"><span class="toc-number">5.4.1.</span> <span class="toc-text">读方式一</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E6%96%B9%E5%BC%8F%E4%BA%8C"><span class="toc-number">5.4.2.</span> <span class="toc-text">读方式二</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%99%E6%96%B9%E5%BC%8F%E4%B8%80"><span class="toc-number">5.4.3.</span> <span class="toc-text">写方式一</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%99%E6%96%B9%E5%BC%8F%E4%BA%8C"><span class="toc-number">5.4.4.</span> <span class="toc-text">写方式二</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93"><span class="toc-number">5.5.</span> <span class="toc-text">Hive数据仓库</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%85%E5%B5%8C%E7%9A%84Hive%E5%BA%94%E7%94%A8"><span class="toc-number">5.5.1.</span> <span class="toc-text">内嵌的Hive应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%96%E9%83%A8%E7%9A%84Hive%E5%BA%94%E7%94%A8"><span class="toc-number">5.5.2.</span> <span class="toc-text">外部的Hive应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark-SQL-CLI"><span class="toc-number">5.5.3.</span> <span class="toc-text">Spark SQL CLI</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E4%B8%AD%E4%BD%BF%E7%94%A8Hive"><span class="toc-number">5.5.4.</span> <span class="toc-text">代码中使用Hive</span></a></li></ol></li></ol></li></ol></div></div></div><div class="card-widget card-recent-post"><div class="card-content"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/01/10/Api%E5%92%8CSQL/" title="Api和SQL"><img src="https://flink.apache.org/img/flink-header-logo.svg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Api和SQL"/></a><div class="content"><a class="title" href="/2021/01/10/Api%E5%92%8CSQL/" title="Api和SQL">Api和SQL</a><time datetime="2021-01-10T03:51:40.000Z" title="发表于 2021-01-10 11:51:40">2021-01-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/01/02/Flink%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86/" title="Flink状态编程和容错机制"><img src="https://flink.apache.org/img/flink-header-logo.svg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Flink状态编程和容错机制"/></a><div class="content"><a class="title" href="/2021/01/02/Flink%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86/" title="Flink状态编程和容错机制">Flink状态编程和容错机制</a><time datetime="2021-01-02T10:06:02.000Z" title="发表于 2021-01-02 18:06:02">2021-01-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/23/Flink%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89%E5%92%8CWaterMark/" title="Flink的时间语义和watermark"><img src="https://flink.apache.org/img/flink-header-logo.svg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Flink的时间语义和watermark"/></a><div class="content"><a class="title" href="/2020/12/23/Flink%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89%E5%92%8CWaterMark/" title="Flink的时间语义和watermark">Flink的时间语义和watermark</a><time datetime="2020-12-23T10:59:07.000Z" title="发表于 2020-12-23 18:59:07">2020-12-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/17/Flink-API/" title="Flink Api学习"><img src="https://flink.apache.org/img/flink-header-logo.svg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Flink Api学习"/></a><div class="content"><a class="title" href="/2020/12/17/Flink-API/" title="Flink Api学习">Flink Api学习</a><time datetime="2020-12-17T12:36:28.000Z" title="发表于 2020-12-17 20:36:28">2020-12-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/17/ready/" title="ready!"><img src="https://images.pexels.com/photos/924824/pexels-photo-924824.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ready!"/></a><div class="content"><a class="title" href="/2020/12/17/ready/" title="ready!">ready!</a><time datetime="2020-12-16T16:02:06.000Z" title="发表于 2020-12-17 00:02:06">2020-12-17</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Xiangjie</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script></div></body></html>