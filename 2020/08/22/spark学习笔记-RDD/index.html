<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>spark学习笔记-RDD | zxj</title><meta name="keywords" content="Spark"><meta name="author" content="Xiangjie"><meta name="copyright" content="Xiangjie"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="前言由Java IO引申出Spark RDD。 Java IOJava IO和RDD的相同点：  装饰者模式 延时计算  Java IO涉及到的名词：  输入：input 输出：output 字节流：所有都可以使用字节流读取，一般图片、视频、压缩包之类的只能使用字节流读取 字符流：文本文件使用字符流读取    Java IO的装饰者模式1.  我们一般读取文件使用以下的操作：文件输入流 Input">
<meta property="og:type" content="article">
<meta property="og:title" content="spark学习笔记-RDD">
<meta property="og:url" content="https://awslzhang.top/2020/08/22/spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-RDD/index.html">
<meta property="og:site_name" content="zxj">
<meta property="og:description" content="前言由Java IO引申出Spark RDD。 Java IOJava IO和RDD的相同点：  装饰者模式 延时计算  Java IO涉及到的名词：  输入：input 输出：output 字节流：所有都可以使用字节流读取，一般图片、视频、压缩包之类的只能使用字节流读取 字符流：文本文件使用字符流读取    Java IO的装饰者模式1.  我们一般读取文件使用以下的操作：文件输入流 Input">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://spark.apache.org/images/spark-logo-trademark.png">
<meta property="article:published_time" content="2020-08-22T03:38:04.000Z">
<meta property="article:modified_time" content="2021-01-01T05:50:00.042Z">
<meta property="article:author" content="Xiangjie">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://spark.apache.org/images/spark-logo-trademark.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://awslzhang.top/2020/08/22/spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-RDD/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: Xiangjie","link":"链接: ","source":"来源: zxj","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}

// https://stackoverflow.com/questions/16839698/jquery-getscript-alternative-in-native-javascript
const getScript = url => new Promise((resolve, reject) => {
  const script = document.createElement('script')
  script.src = url
  script.async = true
  script.onerror = reject
  script.onload = script.onreadystatechange = function() {
    const loadState = this.readyState
    if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
    script.onload = script.onreadystatechange = null
    resolve()
  }
  document.head.appendChild(script)
})</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2021-01-01 13:50:00'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
   if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }
  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified
    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }const asideStatus = saveToLocal.get('aside-status')
if (asideStatus !== undefined) {
   if (asideStatus === 'hide') {
     document.documentElement.classList.add('hide-aside')
   } else {
     document.documentElement.classList.remove('hide-aside')
   }
}})()</script><meta name="generator" content="Hexo 5.0.2"><link rel="alternate" href="/atom.xml" title="zxj" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/null" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">83</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">43</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">25</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 目录</span></a></div></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://spark.apache.org/images/spark-logo-trademark.png)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">zxj</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 目录</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">spark学习笔记-RDD</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-08-22T03:38:04.000Z" title="发表于 2020-08-22 11:38:04">2020-08-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-01-01T05:50:00.042Z" title="更新于 2021-01-01 13:50:00">2021-01-01</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Spark/">Spark</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">10.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>39分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>由<code>Java IO</code>引申出<code>Spark RDD</code>。</p>
<h2 id="Java-IO"><a href="#Java-IO" class="headerlink" title="Java IO"></a>Java IO</h2><p>Java IO和RDD的相同点：</p>
<ul>
<li>装饰者模式</li>
<li>延时计算</li>
</ul>
<p>Java IO涉及到的名词：</p>
<ul>
<li>输入：<code>input</code></li>
<li>输出：<code>output</code></li>
<li>字节流：所有都可以使用字节流读取，一般图片、视频、压缩包之类的只能使用字节流读取</li>
<li>字符流：文本文件使用字符流读取</li>
</ul>
<br/>

<h3 id="Java-IO的装饰者模式"><a href="#Java-IO的装饰者模式" class="headerlink" title="Java IO的装饰者模式"></a>Java IO的装饰者模式</h3><p><strong>1.  我们一般读取文件使用以下的操作：文件输入流</strong></p>
<p><code>InputStream inputStream = new FileInputStream(new File(&quot;&quot;));</code></p>
<p>效率不高，读一字节传一字节。</p>
<p><strong>2.  文件输入流，效率不高；所以可以使用缓冲流</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">InputStream inputStream = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(<span class="string">&quot;&quot;</span>));</span><br><span class="line">InputStream bufferedInputStream = <span class="keyword">new</span> BufferedInputStream(inputStream);</span><br></pre></td></tr></table></figure>

<p>可以看到<strong>缓冲流的参数是一个文件输入流的对象</strong>，这是因为Java IO体现了一种<font color="red"><strong>装饰者模式</strong>：</font></p>
<ul>
<li><p>它表示者功能的扩展，原来的文件输入流没有缓冲的功能，但是对文件输入流包装了一层之后，就有了缓冲的功能。</p>
</li>
<li><p>这样读文件的还是文件输入流，而<strong>做到缓冲功能的是缓冲流</strong></p>
</li>
</ul>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200822120437040.png" alt="装饰者扩展功能"></p>
<p><strong>3. 使用字符流来快速读取文本</strong></p>
<p>上面的两个都是字节流：</p>
<ul>
<li>文件输入流没有缓冲，效率慢</li>
<li>缓冲流对文件输入流做了缓存，好一点；但是如果读取的是文本文件时，想一次性读取一行数据，它做不到，它读取的还是字节</li>
<li>可以使用字符流来读取文本文件</li>
</ul>
<p>根据Java IO的装饰者模式，字符流创建时的参数也是文件输入流吗？</p>
<p>答案是错误的，因为文件输入流读取的是字节，而我们字符流需要的是字符；。<font color="red">在不同的编码格式下字节和字符的转换规则不一致的</font>。所以字符流的参数应该是将<strong>字节变为字符的转换器</strong>，<strong>转换器的参数是字节流</strong>。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Reader reader = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(<span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(<span class="string">&quot;&quot;</span>))));</span><br></pre></td></tr></table></figure>

<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200822121143811.png" alt="image-20200822121143811"></p>
<h3 id="Java-IO的读取"><a href="#Java-IO的读取" class="headerlink" title="Java IO的读取"></a>Java IO的读取</h3><p>如上述字符流，当我们创建对象时并没有读取数据。当我们进行<code>readLine</code>时才会触发读取操作。逐级向上读取。</p>
<h1 id="RDD概述"><a href="#RDD概述" class="headerlink" title="RDD概述"></a>RDD概述</h1><h2 id="RDD的简介"><a href="#RDD的简介" class="headerlink" title="RDD的简介"></a>RDD的简介</h2><p><code>RDD(Resilient Distributed Dataset)</code>叫做弹性分布式数据集，是Spark中最基本的数据抽象。</p>
<p><code>RDD</code>代码中是一个抽象类，它代表一个<strong>不可变、可分区</strong>、里面的元素<strong>可并行计算</strong>的集合。</p>
<p><font color="red"><strong>RDD不存储数据，存储的是计算的逻辑。只有进行行动算子时，RDD才会开始以管道形式计算</strong></font></p>
<h2 id="RDD的属性"><a href="#RDD的属性" class="headerlink" title="RDD的属性"></a>RDD的属性</h2><ol>
<li>一组分区（Partition），即数据集的基本组成单位;(RDD中的多个分区)</li>
<li>一个计算每个分区的函数;</li>
<li>RDD之间的依赖关系;</li>
<li>一个Partitioner，即RDD的分片函数;</li>
<li>一个列表，存储存取每个Partition的优先位置（preferred location）。(位置优先为有数据的节点)</li>
</ol>
<h2 id="RDD特点"><a href="#RDD特点" class="headerlink" title="RDD特点"></a>RDD特点</h2><p>RDD表示只读的分区的数据集，对RDD进行改动，只能通过RDD的转换操作，由一个RDD得到一个新的RDD，新的RDD包含了从其他RDD衍生所必需的信息。</p>
<p>RDDs之间存在依赖，RDD的执行是按照血缘关系延时计算的。如果血缘关系较长，可以通过持久化RDD来切断血缘关系。</p>
<h3 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h3><p>RDD逻辑上是分区的，每个分区的数据是抽象存在的。计算时每个分区的计算逻辑会交给一个<code>Task</code>来执行，也就是分区可以理解为RDD执行是的并行度。</p>
<h3 id="不可变-只读"><a href="#不可变-只读" class="headerlink" title="不可变(只读)"></a>不可变(只读)</h3><p>如下图所示，RDD是只读的，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。</p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200822122537700.png" alt="RDD不可变"></p>
<p> 由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了，如下图所示。</p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200822122611888.png" alt="image-20200822122611888"></p>
<p> <strong>RDD的操作算子包括两类：</strong></p>
<ul>
<li>一类叫做<code>transformations</code>，它是用来将RDD进行转化，构建RDD的血缘关系</li>
<li>另一类叫做<code>actions</code>，它是用来触发RDD的计算，得到RDD的相关计算结果或者将RDD保存的文件系统中。</li>
</ul>
<h3 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h3><p>RDDs通过操作算子进行转换，转换得到的新RDD包含了从其他RDDs衍生所必需的信息，RDDs之间维护着这种血缘关系，也称之为依赖。如以下程序：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 创建上下文对象</span></span><br><span class="line">    <span class="keyword">val</span> config: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;toDebugString&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(config)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> initRDD: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">    <span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = initRDD.map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    println(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">    println(mapRDD.toDebugString)</span><br><span class="line">    println(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200822123851822.png" alt="image-20200822123851822"></p>
<p><font color="red"><strong>我们打印了<code>mapRDD</code>的依赖关系，发现它会保存它依赖的所有RDD包含行数，参数等…</strong></font></p>
<hr>
<p>依赖包括两种，一种是窄依赖，RDDs之间分区是一一对应的，另一种是宽依赖，下游RDD的每个分区与上游RDD(也称之为父RDD)的每个分区都有关，是多对多的关系。</p>
<ul>
<li>窄依赖：父子RDD的分区是一一对应的</li>
<li>宽依赖：父RDD的分区内数据被打乱重组到子RDD的不同分区中，同时涉及<code>shuffle</code></li>
</ul>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200822124530978.png" alt="image-20200822124530978"></p>
<p><strong>依赖还有个作用就是保证容错性</strong>，因为RDD是在内存中的，假设有这个RDD链：<code>a-&gt;b-&gt;c</code>，如果b丢失了，是不是程序就失败了，正是因为有RDD的依赖关系存在我们可以通过c的数据反推出来b。保证了程序的容错性，不会因为小错误而导致整个程序失败。</p>
<h3 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h3><p>如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该RDD的时候，<strong>会直接从缓存处取而不用再根据血缘关系计算</strong>，这样就加速后期的重用。</p>
<p>如下图所示，RDD-1经过一系列的转换后得到RDD-n并保存到hdfs，RDD-1在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的RDD-1转换到RDD-m这一过程中，就不会计算其之前的RDD-0了。</p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200822125241010.png" alt="image-20200822125241010"></p>
<h1 id="RDD编程"><a href="#RDD编程" class="headerlink" title="RDD编程"></a>RDD编程</h1><p>Spark中的所有RDD方法都称为之<font color="red"><strong>算子</strong></font>，但是分为2大类：</p>
<ol>
<li><code>Transformation</code>：转换算子</li>
<li><code>Action</code>：行为算子</li>
</ol>
<h2 id="RDD编程模型"><a href="#RDD编程模型" class="headerlink" title="RDD编程模型"></a>RDD编程模型</h2><p>在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的transformations定义RDD之后，就可以调用actions触发RDD的计算，action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到action，才会执行RDD的计算(即延迟计算)，这样在运行时可以通过管道的方式传输多个转换。</p>
<p>  要使用Spark，开发者需要编写一个Driver程序，它被提交到集群以调度运行Worker，如下图所示。Driver中定义了一个或多个RDD，并调用RDD上的action，Worker则执行RDD分区计算任务。</p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200822130931511.png" alt="image-20200822130931511"></p>
<h2 id="RDD创建"><a href="#RDD创建" class="headerlink" title="RDD创建"></a>RDD创建</h2><p>在Spark中创建RDD的创建方式可以分为三种：<strong>从集合中创建RDD；从外部存储创建RDD；从其他RDD创建</strong>。</p>
<h3 id="集合中创建"><a href="#集合中创建" class="headerlink" title="集合中创建"></a>集合中创建</h3><h4 id="makeRDD"><a href="#makeRDD" class="headerlink" title="makeRDD"></a>makeRDD</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 创建上下文对象</span></span><br><span class="line">    <span class="keyword">val</span> config: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;toDebugString&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(config)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> initRDD: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">    initRDD.collect().foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们查看一下<code>makeRDD</code>的代码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">     seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">     numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">   parallelize(seq, numSlices)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>发现它调用的就是<code>parallelize</code>方法</p>
<h4 id="parallelize"><a href="#parallelize" class="headerlink" title="parallelize"></a>parallelize</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 创建上下文对象</span></span><br><span class="line">    <span class="keyword">val</span> config: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;toDebugString&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(config)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> initRDD: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>, <span class="number">2</span>)</span><br><span class="line">    println(initRDD.partitions.length)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们查看一下<code>parallelize</code>的代码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parallelize</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">    numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ParallelCollectionRDD</span>[<span class="type">T</span>](<span class="keyword">this</span>, seq, numSlices, <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">String</span>]]())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们发现这两个方法都有默认参数：<code>numSlices</code>。它是设置RDD的分区数。</p>
<p>通过一系列的追踪，发现本地模式下，这个默认参数是这样设置的：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">defaultParallelism</span></span>(): <span class="type">Int</span> =</span><br><span class="line">    scheduler.conf.getInt(<span class="string">&quot;jiuzspark.default.parallelism&quot;</span>, totalCores)</span><br></pre></td></tr></table></figure>

<p>如果能读取到<code>jiuzspark.default.parallelism</code>，就是用它的值，否则使用当前被分到的所有的核。</p>
<hr>
<p><font color="red">当我们手动指定分区数时，指定多少它就开启多少。</font></p>
<h3 id="由外部存储系统的数据集创建"><a href="#由外部存储系统的数据集创建" class="headerlink" title="由外部存储系统的数据集创建"></a>由外部存储系统的数据集创建</h3><h4 id="textFile"><a href="#textFile" class="headerlink" title="textFile"></a>textFile</h4><p>包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等</p>
<p><code>sc.textFile(&quot;hdfs://hadoop102:9000/RELEASE&quot;)</code></p>
<p><font color="red"><code>textFile</code>也支持输入分区数，但是它的参数名为<strong>最小分区数</strong>，这个值不是最终的分区数，但是最终的分区数一定大于等于它。</font></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textFile</span></span>(</span><br><span class="line">      path: <span class="type">String</span>,</span><br><span class="line">      minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[<span class="type">String</span>] = withScope &#123;</span><br><span class="line">    assertNotStopped()</span><br><span class="line">    hadoopFile(path, classOf[<span class="type">TextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>],</span><br><span class="line">      minPartitions).map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>因为Spark没有自己的读取 文件方式，它直接采用了Hadoop的读取方式，这就导致了Hadoop的规则进行分片，所以有可能大于你指定的值。</p>
<h2 id="RDD的转换算子🔺"><a href="#RDD的转换算子🔺" class="headerlink" title="RDD的转换算子🔺"></a>RDD的转换算子🔺</h2><h3 id="Value类型"><a href="#Value类型" class="headerlink" title="Value类型"></a>Value类型</h3><h4 id="map"><a href="#map" class="headerlink" title="map"></a>map</h4><p><code>map(func)</code></p>
<p>返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成</p>
<p>创建一个1-10数组的RDD，将所有元素*2形成新的RDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  <span class="keyword">var</span> source  = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">source: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> mapadd = source.map(_ * <span class="number">2</span>)</span><br><span class="line">mapadd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at map at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; mapadd.collect()</span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">14</span>, <span class="number">16</span>, <span class="number">18</span>, <span class="number">20</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h4><p><code>mapPartitions(func) </code></p>
<p>类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U]。<strong>假设有N个元素，有M个分区，那么map的函数的将被调用N次,而mapPartitions被调用M次,一个函数一次处理所有分区。</strong></p>
<p><strong>对比map函数</strong></p>
<ul>
<li>mapPartitions对一个RDD中所有的分区进行遍历，效率优于map算子，减少了发送到执行器执行交互次数</li>
<li>但是它在计算单一分区数据时是不释放内存的，可能会出现OOM</li>
</ul>
<p>所以内存足够时，为了保证效率，可以使用mapPartitions方法</p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200822135907128.png" alt="image-20200822135907128"></p>
<p>创建一个RDD，使每个元素*2组成新的RDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">4</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.mapPartitions(x=&gt;x.map(_*<span class="number">2</span>))</span><br><span class="line">res3: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">6</span>] at mapPartitions at &lt;console&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; res3.collect</span><br><span class="line">res4: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>)</span><br></pre></td></tr></table></figure>

<h4 id="mapPartitionsWithIndex"><a href="#mapPartitionsWithIndex" class="headerlink" title="mapPartitionsWithIndex"></a>mapPartitionsWithIndex</h4><p><code>mapPartitionsWithIndex(func) </code></p>
<p>类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是<code>(Int, Interator[T]) =&gt; Iterator[U]；</code>。Int就是此分区分区号</p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200822135919944.png" alt="image-20200822135919944"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">4</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> indexRdd = rdd.mapPartitionsWithIndex((index,items)=&gt;(items.map((index,_))))</span><br><span class="line">indexRdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">5</span>] at mapPartitionsWithIndex at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; indexRdd.collect</span><br><span class="line">res2: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">0</span>,<span class="number">1</span>), (<span class="number">0</span>,<span class="number">2</span>), (<span class="number">1</span>,<span class="number">3</span>), (<span class="number">1</span>,<span class="number">4</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h4><p><code>flatMap(func) </code></p>
<p>类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素）</p>
<p>创建一个元素为1-5的RDD，运用flatMap创建一个新的RDD，新的RDD为原RDD的每个元素的2倍（2，4，6，8，10）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> sourceFlat = sc.parallelize(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line">sourceFlat: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">12</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; sourceFlat.collect()</span><br><span class="line">res11: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> flatMap = sourceFlat.flatMap(<span class="number">1</span> to _)</span><br><span class="line">flatMap: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">13</span>] at flatMap at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; flatMap.collect()</span><br><span class="line">res12: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="glom"><a href="#glom" class="headerlink" title="glom"></a>glom</h4><p>将每一个分区形成一个数组，形成新的RDD类型时RDD[Array[T]]</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">16</span>,<span class="number">4</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">65</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.glom().collect()</span><br><span class="line">res25: <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = <span class="type">Array</span>(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="type">Array</span>(<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>), <span class="type">Array</span>(<span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>), <span class="type">Array</span>(<span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h4><p><code>groupBy(func)</code></p>
<p>分组，按照传入函数的返回值进行分组。将相同的key对应的值放入一个迭代器。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">4</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">65</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> group = rdd.groupBy(_%<span class="number">2</span>)</span><br><span class="line">group: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = <span class="type">ShuffledRDD</span>[<span class="number">2</span>] at groupBy at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; group.collect</span><br><span class="line">res0: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = <span class="type">Array</span>((<span class="number">0</span>,<span class="type">CompactBuffer</span>(<span class="number">2</span>, <span class="number">4</span>)), (<span class="number">1</span>,<span class="type">CompactBuffer</span>(<span class="number">1</span>, <span class="number">3</span>)))</span><br></pre></td></tr></table></figure>

<h4 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h4><p><code>filter(func) </code></p>
<p>过滤。返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成。</p>
<p>需求：创建一个RDD（由字符串组成），过滤出一个新RDD（包含”xiao”子串）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> sourceFilter = sc.parallelize(<span class="type">Array</span>(<span class="string">&quot;xiaoming&quot;</span>,<span class="string">&quot;xiaojiang&quot;</span>,<span class="string">&quot;xiaohe&quot;</span>,<span class="string">&quot;dazhi&quot;</span>))</span><br><span class="line">sourceFilter: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">10</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; sourceFilter.collect()</span><br><span class="line">res9: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(xiaoming, xiaojiang, xiaohe, dazhi)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> filter = sourceFilter.filter(_.contains(<span class="string">&quot;xiao&quot;</span>))</span><br><span class="line">filter: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">11</span>] at filter at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; filter.collect()</span><br><span class="line">res10: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(xiaoming, xiaojiang, xiaohe)</span><br></pre></td></tr></table></figure>

<h4 id="sample"><a href="#sample" class="headerlink" title="sample"></a>sample</h4><p><code>sample(withReplacement, fraction, seed) </code></p>
<p>以指定的随机种子随机抽样出数量为fraction的数据，withReplacement表示是抽出的数据是否放回，true为有放回的抽样，false为无放回的抽样，seed用于指定随机数生成器种子。</p>
<p>需求：创建一个RDD（1-10），从中选择放回和不放回抽样</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//（1）创建RDD</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">20</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"><span class="comment">//（2）打印</span></span><br><span class="line">scala&gt; rdd.collect()</span><br><span class="line">res15: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br><span class="line"><span class="comment">//（3）放回抽样</span></span><br><span class="line">scala&gt; <span class="keyword">var</span> sample1 = rdd.sample(<span class="literal">true</span>,<span class="number">0.4</span>,<span class="number">2</span>)</span><br><span class="line">sample1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">PartitionwiseSampledRDD</span>[<span class="number">21</span>] at sample at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"><span class="comment">//（4）打印放回抽样结果</span></span><br><span class="line">scala&gt; sample1.collect()</span><br><span class="line">res16: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>)</span><br><span class="line"><span class="comment">//（5）不放回抽样</span></span><br><span class="line">scala&gt; <span class="keyword">var</span> sample2 = rdd.sample(<span class="literal">false</span>,<span class="number">0.2</span>,<span class="number">3</span>)</span><br><span class="line">sample2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">PartitionwiseSampledRDD</span>[<span class="number">22</span>] at sample at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"><span class="comment">//（6）打印不放回抽样结果</span></span><br><span class="line">scala&gt; sample2.collect()</span><br><span class="line">res17: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">9</span>)</span><br></pre></td></tr></table></figure>

<h4 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h4><p><code>distinct([numTasks])) </code></p>
<p>对源RDD进行去重后返回一个新的RDD。默认情况下，只有8个并行任务来操作，但是可以传入一个可选的numTasks参数改变它。</p>
<p>会导致shuffle</p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200822140853713.png" alt="image-20200822140853713"></p>
<h4 id="coalesce"><a href="#coalesce" class="headerlink" title="coalesce"></a>coalesce</h4><p><code>coalesce(numPartitions) </code></p>
<p>缩减分区数，用于大数据集过滤后，提高小数据集的执行效率。</p>
<ul>
<li>第二个参数默认：false，只合并分区没有shuffle</li>
<li>第二个参数设置为true，重分区，有shuffle</li>
</ul>
<h4 id="repartition"><a href="#repartition" class="headerlink" title="repartition"></a>repartition</h4><p>根据分区数，重新通过网络随机洗牌所有数据。shuffle</p>
<p>底层原理：<code>coalesce(n, true)</code></p>
<h4 id="sortBy"><a href="#sortBy" class="headerlink" title="sortBy"></a>sortBy</h4><p><code>sortBy(func,[ascending], [numTasks])</code></p>
<p>使用func先对数据进行处理，按照处理后的数据比较结果排序，默认为正序。</p>
<p>ascending:</p>
<ul>
<li>true：正序</li>
<li>false：倒序</li>
</ul>
<h3 id="双V类型"><a href="#双V类型" class="headerlink" title="双V类型"></a>双V类型</h3><h4 id="union"><a href="#union" class="headerlink" title="union"></a>union</h4><p><code>union(otherDataset)</code></p>
<p>对源RDD和参数RDD求并集后返回一个新的RDD</p>
<h4 id="subtract"><a href="#subtract" class="headerlink" title="subtract"></a>subtract</h4><p><code>subtract (otherDataset)</code></p>
<p>计算差的一种函数，去除两个RDD中相同的元素，不同的RDD将保留下来</p>
<h4 id="intersection"><a href="#intersection" class="headerlink" title="intersection"></a>intersection</h4><p><code> intersection(otherDataset)</code></p>
<p>对源RDD和参数RDD求交集后返回一个新的RDD</p>
<h4 id="zip"><a href="#zip" class="headerlink" title="zip"></a>zip</h4><p><code>zip(otherDataset)</code></p>
<p>将两个RDD组合成Key/Value形式的RDD,这里默认两个RDD的partition数量以及元素数量都相同，否则会抛出异常。</p>
<p><strong>需要保证</strong></p>
<ul>
<li>两个RDD的分区相同</li>
<li>RDD每个分区中的数据量相同</li>
</ul>
<h3 id="KV类型"><a href="#KV类型" class="headerlink" title="KV类型"></a>KV类型</h3><h4 id="partitionBy"><a href="#partitionBy" class="headerlink" title="partitionBy"></a>partitionBy</h4><p>对pairRDD进行分区操作，如果原有的partionRDD和现有的partionRDD是一致的话就不进行分区， <strong>否则会生成ShuffleRDD，即会产生shuffle过程。</strong></p>
<p>参数是一个分区器，例如：<code>org.apache.spark.HashPartitioner</code>。用户可以自定义分区器，照着它抄就可以</p>
<p><code>rdd.partitionBy(new org.apache.spark.HashPartitioner(2))</code></p>
<h4 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h4><p>groupByKey也是对每个key进行操作，但只生成一个sequence</p>
<h4 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h4><p><code>reduceByKey(func, [numTasks])</code></p>
<p>在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，reduce任务的个数可以通过第二个可选的参数来设置</p>
<p><strong>对比groupByKey</strong></p>
<ol>
<li>reduceByKey：按照key进行聚合，在shuffle之前有combine（预聚合）操作，返回结果是RDD[k,v].</li>
<li>groupByKey：按照key进行分组，直接进行shuffle。</li>
<li>开发指导：reduceByKey比groupByKey，建议使用。但是需要注意是否会影响业务逻辑。</li>
</ol>
<h4 id="aggregateByKey🔺"><a href="#aggregateByKey🔺" class="headerlink" title="aggregateByKey🔺"></a>aggregateByKey🔺</h4><p>reduceByKey的操作只能确保分区内数据操作的逻辑和分区间的操作逻辑一致，但若使分区内的数据计算逻辑和分区间的计算逻辑不同的话，此函数就无法实现了。这时就用到了aggregateByKey</p>
<p>参数：<code>(zeroValue:U,[partitioner: Partitioner]) (seqOp: (U, V) =&gt; U,combOp: (U, U) =&gt; U)</code></p>
<ul>
<li><code>zeroValue</code>：分区内数据计算的初始值</li>
<li><code>seqOp</code>：分区内数据计算逻辑</li>
<li><code>combOp</code>：分区间数据计算逻辑</li>
</ul>
<p>在kv对的RDD中，，按key将value进行分组合并，合并时，将每个value和初始值作为seq函数的参数，进行计算，返回的结果作为一个新的kv对，然后再将结果按照key进行合并，最后将每个分组的value传递给combine函数进行计算（先将前两个value进行计算，将返回结果和下一个value传给combine函数，以此类推），将key与计算结果作为一个新的kv对输出。</p>
<p><strong>需求</strong></p>
<p>创建一个pairRDD，取出每个分区相同key对应值的最大值，然后相加</p>
<p><strong>分析</strong></p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200822142711127.png" alt="image-20200822142711127"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">3</span>),(<span class="string">&quot;a&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">4</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">3</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">6</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">8</span>)),<span class="number">2</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"><span class="comment">//（2）取出每个分区相同key对应值的最大值，然后相加</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> agg = rdd.aggregateByKey(<span class="number">0</span>)(math.max(_,_),_+_)</span><br><span class="line">agg: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">1</span>] at aggregateByKey at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"><span class="comment">//（3）打印结果</span></span><br><span class="line">scala&gt; agg.collect()</span><br><span class="line">res0: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((b,<span class="number">3</span>), (a,<span class="number">3</span>), (c,<span class="number">12</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>如果用此函数使用reduceByKey的功能的话：<code>aggregateByKey(0)(_+_)</code></p>
<h4 id="foldByKey🔺"><a href="#foldByKey🔺" class="headerlink" title="foldByKey🔺"></a>foldByKey🔺</h4><p>参数：<code>(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]</code></p>
<p><code>aggregateByKey</code>的简化版本，与它的区别就是：<code>seqop和combop相同</code>。即分区内和分区间的计算逻辑相同</p>
<h4 id="combineByKey-C-🔺"><a href="#combineByKey-C-🔺" class="headerlink" title="combineByKey[C]🔺"></a>combineByKey[C]🔺</h4><p>参数：<code>(createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C)</code></p>
<p><code>aggregateByKey</code>的困难版本，区别：第一个参数不是初始值，而是根据V中数据形成新的数据类型C，后面的计算全是基于新的数据类型C计算。</p>
<p><strong>需求</strong></p>
<p>创建一个pairRDD，根据key计算每种key的均值。（先计算每个key出现的次数以及可以对应值的总和，再相除得到结果）</p>
<p><strong>需求分析</strong></p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200822143200358.png" alt="image-20200822143200358"></p>
<h4 id="sortByKey"><a href="#sortByKey" class="headerlink" title="sortByKey"></a>sortByKey</h4><p><code>sortByKey([ascending], [numTasks]) </code></p>
<p>在一个(K,V)的RDD上调用，<strong>K必须实现Ordered接口</strong>，返回一个按照key进行排序的(K,V)的RDD</p>
<ul>
<li>true：正序</li>
<li>false：倒序</li>
</ul>
<h4 id="mapValues"><a href="#mapValues" class="headerlink" title="mapValues"></a>mapValues</h4><p>针对于(K,V)形式的类型只对V进行操作</p>
<h4 id="join"><a href="#join" class="headerlink" title="join"></a>join</h4><p><code> join(otherDataset, [numTasks])</code></p>
<p>在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD</p>
<p><font color="red">没有相同的key，join之后此key就不存在了。</font></p>
<h4 id="cogroup"><a href="#cogroup" class="headerlink" title="cogroup"></a>cogroup</h4><p><code>cogroup(otherDataset, [numTasks]) </code></p>
<p>在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<V>,Iterable<W>))类型的RDD</p>
<p><font color="red">没有相同的key，cogroup之后还存在。</font></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">&quot;a&quot;</span>),(<span class="number">2</span>,<span class="string">&quot;b&quot;</span>),(<span class="number">3</span>,<span class="string">&quot;c&quot;</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">37</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"><span class="comment">//（2）创建第二个pairRDD</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">5</span>),(<span class="number">3</span>,<span class="number">6</span>)))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">38</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"><span class="comment">//（3）cogroup两个RDD并打印结果</span></span><br><span class="line">scala&gt; rdd.cogroup(rdd1).collect()</span><br><span class="line">res14: <span class="type">Array</span>[(<span class="type">Int</span>, (<span class="type">Iterable</span>[<span class="type">String</span>], <span class="type">Iterable</span>[<span class="type">Int</span>]))] = <span class="type">Array</span>((<span class="number">1</span>,(<span class="type">CompactBuffer</span>(a),<span class="type">CompactBuffer</span>(<span class="number">4</span>))), (<span class="number">2</span>,(<span class="type">CompactBuffer</span>(b),<span class="type">CompactBuffer</span>(<span class="number">5</span>))), (<span class="number">3</span>,(<span class="type">CompactBuffer</span>(c),<span class="type">CompactBuffer</span>(<span class="number">6</span>))))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Action算子🔺"><a href="#Action算子🔺" class="headerlink" title="Action算子🔺"></a>Action算子🔺</h2><h3 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h3><p><code>reduce(func)</code></p>
<p>通过func函数聚集RDD中的所有元素，先聚合分区内数据，再聚合分区间数据。</p>
<h3 id="collect"><a href="#collect" class="headerlink" title="collect()"></a>collect()</h3><p>在驱动程序中，以数组的形式返回数据集的所有元素</p>
<h3 id="count"><a href="#count" class="headerlink" title="count()"></a>count()</h3><p>返回RDD中元素的个数</p>
<h3 id="取固定值"><a href="#取固定值" class="headerlink" title="取固定值"></a>取固定值</h3><ul>
<li>first()：返回RDD中的第一个元素</li>
<li>take(n)：返回一个由RDD的前n个元素组成的数组</li>
<li>takeOrdered(n)：返回该RDD排序后的前n个元素组成的数组</li>
</ul>
<h3 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate"></a>aggregate</h3><p>参数：(zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U)</p>
<p>和KV中的aggregateByKey一个用法，不过不对Key分组了</p>
<h3 id="fold"><a href="#fold" class="headerlink" title="fold"></a>fold</h3><p><code>fold(num)(func)</code></p>
<p>同上</p>
<h3 id="保存"><a href="#保存" class="headerlink" title="保存"></a>保存</h3><ul>
<li><code>saveAsTextFile(path)</code>：将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本</li>
<li><code>saveAsSequenceFile(path)</code>：将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统</li>
<li><code>saveAsObjectFile(path)</code>：用于将RDD中的元素序列化成对象，存储到文件中。</li>
</ul>
<h3 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h3><p><code>foreach(func)</code></p>
<p>在数据集的每一个元素上，运行函数func进行更新.</p>
<p><font color="red">此运行方法是在Executor中执行的，如果你打印一些数据，这将无意义。因为Driver端看不到</font></p>
<h2 id="RDD函数传递"><a href="#RDD函数传递" class="headerlink" title="RDD函数传递"></a>RDD函数传递</h2><p>在实际开发中我们往往需要自己定义一些对于RDD的操作，那么此时需要主要的是，<font color="red">初始化工作是在Driver端进行的，而实际运行程序是在Executor端进行的，这就涉及到了跨进程通信，是需要序列化的</font>。下面我们看几个例子：</p>
<h3 id="传递一个方法"><a href="#传递一个方法" class="headerlink" title="传递一个方法"></a>传递一个方法</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Search</span>(<span class="params">query:<span class="type">String</span></span>)</span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//过滤出包含字符串的数据</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isMatch</span></span>(s: <span class="type">String</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    s.contains(query)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//过滤出包含字符串的RDD</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getMatch1</span> </span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    rdd.filter(isMatch)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//过滤出包含字符串的RDD</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getMatche2</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    rdd.filter(x =&gt; x.contains(query))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.初始化配置信息及SparkContext</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.创建一个RDD</span></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.parallelize(<span class="type">Array</span>(<span class="string">&quot;hadoop&quot;</span>, <span class="string">&quot;spark&quot;</span>, <span class="string">&quot;hive&quot;</span>, <span class="string">&quot;atguigu&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.创建一个Search对象</span></span><br><span class="line">    <span class="keyword">val</span> search = <span class="keyword">new</span> <span class="type">Search</span>(<span class="string">&quot;h&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.运用第一个过滤函数并打印结果</span></span><br><span class="line">    <span class="keyword">val</span> match1: <span class="type">RDD</span>[<span class="type">String</span>] = search.getMatch1(rdd)</span><br><span class="line">    match1.collect().foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>产生错误：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; org.apache.spark.SparkException: Task not serializable</span><br><span class="line">    at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:298)</span><br><span class="line">    at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:288)</span><br><span class="line">    at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:108)</span><br><span class="line">    at org.apache.spark.SparkContext.clean(SparkContext.scala:2101)</span><br><span class="line">    at org.apache.spark.rdd.RDD$$anonfun$filter$1.apply(RDD.scala:387)</span><br><span class="line">    at org.apache.spark.rdd.RDD$$anonfun$filter$1.apply(RDD.scala:386)</span><br><span class="line">    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)</span><br><span class="line">    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)</span><br><span class="line">    at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)</span><br><span class="line">    at org.apache.spark.rdd.RDD.filter(RDD.scala:386)</span><br><span class="line">    at com.atguigu.Search.getMatche1(SeriTest.scala:39)</span><br><span class="line">    at com.atguigu.SeriTest$.main(SeriTest.scala:18)</span><br><span class="line">    at com.atguigu.SeriTest.main(SeriTest.scala)</span><br><span class="line">Caused by: java.io.NotSerializableException: com.atguigu.Search</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><font color="red">java.io.NotSerializableException: com.atguigu.Search</font></p>
<p><strong>错误原因</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//过滤出包含字符串的RDD</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getMatch1</span> </span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    rdd.filter(isMatch)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在这个方法中所调用的方法isMatch()是定义在Search这个类中的，实际上调用的是this. isMatch()，this表示Search这个类的对象，程序在运行过程中需要将Search对象序列化以后传递到Executor端。</p>
<p><strong>解决方案</strong></p>
<p>使类继承scala.Serializable即可。</p>
<p><code>class Search() extends Serializable&#123;...&#125;</code></p>
<h3 id="传递一个属性"><a href="#传递一个属性" class="headerlink" title="传递一个属性"></a>传递一个属性</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Search</span>(<span class="params">query:<span class="type">String</span></span>)</span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//过滤出包含字符串的数据</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isMatch</span></span>(s: <span class="type">String</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    s.contains(query)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//过滤出包含字符串的RDD</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getMatch1</span> </span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    rdd.filter(isMatch)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//过滤出包含字符串的RDD</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getMatche2</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    rdd.filter(x =&gt; x.contains(query))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.初始化配置信息及SparkContext</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.创建一个RDD</span></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.parallelize(<span class="type">Array</span>(<span class="string">&quot;hadoop&quot;</span>, <span class="string">&quot;spark&quot;</span>, <span class="string">&quot;hive&quot;</span>, <span class="string">&quot;atguigu&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.创建一个Search对象</span></span><br><span class="line">    <span class="keyword">val</span> search = <span class="keyword">new</span> <span class="type">Search</span>(<span class="string">&quot;h&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.运用第一个过滤函数并打印结果</span></span><br><span class="line">    <span class="keyword">val</span> match1: <span class="type">RDD</span>[<span class="type">String</span>] = search.getMatche2(rdd)</span><br><span class="line">    match1.collect().foreach(println)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>发现出现了同样的错误，错误原因</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//过滤出包含字符串的RDD</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">getMatche2</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">   rdd.filter(x =&gt; x.contains(query))</span><br><span class="line"> &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在这个方法中所调用的方法query是定义在<strong>Search这个类中的字段</strong>，实际上调用的是this. query，this表示Search这个类的对象，程序在运行过程中需要将Search对象序列化以后传递到Executor端。</p>
<p><strong>解决办法</strong></p>
<ol>
<li>将query变为变量，撇开与Search类的关系</li>
<li><code>class Search() extends Serializable&#123;...&#125;</code></li>
</ol>
<br/>

<h2 id="RDD依赖关系"><a href="#RDD依赖关系" class="headerlink" title="RDD依赖关系"></a>RDD依赖关系</h2><h3 id="血缘Lineage"><a href="#血缘Lineage" class="headerlink" title="血缘Lineage"></a>血缘Lineage</h3><p>RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。</p>
<p><strong>查看RDD存储的血缘</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.makeRDD(<span class="number">10</span> to <span class="number">15</span>).map((_,<span class="number">1</span>)).reduceByKey(_+_).toDebugString</span><br><span class="line">res0: <span class="type">String</span> =</span><br><span class="line">(<span class="number">2</span>) <span class="type">ShuffledRDD</span>[<span class="number">2</span>] at reduceByKey at &lt;console&gt;:<span class="number">25</span> []</span><br><span class="line"> +-(<span class="number">2</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">1</span>] at map at &lt;console&gt;:<span class="number">25</span> []</span><br><span class="line">    |  <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at makeRDD at &lt;console&gt;:<span class="number">25</span> []</span><br><span class="line"></span><br><span class="line">scala&gt; sc.makeRDD(<span class="number">10</span> to <span class="number">15</span>).map((_,<span class="number">1</span>)).toDebugString</span><br><span class="line">res1: <span class="type">String</span> =</span><br><span class="line">(<span class="number">2</span>) <span class="type">MapPartitionsRDD</span>[<span class="number">4</span>] at map at &lt;console&gt;:<span class="number">25</span> []</span><br><span class="line"> |  <span class="type">ParallelCollectionRDD</span>[<span class="number">3</span>] at makeRDD at &lt;console&gt;:<span class="number">25</span> []</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>

<p>我们可以清除的看到不同的RDD存储的血缘关系。</p>
<p> <font color="red">注意：RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。</font></p>
<h3 id="窄依赖"><a href="#窄依赖" class="headerlink" title="窄依赖"></a>窄依赖</h3><p>窄依赖指的是<strong>每一个父RDD的Partition最多被子RDD的一个Partition使用</strong>,窄依赖我们形象的比喻为独生子女</p>
<p><font color="red">父RDD的一个Partition只会有一个分支流到子类RDD</font></p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200823185106507.png" alt="image-20200823185106507"></p>
<h3 id="宽依赖"><a href="#宽依赖" class="headerlink" title="宽依赖"></a>宽依赖</h3><p>宽依赖指的是<strong>多个子RDD的Partition会依赖同一个父RDD的Partition</strong>，会引起shuffle,总结：宽依赖我们形象的比喻为超生</p>
<p><font color="red">父RDD的一个Partition会有多个分支流到子类RDD</font></p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200823185145355.png" alt="image-20200823185145355"></p>
<h3 id="DAG"><a href="#DAG" class="headerlink" title="DAG"></a>DAG</h3><p><code>DAG(Directed Acyclic Graph)</code>叫做有向无环图。有向无环图是一种拓扑图，而拓扑图就是由点和线组成的图形。而DAG被称为有向无环图，是它的图形不允许形成环结构。</p>
<p>原始的RDD通过一系列的转换就就形成了DAG，<strong>根据RDD之间的依赖关系的不同将DAG划分成不同的Stage</strong>。</p>
<ul>
<li>对于窄依赖，<strong>partition的转换处理在Stage中完成计算。</strong></li>
<li>对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，<font color="red">因此<strong>宽依赖是划分Stage的依据</strong>。</font></li>
</ul>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200823185734662.png" alt="image-20200823185734662"></p>
<h3 id="任务划分🔺"><a href="#任务划分🔺" class="headerlink" title="任务划分🔺"></a>任务划分🔺</h3><p>RDD任务切分中间分为：</p>
<ol>
<li><code>Application</code>：初始化一个<code>SparkContext</code>即生成一个<code>Application</code></li>
<li><code>Job</code>：一个<code>Action</code>算子就会生成一个Job</li>
<li><code>Stage</code>：根据RDD之间的依赖关系的不同将<code>Job</code>划分成不同的Stage，<strong>遇到一个宽依赖则划分一个Stage。</strong></li>
<li><code>Task</code>：在集群运行时最小的执行单元(Thread)</li>
</ol>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200823185925589.png" alt="image-20200823185925589"></p>
<p>task0:这条线所贯穿的所有的partition中的计算逻辑，并且以递归函数的形式整合在一起，例如：fun2(fun1(textFile(blcok1))),这个计算最好发送到block1或者它的副本节点上去计算</p>
<p><font color="red">注意：Application-&gt;Job-&gt;Stage-&gt; Task每一层都是1对n的关系。</font></p>
<h2 id="RDD缓存"><a href="#RDD缓存" class="headerlink" title="RDD缓存"></a>RDD缓存</h2><p>RDD通过<code>persist</code>方法或<code>cache</code>方法可以将前面的计算结果缓存，默认情况下 <code>persist() </code>会把数据以序列化的形式缓存在 <strong>JVM 的堆空间中。</strong> </p>
<p><strong>但是并不是这两个方法被调用时立即缓存，而是触发后面的action时</strong>，该RDD将会被缓存在计算节点的内存中，并供后面重用。</p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200823190053597.png" alt="image-20200823190053597"></p>
<p>通过查看源码发现<code>cache</code>最终也是调用了<code>persist</code>方法，<font color="red">默认的存储级别都是仅在内存存储一份</font>，Spark的存储级别还有好多种，存储级别在<code>object StorageLevel</code>中定义的。</p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200823190209111.png" alt="image-20200823190209111"></p>
<p><strong>在存储级别的末尾加上“_2”来把持久化数据存为两份</strong></p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200823190444179.png" alt="image-20200823190444179"></p>
<p>缓存有可能丢失，或者存储存储于内存的数据由于内存不足而被删除，<strong>RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行</strong>。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。</p>
<p><strong>例如：</strong></p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200823191051834.png" alt="image-20200823191051834"></p>
<ul>
<li>缓存的RDD每次内容一致</li>
<li>没有被缓存的RDD每次内容不一致，证明是每次都重新计算</li>
</ul>
<p><font color="red"><strong>注意：</strong>但是并不是这两个方法被调用时立即缓存，而是触发后面的action时<strong>，该RDD将会被缓存在计算节点的内存中，后面再次使用被缓存的RDD才是用缓存中拿取，不是重新计算。被缓存的RDD必须由第一个action算子来触发。</strong></font></p>
<h2 id="RDD-CheckPoint"><a href="#RDD-CheckPoint" class="headerlink" title="RDD CheckPoint"></a>RDD CheckPoint</h2><p>Spark中对于数据的保存除了持久化操作之外，<strong>还提供了一种检查点的机制，检查点（本质是通过将RDD写入Disk做检查点）是为了通过lineage做容错的辅助</strong>，lineage过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，<strong>如果之后有节点出现问题而丢失分区，从做检查点的RDD开始重做Lineage，就会减少开销</strong>。检查点通过将数据写入到HDFS文件系统实现了RDD的检查点功能。</p>
<p>为当前RDD设置检查点。该函数将会创建一个二进制的文件，并存储到checkpoint目录中，该目录是用<code>SparkContext.setCheckpointDir()</code>设置的。在checkpoint的过程中，<font color="red"><strong>该RDD的所有依赖于父RDD中的信息将全部被移除。对RDD进行checkpoint操作并不会马上被执行，必须执行Action操作才能触发</strong>。</font></p>
<p>必须设置检查点的路径：</p>
<p><code>sc.setCheckpointDir(&quot;hdfs://hadoop102:9000/checkpoint&quot;)</code></p>
<h1 id="RDD分区"><a href="#RDD分区" class="headerlink" title="RDD分区"></a>RDD分区</h1><p>Spark目前支持<code>Hash分区</code>和<code>Range分区</code>，用户也可以自定义分区，<strong>Hash分区为当前的默认分区</strong>，Spark中分区器直接决定了RDD中分区的个数、RDD中每条数据经过Shuffle过程属于哪个分区和Reduce的个数</p>
<p>注意：</p>
<ol>
<li>只有Key-Value类型的RDD才有分区器的，非Key-Value类型的RDD分区器的值是None</li>
<li>每个RDD的分区ID范围：0~numPartitions-1，决定这个值是属于那个分区的。</li>
</ol>
<h2 id="Hash分区"><a href="#Hash分区" class="headerlink" title="Hash分区"></a>Hash分区</h2><p><a href="#partitionBy">查看</a></p>
<h2 id="自定义分区"><a href="#自定义分区" class="headerlink" title="自定义分区"></a>自定义分区</h2><ol>
<li>继承<code>org.apache.spark.Partitioner</code></li>
<li>实现<code>numPartitions: Int</code>:返回创建出来的分区数</li>
<li>实现<code>getPartition(key: Any): Int</code>:返回给定键的分区编号(0到numPartitions-1)</li>
<li><code>equals():Java</code> 判断相等性的标准方法。这个方法的实现非常重要，Spark 需要用这个方法来检查你的分区器对象是否和其他分区器实例相同，这样 Spark 才可以判断两个 RDD 的分区方式是否相同。</li>
</ol>
<h1 id="数据读取与保存🔺"><a href="#数据读取与保存🔺" class="headerlink" title="数据读取与保存🔺"></a>数据读取与保存🔺</h1><p>Spark的数据读取及数据保存可以从两个维度来作区分：<font color="red"><strong>文件格式以及文件系统。</strong></font></p>
<p>文件格式分为：<strong>Text文件、Json文件</strong>、Csv文件、Sequence文件以及Object文件；</p>
<p>文件系统分为：本地文件系统<strong>、HDFS、HBASE</strong>以及数据库。</p>
<h2 id="文件格式数据读取与保存"><a href="#文件格式数据读取与保存" class="headerlink" title="文件格式数据读取与保存"></a>文件格式数据读取与保存</h2><h3 id="Text"><a href="#Text" class="headerlink" title="Text"></a>Text</h3><h4 id="读取"><a href="#读取" class="headerlink" title="读取"></a>读取</h4><p><code>val hdfsFile = sc.textFile(&quot;hdfs://hadoop102:9000/fruit.txt&quot;)</code></p>
<h4 id="保存-1"><a href="#保存-1" class="headerlink" title="保存"></a>保存</h4><p><code>hdfsFile.saveAsTextFile(&quot;/fruitOut&quot;)</code></p>
<h3 id="Json"><a href="#Json" class="headerlink" title="Json"></a>Json</h3><p><font color="red">Spark读取的Json文件其实并不符合Json文件的规范，它的原理是每行是一个完整的Json但是整个文件并不是Json。他按照每行读取，然后利用相关的JSON库对每一条数据进行JSON解析</font></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.util.parsing.json.<span class="type">JSON</span></span><br><span class="line"></span><br><span class="line">sc.textFile(<span class="string">&quot;/people.json&quot;</span>).map(<span class="type">JSON</span>.parseFull) <span class="comment">// 形成Map</span></span><br></pre></td></tr></table></figure>

<h3 id="Sequence"><a href="#Sequence" class="headerlink" title="Sequence"></a>Sequence</h3><p> SequenceFile文件是Hadoop用来存储二进制形式的key-value对而设计的一种平面文件(Flat File)。Spark 有专门用来读取 SequenceFile 的接口。<br><font color="red"><strong>注意：SequenceFile文件只针对PairRDD</strong></font></p>
<h4 id="读取-1"><a href="#读取-1" class="headerlink" title="读取"></a>读取</h4><p><code>val seq = sc.sequenceFile[Int,Int](&quot;file:///opt/module/spark/seqFile&quot;)</code></p>
<h4 id="保存-2"><a href="#保存-2" class="headerlink" title="保存"></a>保存</h4><p><code>rdd.saveAsSequenceFile(&quot;file:///opt/module/spark/seqFile&quot;)</code></p>
<h3 id="对象文件"><a href="#对象文件" class="headerlink" title="对象文件"></a>对象文件</h3><p>对象文件是将对象序列化后保存的文件，采用Java的序列化机制。可以通过objectFile<a href="path">k,v</a> 函数接收一个路径，读取对象文件，返回对应的 RDD，也可以通过调用saveAsObjectFile() 实现对对象文件的输出。因为是序列化所以要指定类型。</p>
<h4 id="读取-2"><a href="#读取-2" class="headerlink" title="读取"></a>读取</h4><p><code> val objFile = sc.objectFile[Int](&quot;file:///opt/module/spark/objectFile&quot;)</code></p>
<h4 id="保存-3"><a href="#保存-3" class="headerlink" title="保存"></a>保存</h4><p><code>rdd.saveAsObjectFile(&quot;file:///opt/module/spark/objectFile&quot;)</code></p>
<h2 id="文件系统类数据读取与保存"><a href="#文件系统类数据读取与保存" class="headerlink" title="文件系统类数据读取与保存"></a>文件系统类数据读取与保存</h2><h3 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h3><p><a href="#Text">文件格式数据读取与保存/Text</a>中相同的操作，读取与保存时输入：<code>hdfs://主机/xxx</code></p>
<h3 id="MYSQL数据库连接"><a href="#MYSQL数据库连接" class="headerlink" title="MYSQL数据库连接"></a>MYSQL数据库连接</h3><p>支持通过Java JDBC访问关系型数据库。需要通过JdbcRDD进行，示例如下:</p>
<p><strong>pom依赖</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.27<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="读取-3"><a href="#读取-3" class="headerlink" title="读取"></a>读取</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"> <span class="class"><span class="keyword">object</span> <span class="title">Read</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.创建spark配置信息</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;JdbcRDD&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.创建SparkContext</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.定义连接mysql的参数</span></span><br><span class="line">    <span class="keyword">val</span> driver = <span class="string">&quot;com.mysql.jdbc.Driver&quot;</span></span><br><span class="line">    <span class="keyword">val</span> url = <span class="string">&quot;jdbc:mysql://192.168.0.201:3306/zxj&quot;</span></span><br><span class="line">    <span class="keyword">val</span> userName = <span class="string">&quot;root&quot;</span></span><br><span class="line">    <span class="keyword">val</span> passWd = <span class="string">&quot;123456&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建JdbcRDD</span></span><br><span class="line">    <span class="keyword">val</span> rdd = <span class="keyword">new</span> <span class="type">JdbcRDD</span>(sc, () =&gt; &#123;</span><br><span class="line">      <span class="type">Class</span>.forName(driver)</span><br><span class="line">      <span class="type">DriverManager</span>.getConnection(url, userName, passWd)</span><br><span class="line">    &#125;,</span><br><span class="line">      <span class="string">&quot;select * from `test` where `id`&gt;=?;&quot;</span>,</span><br><span class="line">      <span class="number">1</span>,</span><br><span class="line">      <span class="number">10</span>,</span><br><span class="line">      <span class="number">1</span>,</span><br><span class="line">      r =&gt; (r.getInt(<span class="number">1</span>), r.getString(<span class="number">2</span>))</span><br><span class="line">    )</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="写"><a href="#写" class="headerlink" title="写"></a>写</h4><p><strong>错误实例</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Write</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[2]&quot;</span>).setAppName(<span class="string">&quot;HBaseApp&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line">    <span class="keyword">val</span> data = sc.parallelize(<span class="type">List</span>(<span class="string">&quot;Female&quot;</span>, <span class="string">&quot;Male&quot;</span>,<span class="string">&quot;Female&quot;</span>))</span><br><span class="line"></span><br><span class="line">    data.foreach(insertData)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">insertData</span></span>(iterator: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="type">Class</span>.forName (<span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>).newInstance()</span><br><span class="line">    <span class="keyword">val</span> conn: <span class="type">Connection</span> = java.sql.<span class="type">DriverManager</span>.getConnection(<span class="string">&quot;jdbc:mysql://192.168.0.201:3306/zxj&quot;</span>, <span class="string">&quot;root&quot;</span>, </span><br><span class="line">      <span class="string">&quot;123456&quot;</span>)</span><br><span class="line">   </span><br><span class="line">      <span class="keyword">val</span> ps: <span class="type">PreparedStatement</span> = conn.prepareStatement(<span class="string">&quot;insert into rddtable(name) values (?)&quot;</span>)</span><br><span class="line">      ps.setString(<span class="number">1</span>, iterator)</span><br><span class="line">      ps.executeUpdate()</span><br><span class="line">  </span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><font color="red"><strong>这样每个分区中的每条数据都会创建一个连接对象，这是极其不好的，因为大数据动辄前万条数据，这样创建那么多数据库连接对象会拖垮数据库。</strong></font></p>
<p>这样我们可以提出两种解决方法：</p>
<ol>
<li>将数据库连接创建提出，放入Driver端代码。但是这样报错，数据库连接对象没有序列化，这条路封死了，因为我们无法序列化第三方提供的类</li>
<li>使用foreachPartition方法，使每个分区创建一个连接对象，分区内数据共用一个连接对象，这样也能大大减少连接对象的创建。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Write</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[2]&quot;</span>).setAppName(<span class="string">&quot;HBaseApp&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line">    <span class="keyword">val</span> data = sc.parallelize(<span class="type">List</span>(<span class="string">&quot;Female&quot;</span>, <span class="string">&quot;Male&quot;</span>,<span class="string">&quot;Female&quot;</span>))</span><br><span class="line"></span><br><span class="line">    data.foreachPartition(insertData)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">insertData</span></span>(iterator: <span class="type">Iterator</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="type">Class</span>.forName (<span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>).newInstance()</span><br><span class="line">    <span class="keyword">val</span> conn: <span class="type">Connection</span> = java.sql.<span class="type">DriverManager</span>.getConnection(<span class="string">&quot;jdbc:mysql://192.168.0.201:3306/zxj&quot;</span>, <span class="string">&quot;root&quot;</span>,</span><br><span class="line">      <span class="string">&quot;123456&quot;</span>)</span><br><span class="line">    iterator.foreach(data =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> ps: <span class="type">PreparedStatement</span> = conn.prepareStatement(<span class="string">&quot;insert into rddtable(name) values (?)&quot;</span>)</span><br><span class="line">      ps.setString(<span class="number">1</span>, data)</span><br><span class="line">      ps.executeUpdate()</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h3><p>由于<code>org.apache.hadoop.hbase.mapreduce.TableInputFormat </code>类的实现，Spark 可以通过Hadoop输入格式访问HBase。这个输入格式会返回键值对数据，其中键的类型为<code>org. apache.hadoop.hbase.io.ImmutableBytesWritable</code>，而值的类型为<code>org.apache.hadoop.hbase.client.Result</code>。</p>
<ul>
<li><code>org. apache.hadoop.hbase.io.ImmutableBytesWritable</code>指的是Rowkey</li>
<li><code>org.apache.hadoop.hbase.client.Result</code>指得是查询结果</li>
<li><code>put</code>需要插入的数据</li>
</ul>
<p><strong>pom</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.<span class="type">HBaseConfiguration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.<span class="type">Result</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.<span class="type">ImmutableBytesWritable</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.<span class="type">TableInputFormat</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.<span class="type">Bytes</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HBaseSpark</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建spark配置信息</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;JdbcRDD&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建SparkContext</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//构建HBase配置信息</span></span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">Configuration</span> = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">      <span class="comment">// 使用配置文件或者在代码中编写</span></span><br><span class="line">    conf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop102,hadoop103,hadoop104&quot;</span>)</span><br><span class="line">      <span class="comment">// 声明要读取的表，然后传入读取函数</span></span><br><span class="line">    conf.set(<span class="type">TableInputFormat</span>.<span class="type">INPUT_TABLE</span>, <span class="string">&quot;rddtable&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//从HBase读取数据形成RDD</span></span><br><span class="line">    <span class="keyword">val</span> hbaseRDD: <span class="type">RDD</span>[(<span class="type">ImmutableBytesWritable</span>, <span class="type">Result</span>)] = sc.newAPIHadoopRDD(</span><br><span class="line">      conf,</span><br><span class="line">      classOf[<span class="type">TableInputFormat</span>],</span><br><span class="line">      classOf[<span class="type">ImmutableBytesWritable</span>],</span><br><span class="line">      classOf[<span class="type">Result</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> count: <span class="type">Long</span> = hbaseRDD.count()</span><br><span class="line">    println(count)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//对hbaseRDD进行处理</span></span><br><span class="line">    hbaseRDD.foreach &#123;</span><br><span class="line">      <span class="keyword">case</span> (_, result) =&gt;</span><br><span class="line">        <span class="keyword">val</span> key: <span class="type">String</span> = <span class="type">Bytes</span>.toString(result.getRow)</span><br><span class="line">        <span class="keyword">val</span> name: <span class="type">String</span> = <span class="type">Bytes</span>.toString(result.getValue(<span class="type">Bytes</span>.toBytes(<span class="string">&quot;info&quot;</span>), <span class="type">Bytes</span>.toBytes(<span class="string">&quot;name&quot;</span>)))</span><br><span class="line">        <span class="keyword">val</span> color: <span class="type">String</span> = <span class="type">Bytes</span>.toString(result.getValue(<span class="type">Bytes</span>.toBytes(<span class="string">&quot;info&quot;</span>), <span class="type">Bytes</span>.toBytes(<span class="string">&quot;color&quot;</span>)))</span><br><span class="line">        println(<span class="string">&quot;RowKey:&quot;</span> + key + <span class="string">&quot;,Name:&quot;</span> + name + <span class="string">&quot;,Color:&quot;</span> + color)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//关闭连接</span></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="数据保存"><a href="#数据保存" class="headerlink" title="数据保存"></a>数据保存</h4><p>我们只需要构建rowkey,put类型对象即可</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"><span class="comment">//获取Spark配置信息并创建与spark的连接</span></span><br><span class="line">  <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;HBaseApp&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建HBaseConf</span></span><br><span class="line">  <span class="keyword">val</span> conf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">  <span class="keyword">val</span> jobConf = <span class="keyword">new</span> <span class="type">JobConf</span>(conf)</span><br><span class="line">  jobConf.setOutputFormat(classOf[<span class="type">TableOutputFormat</span>])</span><br><span class="line">  jobConf.set(<span class="type">TableOutputFormat</span>.<span class="type">OUTPUT_TABLE</span>, <span class="string">&quot;fruit_spark&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//构建Hbase表描述器</span></span><br><span class="line">  <span class="keyword">val</span> fruitTable = <span class="type">TableName</span>.valueOf(<span class="string">&quot;fruit_spark&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> tableDescr = <span class="keyword">new</span> <span class="type">HTableDescriptor</span>(fruitTable)</span><br><span class="line">  tableDescr.addFamily(<span class="keyword">new</span> <span class="type">HColumnDescriptor</span>(<span class="string">&quot;info&quot;</span>.getBytes))</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建Hbase表</span></span><br><span class="line">  <span class="keyword">val</span> admin = <span class="keyword">new</span> <span class="type">HBaseAdmin</span>(conf)</span><br><span class="line">  <span class="keyword">if</span> (admin.tableExists(fruitTable)) &#123;</span><br><span class="line">    admin.disableTable(fruitTable)</span><br><span class="line">    admin.deleteTable(fruitTable)</span><br><span class="line">  &#125;</span><br><span class="line">  admin.createTable(tableDescr)</span><br><span class="line"></span><br><span class="line"><span class="comment">//定义往Hbase插入数据的方法</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">convert</span></span>(triple: (<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Int</span>)) = &#123;</span><br><span class="line">    <span class="keyword">val</span> put = <span class="keyword">new</span> <span class="type">Put</span>(<span class="type">Bytes</span>.toBytes(triple._1))</span><br><span class="line">    put.addImmutable(<span class="type">Bytes</span>.toBytes(<span class="string">&quot;info&quot;</span>), <span class="type">Bytes</span>.toBytes(<span class="string">&quot;name&quot;</span>), <span class="type">Bytes</span>.toBytes(triple._2))</span><br><span class="line">    put.addImmutable(<span class="type">Bytes</span>.toBytes(<span class="string">&quot;info&quot;</span>), <span class="type">Bytes</span>.toBytes(<span class="string">&quot;price&quot;</span>), <span class="type">Bytes</span>.toBytes(triple._3))</span><br><span class="line">    (<span class="keyword">new</span> <span class="type">ImmutableBytesWritable</span>, put)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建一个RDD</span></span><br><span class="line">  <span class="keyword">val</span> initialRDD = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="string">&quot;apple&quot;</span>,<span class="number">11</span>), (<span class="number">2</span>,<span class="string">&quot;banana&quot;</span>,<span class="number">12</span>), (<span class="number">3</span>,<span class="string">&quot;pear&quot;</span>,<span class="number">13</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment">//将RDD内容写到HBase</span></span><br><span class="line">  <span class="keyword">val</span> localData = initialRDD.map(convert)</span><br><span class="line"></span><br><span class="line">  localData.saveAsHadoopDataset(jobConf)</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200823202947430.png" alt="image-20200823202947430"></p>
<p>输入可能以多个文件的形式存储在HDFS上，每个File都包含了很多块，称为Block。当Spark读取这些文件作为输入时，会根据具体数据格式对应的InputFormat进行解析，一般是将若干个Block合并成一个输入分片，称为InputSplit，注意InputSplit不能跨越文件。随后将为这些输入分片生成具体的Task。InputSplit与Task是一一对应的关系。随后这些具体的Task每个都会被分配到集群上的某个节点的某个Executor去执行。</p>
<ol>
<li>每个节点可以起一个或多个Executor。</li>
<li>每个Executor由若干core组成，每个Executor的每个core一次只能执行一个Task。</li>
<li>每个Task执行的结果就是生成了目标RDD的一个partiton。</li>
</ol>
<p>注意: 这里的core是虚拟的core而不是机器的物理CPU核，可以理解为就是Executor的一个工作线程。而 Task被执行的并发度 = Executor数目 * 每个Executor核数</p>
<p>至于partition的数目：</p>
<ol>
<li>对于数据读入阶段，例如sc.textFile，输入文件被划分为多少InputSplit就会需要多少初始Task。</li>
<li>在Map阶段partition数目保持不变。</li>
<li>在Reduce阶段，RDD的聚合会触发shuffle操作，聚合后的RDD的partition数目跟具体操作有关，例如repartition操作会聚合成指定分区数，还有一些算子是可配置的。</li>
</ol>
<p>RDD在计算的时候，每个分区都会起一个task，所以rdd的分区数目决定了总的的task数目。申请的计算节点（Executor）数目和每个计算节点核数，决定了你同一时刻可以并行执行的task。</p>
<p>比如的RDD有100个分区，那么计算的时候就会生成100个task，你的资源配置为10个计算节点，每个两2个核，同一时刻可以并行的task数目为20，计算这个RDD就需要5个轮次。如果计算资源不变，你有101个task的话，就需要6个轮次，在最后一轮中，只有一个task在执行，其余核都在空转。如果资源不变，你的RDD只有2个分区，那么同一时刻只有2个task运行，其余18个核空转，造成资源浪费。这就是在spark调优中，增大RDD分区数目，增大任务并行度的做法。</p>
<h2 id="Spark中的各个数量"><a href="#Spark中的各个数量" class="headerlink" title="Spark中的各个数量"></a>Spark中的各个数量</h2><h3 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h3><p>可以在运行参数中设置，设置每个Executor的core，或者总共的core。这里的core并不是指物理核数，而是Executor的Task并行度</p>
<h3 id="partition"><a href="#partition" class="headerlink" title="partition"></a>partition</h3><ul>
<li>默认情况下，读取文件采用的是Hadoop的切片规则，如果读取内存中的数据，可以根据特定的算法进行设定,</li>
<li>可以通过其他算子进行改变。</li>
<li>多个阶段的场合，下一个阶段的分区数量取决于上一个阶段最后RDD的分区数,但是可以在相应的算子中进行修改</li>
</ul>
<h3 id="Stage"><a href="#Stage" class="headerlink" title="Stage"></a>Stage</h3><p>（ResultStage）  + Shuffle依赖的数量( ShuffleMapStage )<br>    划分阶段的目的就是为了任务执行的等待，因为Shuffle的过程需要落盘</p>
<h3 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h3><p>Task：原则上一个分区就是一个任务，但是实际应用中，可以动态调整</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Xiangjie</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://awslzhang.top/2020/08/22/spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-RDD/">https://awslzhang.top/2020/08/22/spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-RDD/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://awslzhang.top" target="_blank">zxj</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><div class="post_share"><div class="social-share" data-image="https://spark.apache.org/images/spark-logo-trademark.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/08/23/spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F%E5%8F%8A%E7%B4%AF%E5%8A%A0%E5%99%A8/"><img class="prev-cover" src="https://spark.apache.org/images/spark-logo-trademark.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">spark学习笔记-广播变量及累加器</div></div></a></div><div class="next-post pull-right"><a href="/2020/08/19/spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AE%80%E5%8D%95%E4%BA%86%E8%A7%A3/"><img class="next-cover" src="https://spark.apache.org/images/spark-logo-trademark.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">spark学习笔记-简单了解</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/08/23/spark学习笔记-广播变量及累加器/" title="spark学习笔记-广播变量及累加器"><img class="cover" src="https://spark.apache.org/images/spark-logo-trademark.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-23</div><div class="title">spark学习笔记-广播变量及累加器</div></div></a></div><div><a href="/2020/08/24/spark学习笔记-SparkSQL/" title="spark学习笔记-SparkSQL"><img class="cover" src="https://spark.apache.org/images/spark-logo-trademark.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-24</div><div class="title">spark学习笔记-SparkSQL</div></div></a></div><div><a href="/2020/08/26/spark学习笔记-SparkStreaming/" title="spark学习笔记-SparkStreaming"><img class="cover" src="https://spark.apache.org/images/spark-logo-trademark.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-26</div><div class="title">spark学习笔记-SparkStreaming</div></div></a></div><div><a href="/2020/08/19/spark学习笔记-简单了解/" title="spark学习笔记-简单了解"><img class="cover" src="https://spark.apache.org/images/spark-logo-trademark.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-19</div><div class="title">spark学习笔记-简单了解</div></div></a></div><div><a href="/2020/08/29/spark学习笔记-SparkCore解析/" title="spark学习笔记-SparkCore解析"><img class="cover" src="https://spark.apache.org/images/spark-logo-trademark.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-29</div><div class="title">spark学习笔记-SparkCore解析</div></div></a></div></div></div></div><div class="aside_content" id="aside_content"><div class="card-widget card-info"><div class="card-content"><div class="card-info-avatar is-center"><img class="avatar-img" src="/null" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Xiangjie</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">83</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">43</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">25</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/XiangJie-Zhang" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:qluzxj@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div></div><div class="card-widget card-announcement"><div class="card-content"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="card-content"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Java-IO"><span class="toc-number">1.1.</span> <span class="toc-text">Java IO</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Java-IO%E7%9A%84%E8%A3%85%E9%A5%B0%E8%80%85%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.1.1.</span> <span class="toc-text">Java IO的装饰者模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Java-IO%E7%9A%84%E8%AF%BB%E5%8F%96"><span class="toc-number">1.1.2.</span> <span class="toc-text">Java IO的读取</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#RDD%E6%A6%82%E8%BF%B0"><span class="toc-number">2.</span> <span class="toc-text">RDD概述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD%E7%9A%84%E7%AE%80%E4%BB%8B"><span class="toc-number">2.1.</span> <span class="toc-text">RDD的简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD%E7%9A%84%E5%B1%9E%E6%80%A7"><span class="toc-number">2.2.</span> <span class="toc-text">RDD的属性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD%E7%89%B9%E7%82%B9"><span class="toc-number">2.3.</span> <span class="toc-text">RDD特点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%8C%BA"><span class="toc-number">2.3.1.</span> <span class="toc-text">分区</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8D%E5%8F%AF%E5%8F%98-%E5%8F%AA%E8%AF%BB"><span class="toc-number">2.3.2.</span> <span class="toc-text">不可变(只读)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BE%9D%E8%B5%96"><span class="toc-number">2.3.3.</span> <span class="toc-text">依赖</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%93%E5%AD%98"><span class="toc-number">2.3.4.</span> <span class="toc-text">缓存</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#RDD%E7%BC%96%E7%A8%8B"><span class="toc-number">3.</span> <span class="toc-text">RDD编程</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.1.</span> <span class="toc-text">RDD编程模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD%E5%88%9B%E5%BB%BA"><span class="toc-number">3.2.</span> <span class="toc-text">RDD创建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9B%86%E5%90%88%E4%B8%AD%E5%88%9B%E5%BB%BA"><span class="toc-number">3.2.1.</span> <span class="toc-text">集合中创建</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#makeRDD"><span class="toc-number">3.2.1.1.</span> <span class="toc-text">makeRDD</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#parallelize"><span class="toc-number">3.2.1.2.</span> <span class="toc-text">parallelize</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%B1%E5%A4%96%E9%83%A8%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%9B%E5%BB%BA"><span class="toc-number">3.2.2.</span> <span class="toc-text">由外部存储系统的数据集创建</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#textFile"><span class="toc-number">3.2.2.1.</span> <span class="toc-text">textFile</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD%E7%9A%84%E8%BD%AC%E6%8D%A2%E7%AE%97%E5%AD%90%F0%9F%94%BA"><span class="toc-number">3.3.</span> <span class="toc-text">RDD的转换算子🔺</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Value%E7%B1%BB%E5%9E%8B"><span class="toc-number">3.3.1.</span> <span class="toc-text">Value类型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#map"><span class="toc-number">3.3.1.1.</span> <span class="toc-text">map</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#mapPartitions"><span class="toc-number">3.3.1.2.</span> <span class="toc-text">mapPartitions</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#mapPartitionsWithIndex"><span class="toc-number">3.3.1.3.</span> <span class="toc-text">mapPartitionsWithIndex</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#flatMap"><span class="toc-number">3.3.1.4.</span> <span class="toc-text">flatMap</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#glom"><span class="toc-number">3.3.1.5.</span> <span class="toc-text">glom</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#groupBy"><span class="toc-number">3.3.1.6.</span> <span class="toc-text">groupBy</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#filter"><span class="toc-number">3.3.1.7.</span> <span class="toc-text">filter</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#sample"><span class="toc-number">3.3.1.8.</span> <span class="toc-text">sample</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#distinct"><span class="toc-number">3.3.1.9.</span> <span class="toc-text">distinct</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#coalesce"><span class="toc-number">3.3.1.10.</span> <span class="toc-text">coalesce</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#repartition"><span class="toc-number">3.3.1.11.</span> <span class="toc-text">repartition</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#sortBy"><span class="toc-number">3.3.1.12.</span> <span class="toc-text">sortBy</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8CV%E7%B1%BB%E5%9E%8B"><span class="toc-number">3.3.2.</span> <span class="toc-text">双V类型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#union"><span class="toc-number">3.3.2.1.</span> <span class="toc-text">union</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#subtract"><span class="toc-number">3.3.2.2.</span> <span class="toc-text">subtract</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#intersection"><span class="toc-number">3.3.2.3.</span> <span class="toc-text">intersection</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#zip"><span class="toc-number">3.3.2.4.</span> <span class="toc-text">zip</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#KV%E7%B1%BB%E5%9E%8B"><span class="toc-number">3.3.3.</span> <span class="toc-text">KV类型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#partitionBy"><span class="toc-number">3.3.3.1.</span> <span class="toc-text">partitionBy</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#groupByKey"><span class="toc-number">3.3.3.2.</span> <span class="toc-text">groupByKey</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#reduceByKey"><span class="toc-number">3.3.3.3.</span> <span class="toc-text">reduceByKey</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#aggregateByKey%F0%9F%94%BA"><span class="toc-number">3.3.3.4.</span> <span class="toc-text">aggregateByKey🔺</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#foldByKey%F0%9F%94%BA"><span class="toc-number">3.3.3.5.</span> <span class="toc-text">foldByKey🔺</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#combineByKey-C-%F0%9F%94%BA"><span class="toc-number">3.3.3.6.</span> <span class="toc-text">combineByKey[C]🔺</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#sortByKey"><span class="toc-number">3.3.3.7.</span> <span class="toc-text">sortByKey</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#mapValues"><span class="toc-number">3.3.3.8.</span> <span class="toc-text">mapValues</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#join"><span class="toc-number">3.3.3.9.</span> <span class="toc-text">join</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#cogroup"><span class="toc-number">3.3.3.10.</span> <span class="toc-text">cogroup</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Action%E7%AE%97%E5%AD%90%F0%9F%94%BA"><span class="toc-number">3.4.</span> <span class="toc-text">Action算子🔺</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#reduce"><span class="toc-number">3.4.1.</span> <span class="toc-text">reduce</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#collect"><span class="toc-number">3.4.2.</span> <span class="toc-text">collect()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#count"><span class="toc-number">3.4.3.</span> <span class="toc-text">count()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%96%E5%9B%BA%E5%AE%9A%E5%80%BC"><span class="toc-number">3.4.4.</span> <span class="toc-text">取固定值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#aggregate"><span class="toc-number">3.4.5.</span> <span class="toc-text">aggregate</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#fold"><span class="toc-number">3.4.6.</span> <span class="toc-text">fold</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98"><span class="toc-number">3.4.7.</span> <span class="toc-text">保存</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#foreach"><span class="toc-number">3.4.8.</span> <span class="toc-text">foreach</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD%E5%87%BD%E6%95%B0%E4%BC%A0%E9%80%92"><span class="toc-number">3.5.</span> <span class="toc-text">RDD函数传递</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%A0%E9%80%92%E4%B8%80%E4%B8%AA%E6%96%B9%E6%B3%95"><span class="toc-number">3.5.1.</span> <span class="toc-text">传递一个方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%A0%E9%80%92%E4%B8%80%E4%B8%AA%E5%B1%9E%E6%80%A7"><span class="toc-number">3.5.2.</span> <span class="toc-text">传递一个属性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB"><span class="toc-number">3.6.</span> <span class="toc-text">RDD依赖关系</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%80%E7%BC%98Lineage"><span class="toc-number">3.6.1.</span> <span class="toc-text">血缘Lineage</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AA%84%E4%BE%9D%E8%B5%96"><span class="toc-number">3.6.2.</span> <span class="toc-text">窄依赖</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%BD%E4%BE%9D%E8%B5%96"><span class="toc-number">3.6.3.</span> <span class="toc-text">宽依赖</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DAG"><span class="toc-number">3.6.4.</span> <span class="toc-text">DAG</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E5%88%92%E5%88%86%F0%9F%94%BA"><span class="toc-number">3.6.5.</span> <span class="toc-text">任务划分🔺</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD%E7%BC%93%E5%AD%98"><span class="toc-number">3.7.</span> <span class="toc-text">RDD缓存</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD-CheckPoint"><span class="toc-number">3.8.</span> <span class="toc-text">RDD CheckPoint</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#RDD%E5%88%86%E5%8C%BA"><span class="toc-number">4.</span> <span class="toc-text">RDD分区</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Hash%E5%88%86%E5%8C%BA"><span class="toc-number">4.1.</span> <span class="toc-text">Hash分区</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%86%E5%8C%BA"><span class="toc-number">4.2.</span> <span class="toc-text">自定义分区</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E4%B8%8E%E4%BF%9D%E5%AD%98%F0%9F%94%BA"><span class="toc-number">5.</span> <span class="toc-text">数据读取与保存🔺</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E4%B8%8E%E4%BF%9D%E5%AD%98"><span class="toc-number">5.1.</span> <span class="toc-text">文件格式数据读取与保存</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Text"><span class="toc-number">5.1.1.</span> <span class="toc-text">Text</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96"><span class="toc-number">5.1.1.1.</span> <span class="toc-text">读取</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98-1"><span class="toc-number">5.1.1.2.</span> <span class="toc-text">保存</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Json"><span class="toc-number">5.1.2.</span> <span class="toc-text">Json</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sequence"><span class="toc-number">5.1.3.</span> <span class="toc-text">Sequence</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96-1"><span class="toc-number">5.1.3.1.</span> <span class="toc-text">读取</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98-2"><span class="toc-number">5.1.3.2.</span> <span class="toc-text">保存</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E8%B1%A1%E6%96%87%E4%BB%B6"><span class="toc-number">5.1.4.</span> <span class="toc-text">对象文件</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96-2"><span class="toc-number">5.1.4.1.</span> <span class="toc-text">读取</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98-3"><span class="toc-number">5.1.4.2.</span> <span class="toc-text">保存</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%B1%BB%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E4%B8%8E%E4%BF%9D%E5%AD%98"><span class="toc-number">5.2.</span> <span class="toc-text">文件系统类数据读取与保存</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS"><span class="toc-number">5.2.1.</span> <span class="toc-text">HDFS</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MYSQL%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5"><span class="toc-number">5.2.2.</span> <span class="toc-text">MYSQL数据库连接</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96-3"><span class="toc-number">5.2.2.1.</span> <span class="toc-text">读取</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%99"><span class="toc-number">5.2.2.2.</span> <span class="toc-text">写</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HBase"><span class="toc-number">5.2.3.</span> <span class="toc-text">HBase</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="toc-number">5.2.3.1.</span> <span class="toc-text">读取数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E4%BF%9D%E5%AD%98"><span class="toc-number">5.2.3.2.</span> <span class="toc-text">数据保存</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">6.</span> <span class="toc-text">总结</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark%E4%B8%AD%E7%9A%84%E5%90%84%E4%B8%AA%E6%95%B0%E9%87%8F"><span class="toc-number">6.1.</span> <span class="toc-text">Spark中的各个数量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Executor"><span class="toc-number">6.1.1.</span> <span class="toc-text">Executor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#partition"><span class="toc-number">6.1.2.</span> <span class="toc-text">partition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Stage"><span class="toc-number">6.1.3.</span> <span class="toc-text">Stage</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Task"><span class="toc-number">6.1.4.</span> <span class="toc-text">Task</span></a></li></ol></li></ol></li></ol></div></div></div><div class="card-widget card-recent-post"><div class="card-content"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2020/12/23/Flink%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89%E5%92%8CWaterMark/" title="Flink的时间语义和watermark"><img src="https://flink.apache.org/img/flink-header-logo.svg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Flink的时间语义和watermark"/></a><div class="content"><a class="title" href="/2020/12/23/Flink%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89%E5%92%8CWaterMark/" title="Flink的时间语义和watermark">Flink的时间语义和watermark</a><time datetime="2020-12-23T10:59:07.000Z" title="发表于 2020-12-23 18:59:07">2020-12-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/17/Flink-API/" title="Flink Api学习"><img src="https://flink.apache.org/img/flink-header-logo.svg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Flink Api学习"/></a><div class="content"><a class="title" href="/2020/12/17/Flink-API/" title="Flink Api学习">Flink Api学习</a><time datetime="2020-12-17T12:36:28.000Z" title="发表于 2020-12-17 20:36:28">2020-12-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/17/ready/" title="ready!"><img src="https://images.pexels.com/photos/924824/pexels-photo-924824.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ready!"/></a><div class="content"><a class="title" href="/2020/12/17/ready/" title="ready!">ready!</a><time datetime="2020-12-16T16:02:06.000Z" title="发表于 2020-12-17 00:02:06">2020-12-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/16/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="Flink学习笔记"><img src="https://flink.apache.org/img/flink-header-logo.svg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Flink学习笔记"/></a><div class="content"><a class="title" href="/2020/12/16/Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="Flink学习笔记">Flink学习笔记</a><time datetime="2020-12-16T12:29:56.000Z" title="发表于 2020-12-16 20:29:56">2020-12-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/06/k8s%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7%E3%80%81%E9%AB%98%E5%8F%AF%E7%94%A8%E4%BB%A5%E5%8F%8A%E9%83%A8%E7%BD%B2%E8%87%AA%E5%BB%BAJava%E9%A1%B9%E7%9B%AE/" title="k8s集群监控、高可用以及部署自建Java项目"><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/下载.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="k8s集群监控、高可用以及部署自建Java项目"/></a><div class="content"><a class="title" href="/2020/12/06/k8s%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7%E3%80%81%E9%AB%98%E5%8F%AF%E7%94%A8%E4%BB%A5%E5%8F%8A%E9%83%A8%E7%BD%B2%E8%87%AA%E5%BB%BAJava%E9%A1%B9%E7%9B%AE/" title="k8s集群监控、高可用以及部署自建Java项目">k8s集群监控、高可用以及部署自建Java项目</a><time datetime="2020-12-06T01:26:41.000Z" title="发表于 2020-12-06 09:26:41">2020-12-06</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Xiangjie</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script></div></body></html>