<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>spark学习笔记-SparkStreaming | zxj</title><meta name="keywords" content="Spark"><meta name="author" content="Xiangjie"><meta name="copyright" content="Xiangjie"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="Spark Streaming概述Spark Streaming是什么Spark Streaming用于流式数据的处理。Spark Streaming支持的数据输入源很多，例如：Kafka、Flume、Twitter、ZeroMQ和简单的TCP套接字等等。数据输入后可以用Spark的高度抽象原语如：map、reduce、join、window等进行运算。而结果也能保存在很多地方，如HDFS，数据库">
<meta property="og:type" content="article">
<meta property="og:title" content="spark学习笔记-SparkStreaming">
<meta property="og:url" content="https://awslzhang.top/2020/08/26/spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-SparkStreaming/index.html">
<meta property="og:site_name" content="zxj">
<meta property="og:description" content="Spark Streaming概述Spark Streaming是什么Spark Streaming用于流式数据的处理。Spark Streaming支持的数据输入源很多，例如：Kafka、Flume、Twitter、ZeroMQ和简单的TCP套接字等等。数据输入后可以用Spark的高度抽象原语如：map、reduce、join、window等进行运算。而结果也能保存在很多地方，如HDFS，数据库">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://spark.apache.org/images/spark-logo-trademark.png">
<meta property="article:published_time" content="2020-08-26T14:09:19.000Z">
<meta property="article:modified_time" content="2021-01-01T05:50:00.043Z">
<meta property="article:author" content="Xiangjie">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://spark.apache.org/images/spark-logo-trademark.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://awslzhang.top/2020/08/26/spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-SparkStreaming/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: Xiangjie","link":"链接: ","source":"来源: zxj","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}

// https://stackoverflow.com/questions/16839698/jquery-getscript-alternative-in-native-javascript
const getScript = url => new Promise((resolve, reject) => {
  const script = document.createElement('script')
  script.src = url
  script.async = true
  script.onerror = reject
  script.onload = script.onreadystatechange = function() {
    const loadState = this.readyState
    if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
    script.onload = script.onreadystatechange = null
    resolve()
  }
  document.head.appendChild(script)
})</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2021-01-01 13:50:00'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
   if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }
  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified
    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }const asideStatus = saveToLocal.get('aside-status')
if (asideStatus !== undefined) {
   if (asideStatus === 'hide') {
     document.documentElement.classList.add('hide-aside')
   } else {
     document.documentElement.classList.remove('hide-aside')
   }
}})()</script><meta name="generator" content="Hexo 5.0.2"><link rel="alternate" href="/atom.xml" title="zxj" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/null" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">85</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">43</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">25</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 目录</span></a></div></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://spark.apache.org/images/spark-logo-trademark.png)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">zxj</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 目录</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">spark学习笔记-SparkStreaming</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-08-26T14:09:19.000Z" title="发表于 2020-08-26 22:09:19">2020-08-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-01-01T05:50:00.043Z" title="更新于 2021-01-01 13:50:00">2021-01-01</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Spark/">Spark</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>14分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Spark-Streaming概述"><a href="#Spark-Streaming概述" class="headerlink" title="Spark Streaming概述"></a>Spark Streaming概述</h1><h2 id="Spark-Streaming是什么"><a href="#Spark-Streaming是什么" class="headerlink" title="Spark Streaming是什么"></a>Spark Streaming是什么</h2><p>Spark Streaming用于流式数据的处理。Spark Streaming支持的数据输入源很多，<font color="red">例如：Kafka、Flume、Twitter、ZeroMQ和简单的TCP套接字等等</font>。数据输入后可以用Spark的高度抽象原语如：<strong>map、reduce、join、window等进行运算</strong>。而结果也能保存在很多地方，<strong>如HDFS，数据库</strong>等。</p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200828193447365.png" alt="image-20200828193447365"></p>
<p>和Spark基于RDD的概念很相似，Spark Streaming使用<font color="red">离散化流(discretized stream)</font>作为抽象表示，叫作DStream。DStream 是随时间推移而收到的数据的序列。在内部，<strong>每个时间区间收到的数据都作为 RDD 存在</strong>，而DStream是由这些RDD所组成的序列(因此得名“<strong>离散化</strong>”)。</p>
<p><font color="red"><strong>注意</strong>：离散化的反义词就是连续，这证明SparkStreaming并不是真正的实时处理(来一条计算一条)，而是每次计算小批量的数据，小批量的数据值得就是一定采集周期之内的数据。</font></p>
<h1 id="DStream入门"><a href="#DStream入门" class="headerlink" title="DStream入门"></a>DStream入门</h1><p><code>DStream</code>是SparkStreaming计算的抽象。例如：Spark Core计算中的RDD，SparkSQL中的DS、DF。</p>
<h2 id="无状态的WordCount案例实操"><a href="#无状态的WordCount案例实操" class="headerlink" title="无状态的WordCount案例实操"></a>无状态的WordCount案例实操</h2><p>需求：使用netcat工具向9999端口不断的发送数据，通过SparkStreaming读取端口数据并统计不同单词出现的次数</p>
<p><strong>pom</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>scala</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span></span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.创建SparkConf并设置App名称</span></span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WC&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.初始化SparkStreamingContext</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.通过监控端口创建DStream，读进来的数据为一行行</span></span><br><span class="line">    <span class="keyword">val</span> lineStreams: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">&quot;192.168.0.201&quot;</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将每一行数据做切分，形成一个个单词</span></span><br><span class="line">    <span class="keyword">val</span> wordStreams: <span class="type">DStream</span>[<span class="type">String</span>] = lineStreams.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将单词映射成元组（word,1）</span></span><br><span class="line">    <span class="keyword">val</span> wordAndOneStreams: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordStreams.map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将相同的单词次数做统计</span></span><br><span class="line">    <span class="keyword">val</span> wordAndCountStreams: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordAndOneStreams.reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//打印</span></span><br><span class="line">    wordAndCountStreams.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//启动SparkStreamingContext的收集器，收集器必须一直开启，才能收集到数据</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">// 主程序等待收集器关闭，然后主程序关闭</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200828195052616.png" alt="image-20200828195052616"></p>
<p><font color="red">注意：如果程序运行时，log日志太多，可以将spark conf目录下的log4j文件里面的日志级别改成WARN。并加入到项目中</font></p>
<hr>
<p><strong>程序解析</strong></p>
<p><code>Discretized Stream</code>是Spark Streaming的基础抽象，代表持续性的数据流和经过各种Spark原语操作后的结果数据流。在内部实现上，<strong>DStream是一系列连续的RDD来表示</strong>。每个RDD含有一段时间间隔内的数据，如下图：</p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200828195323123.png" alt="image-20200828195323123"></p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200828195402058.png" alt="image-20200828195402058"></p>
<h1 id="DStream创建"><a href="#DStream创建" class="headerlink" title="DStream创建"></a>DStream创建</h1><p>Spark Streaming原生支持一些不同的数据源。一些“核心”数据源已经被打包到<code>Spark Streaming </code>的 Maven 工件中，而其他的一些则可以通过<code>spark-streaming-kafka</code>等附加工件获取。每个接收器都以 Spark 执行器程序中<font color="red"><strong>一个长期运行的任务的形式运行，因此会占据分配给应用的 CPU 核心。</strong>此外，我们还需要有可用的 CPU 核心来处理数据。</font>这意味着如果要运行多个接收器，<strong>就必须至少有和接收器数目相同的核心数</strong>，还要加上用来完成计算所需要的核心数。</p>
<p><font color="bule">例如，如果我们想要在流计算应用中运行 10 个接收器，那么至少需要为应用分配 11 个 CPU 核心。所以如果在本地模式运行，**不要使用local[1]**。</font>因为1个核心无法同时满足接收器和计算的任务。</p>
<h2 id="文件数据源"><a href="#文件数据源" class="headerlink" title="文件数据源"></a>文件数据源</h2><h3 id="用法及说明"><a href="#用法及说明" class="headerlink" title="用法及说明"></a>用法及说明</h3><p>文件数据流：能够读取所有HDFS API兼容的文件系统文件，通过fileStream方法进行读取，Spark Streaming 将会监控 dataDirectory 目录并不断处理移动进来的文件，。<font color="red"><strong>记住目前不支持嵌套目录</strong></font></p>
<p>一般不会使用，因为FLume是一套处理文件日志的收集系统，用它会好，但是它又没有Kafka好，因为Flume是推数据，不考虑计算节点的处理能力，有可能造成计算节点中数据的堆积，而Kafka不会，是计算节点主动读取Kafka数据，计算能力多大，读取多少数据。</p>
<p><code>streamingContext.textFileStream(dataDirectory)</code></p>
<p><font color="red"><strong>注意事项</strong></font></p>
<ol>
<li>文件需要有相同的数据格式；</li>
<li>文件进入 <code>dataDirectory</code>的方式需要通过移动或者重命名来实现；</li>
<li>一旦文件移动进目录，则不能再修改，即便修改了也不会读取新数据；</li>
</ol>
<h2 id="自定义数据源"><a href="#自定义数据源" class="headerlink" title="自定义数据源"></a>自定义数据源</h2><h3 id="用法及说明-1"><a href="#用法及说明-1" class="headerlink" title="用法及说明"></a>用法及说明</h3><p>需要继承<code>Receiver</code>，并实现<code>onStart</code>、<code>onStop</code>方法来自定义数据源采集。</p>
<h3 id="案例实操"><a href="#案例实操" class="headerlink" title="案例实操"></a>案例实操</h3><p>我们已上面的读取某个套接字的数据的数据源为例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.&#123;<span class="type">BufferedReader</span>, <span class="type">InputStreamReader</span>&#125;</span><br><span class="line"><span class="keyword">import</span> java.net.<span class="type">Socket</span></span><br><span class="line"><span class="keyword">import</span> java.nio.charset.<span class="type">StandardCharsets</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.<span class="type">StorageLevel</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.receiver.<span class="type">Receiver</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomerReceiver</span>(<span class="params">host: <span class="type">String</span>, port: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Receiver</span>[<span class="type">String</span>](<span class="params"><span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span></span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//最初启动的时候，调用该方法，作用为：读数据并将数据发送给Spark</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Thread</span>(<span class="string">&quot;Socket Receiver&quot;</span>) &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</span><br><span class="line">        receive()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;.start()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//读数据并将数据发送给Spark</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建一个Socket</span></span><br><span class="line">    <span class="keyword">var</span> socket: <span class="type">Socket</span> = <span class="keyword">new</span> <span class="type">Socket</span>(host, port)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//定义一个变量，用来接收端口传过来的数据</span></span><br><span class="line">    <span class="keyword">var</span> input: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建一个BufferedReader用于读取端口传来的数据</span></span><br><span class="line">    <span class="keyword">val</span> reader = <span class="keyword">new</span> <span class="type">BufferedReader</span>(<span class="keyword">new</span> <span class="type">InputStreamReader</span>(socket.getInputStream, <span class="type">StandardCharsets</span>.<span class="type">UTF_8</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//读取数据</span></span><br><span class="line">    input = reader.readLine()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//当receiver没有关闭并且输入数据不为空，则循环发送数据给Spark</span></span><br><span class="line">    <span class="keyword">while</span> (!isStopped() &amp;&amp; input != <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="comment">// 将读到的数据加入数据源中存储</span></span><br><span class="line">      store(input)</span><br><span class="line">      input = reader.readLine()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//跳出循环则关闭资源</span></span><br><span class="line">    reader.close()</span><br><span class="line">    socket.close()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//重启任务</span></span><br><span class="line">    restart(<span class="string">&quot;restart&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStop</span></span>(): <span class="type">Unit</span> = &#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>使用自定义的数据源采集数据</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.<span class="type">DStream</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FileStream</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.初始化Spark配置信息</span></span><br><span class="line"><span class="type">Val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">.setAppName(<span class="string">&quot;StreamWordCount&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.初始化SparkStreamingContext</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//3.创建自定义receiver的Streaming</span></span><br><span class="line"><span class="keyword">val</span> lineStream = ssc.receiverStream(<span class="keyword">new</span> <span class="type">CustomerReceiver</span>(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">9999</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.将每一行数据做切分，形成一个个单词</span></span><br><span class="line">    <span class="keyword">val</span> wordStreams = lineStream.flatMap(_.split(<span class="string">&quot;\t&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//5.将单词映射成元组（word,1）</span></span><br><span class="line">    <span class="keyword">val</span> wordAndOneStreams = wordStreams.map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//6.将相同的单词次数做统计</span></span><br><span class="line">    <span class="keyword">val</span> wordAndCountStreams] = wordAndOneStreams.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//7.打印</span></span><br><span class="line">    wordAndCountStreams.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//8.启动SparkStreamingContext</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Kafka数据源🔺"><a href="#Kafka数据源🔺" class="headerlink" title="Kafka数据源🔺"></a>Kafka数据源🔺</h2><h3 id="用法及说明-2"><a href="#用法及说明-2" class="headerlink" title="用法及说明"></a>用法及说明</h3><p><font color="red">在工程中需要引入 Maven 工件· spark- streaming-kafka_2.10 ·来使用它</font>。包内提供的 <code>KafkaUtils </code>对象可以在 <code>StreamingContext</code> 和<code>JavaStreamingContext</code>中以你的 Kafka 消息创建出 DStream。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-8_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>由于 <code>KafkaUtils</code> 可以订阅多个主题，因此它创建出的 DStream 由成对的主题和消息组成。要创建出一个流数据，需要使用 ：</p>
<ol>
<li><code>StreamingContext </code>实例</li>
<li>一个由逗号隔开的<code>ZooKeeper</code>主机列表字符串</li>
<li>消费者组的名字(唯一名字)</li>
<li>以及一个从主题到针对这个主题的接收器线程数的映射表来调用<code>createStream()</code>方法。</li>
</ol>
<p>因为Kafka的topic有分区数的概念，所有分区的数据合起来便是整个topic的数据，而topic的分区至多被一个消费者消费<font color="red">，所以启动的消费者的线程应该&lt;=topic的分区数。</font></p>
<p><code>// Map of (topic_name to numPartitions) to consume. Each partition is consumed in its own thread   </code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment">//3.通过监控端口创建DStream，读进来的数据为一行行</span></span><br><span class="line"><span class="comment">//    lineStreams = k-v  value才是kafka输入的数据</span></span><br><span class="line">    <span class="keyword">val</span> lineStreams: <span class="type">ReceiverInputDStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">KafkaUtils</span>.createStream(</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="string">&quot;192.168.0.201:2181&quot;</span>,</span><br><span class="line">      <span class="string">&quot;group1&quot;</span>,</span><br><span class="line">      <span class="type">Map</span>((<span class="string">&quot;myTopic&quot;</span>, <span class="number">3</span>))</span><br><span class="line">    )</span><br><span class="line"><span class="comment">// Map of (topic_name to numPartitions) to consume. Each partition is consumed in its own thread              </span></span><br></pre></td></tr></table></figure>

<h3 id="WC无状态的案例实操"><a href="#WC无状态的案例实操" class="headerlink" title="WC无状态的案例实操"></a>WC无状态的案例实操</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Kafka</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//1.创建SparkConf并设置App名称</span></span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WC&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.初始化SparkStreamingContext</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.通过监控端口创建DStream，读进来的数据为一行行</span></span><br><span class="line"><span class="comment">//    lineStreams = k-v  value才是kafka输入的数据</span></span><br><span class="line">    <span class="keyword">val</span> lineStreams: <span class="type">ReceiverInputDStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">KafkaUtils</span>.createStream(</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="string">&quot;192.168.0.201:2181&quot;</span>,</span><br><span class="line">      <span class="string">&quot;group1&quot;</span>,</span><br><span class="line">      <span class="type">Map</span>((<span class="string">&quot;myTopic&quot;</span>, <span class="number">3</span>))</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将每一行数据做切分，形成一个个单词</span></span><br><span class="line">    <span class="keyword">val</span> wordStreams: <span class="type">DStream</span>[<span class="type">String</span>] = lineStreams.flatMap(_._2.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将单词映射成元组（word,1）</span></span><br><span class="line">    <span class="keyword">val</span> wordAndOneStreams: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordStreams.map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将相同的单词次数做统计</span></span><br><span class="line">    <span class="keyword">val</span> wordAndCountStreams: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordAndOneStreams.reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//打印</span></span><br><span class="line">    wordAndCountStreams.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//启动SparkStreamingContext的收集器，收集器必须一直开启，才能收集到数据</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">// 主程序等待收集器关闭，然后主程序关闭</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong><code>Kafka</code>的操作</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop201 kafka]$ bin/kafka-topics.sh --zookeeper hadoop201:2181 --create --replication-factor 3 --partitions 3 --topic myTopic</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop201 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop202:9092 --topic myTopic</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>idea程序执行</strong></p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200828203819133.png" alt="image-20200828203819133"></p>
<p><font color="red"><strong>注意</strong>：上面所有的程序都是无状态的计算，即每个采集周期之间的数据互不影响，但这是不合理的，流式处理应该汇总所有的数据进行统计计算。这称为<strong>有状态的计算</strong>，下面开始有状态的计算</font></p>
<h1 id="DStream的转换"><a href="#DStream的转换" class="headerlink" title="DStream的转换"></a>DStream的转换</h1><h2 id="无状态的转换"><a href="#无状态的转换" class="headerlink" title="无状态的转换"></a>无状态的转换</h2><p>无状态转化操作就是把简单的RDD转化操作应用到每个批次上，也就是转化DStream中的每一个RDD。部分无状态转化操作列在了下表中。</p>
<p><font color="red"><strong>注意</strong>，针对键值对的DStream转化操作(比如 reduceByKey())要添加import StreamingContext._才能在Scala中使用。</font></p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200829144132973.png" alt="image-20200829144132973"></p>
<p>需要记住的是，尽管这些函数看起来像作用在整个流上一样，但事实上每个DStream在内部是由许多RDD(批次)组成，且无状态转化操作是分别<strong>应用到每个RDD上的</strong>。<strong>例如，reduceByKey()会归约每个时间区间中的数据，<font color="red">但不会归约不同区间之间的数据</font>。</strong> </p>
<p>举个例子，在之前的wordcount程序中，我们只会统计5秒内接收到的数据的单词个数，而不会累加。 </p>
<h2 id="有状态的转换🔺"><a href="#有状态的转换🔺" class="headerlink" title="有状态的转换🔺"></a>有状态的转换🔺</h2><h3 id="UpdateStateByKey"><a href="#UpdateStateByKey" class="headerlink" title="UpdateStateByKey"></a>UpdateStateByKey</h3><p>UpdateStateByKey原语用于记录历史记录，<font color="red">有时，我们需要在 DStream 中跨批次维护状态(例如流计算中累加wordcount)。</font>针对这种情况，updateStateByKey() 为我们提供了对一个状态变量的访问，<font color="red"><strong>用于键值对形式的 DStream。</strong></font></p>
<p>updateStateByKey() 的结果会是一个新的 DStream，其内部的 RDD 序列是由每个时间区间对应的(键，状态)对组成的。</p>
<p>updateStateByKey操作使得我们可以在用新信息进行更新时保持任意的状态。为使用这个功能，你需要： </p>
<ol>
<li>指定检查点目录，因为每次计算时会用到之前的数据，而不断累加的之前的数据是不能放入内存的，所以要设置目录存放这些数据。</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateStateByKey</span></span>[<span class="type">S</span>: <span class="type">ClassTag</span>](updateFunc: (<span class="type">Seq</span>[<span class="type">V</span>], <span class="type">Option</span>[<span class="type">S</span>]) =&gt; <span class="type">Option</span>[<span class="type">S</span>])</span><br></pre></td></tr></table></figure>

<ol>
<li>可以看到泛型和参数，其中泛型是之前的数据的此KEY的Value的类型</li>
<li>参数中：Seq序列是最新的采集周期的数据相同key的集合</li>
<li>Option{S}是之前计算好的此KEY对应的最后Value，为什么是Option，如果是第一次计算，则他是空</li>
</ol>
<p><strong>改良的WC</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Kafka</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//1.创建SparkConf并设置App名称</span></span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WC&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.初始化SparkStreamingContext  Seconds(5)采集周期，5S生成一个DStream即RDD</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    ssc.sparkContext.setCheckpointDir(<span class="string">&quot;test&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.通过监控端口创建DStream，读进来的数据为一行行</span></span><br><span class="line">    <span class="comment">//    lineStreams = k-v  value才是kafka输入的数据</span></span><br><span class="line">    <span class="keyword">val</span> lineStreams: <span class="type">ReceiverInputDStream</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">KafkaUtils</span>.createStream(</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="string">&quot;192.168.0.201:2181&quot;</span>,</span><br><span class="line">      <span class="string">&quot;group1&quot;</span>,</span><br><span class="line">      <span class="type">Map</span>((<span class="string">&quot;myTopic&quot;</span>, <span class="number">3</span>))</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将每一行数据做切分，形成一个个单词</span></span><br><span class="line">    <span class="keyword">val</span> wordStreams: <span class="type">DStream</span>[<span class="type">String</span>] = lineStreams.flatMap(_._2.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将单词映射成元组（word,1）</span></span><br><span class="line">    <span class="keyword">val</span> wordAndOneStreams: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordStreams.map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//    def updateStateByKey[S: ClassTag](updateFunc: (Seq[V], Option[S]) =&gt; Option[S])</span></span><br><span class="line">    <span class="comment">//  可以看到泛型和参数，其中泛型是之前的数据的此KEY的Value的类型，其中</span></span><br><span class="line">    <span class="comment">// 参数中：Seq序列是最新的采集周期的数据相同key的集合，Option&#123;S&#125;是之前计算好的此KEY对应的最后Value，为什么是Option，如果是第一次计算，则他是空</span></span><br><span class="line">    <span class="comment">// 定义更新状态方法，参数values为当前批次单词频度，state为以往批次单词频度</span></span><br><span class="line">    <span class="keyword">val</span> value: <span class="type">DStream</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordAndOneStreams.updateStateByKey &#123;</span><br><span class="line">      <span class="keyword">case</span> (seq, buffer) =&gt;</span><br><span class="line">        <span class="type">Some</span>(seq.sum + buffer.getOrElse(<span class="number">0</span>))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    value.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//启动SparkStreamingContext的收集器，收集器必须一直开启，才能收集到数据</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">// 主程序等待收集器关闭，然后主程序关闭</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200829152850299.png" alt="image-20200829152850299"></p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200829153242722.png" alt="image-20200829153242722"></p>
<p>可以发现数值一直在变大</p>
<h3 id="Window-Operations"><a href="#Window-Operations" class="headerlink" title="Window Operations"></a>Window Operations</h3><p><code>Window Operations</code>可以设<strong>置窗口的大小</strong>和<strong>滑动窗口的间隔</strong>来动态的获取当前Steaming的允许状态。基于窗口的操作会在一个比 StreamingContext 的批次间隔更长的时间范围内，通过整合多个批次的结果，计算出整个窗口的结果。</p>
<p><strong>其中：</strong></p>
<ol>
<li>窗口的大小必须是采集周期的整数倍</li>
<li>滑动窗口的间隔必须是采集周期的整数倍</li>
</ol>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200829153601198.png" alt="image-20200829153601198"></p>
<p>例如：上图窗口大小就是采集周期的三倍；滑动窗口的间隔则是采集周期的两倍</p>
<p><font color="red"><strong>注意：所有基于窗口的操作都需要两个参数，分别为窗口时长以及滑动步长，两者都必须是 StreamContext 的批次间隔的整数倍</strong></font></p>
<p><code>window(windowLength, slideInterval)</code>: 基于对源DStream窗化的批次进行计算返回一个新的Dstream</p>
<h2 id="其他重要操作🔺"><a href="#其他重要操作🔺" class="headerlink" title="其他重要操作🔺"></a>其他重要操作🔺</h2><h3 id="Transform🔺"><a href="#Transform🔺" class="headerlink" title="Transform🔺"></a>Transform🔺</h3><p>Transform原语允许DStream上执行任意的RDD-to-RDD函数。即使这些函数并没有在DStream的API中暴露出来，通过该函数可以方便的扩展Spark API。<strong>该函数每一批次调度一次</strong>。其实也就是对DStream中的RDD应用转换。</p>
<p><img src="https://zxj-typora.oss-cn-shanghai.aliyuncs.com/img/image-20200829154426732.png" alt="image-20200829154426732"></p>
<h1 id="DStream输出"><a href="#DStream输出" class="headerlink" title="DStream输出"></a>DStream输出</h1><p>输出操作指定了对流数据经转化操作得到的数据所要执行的操作(例如把结果推入外部数据库或输出到屏幕上)。与RDD中的惰性求值类似，如果一个DStream及其派生出的DStream都没有被执行输出操作，那么这些DStream就都不会被求值。 <font color="red">如果StreamingContext中没有设定输出操作，整个context就都不会启动。</font></p>
<p>输出操作如下：</p>
<ol>
<li>print()：在运行流程序的驱动结点上打印DStream中每一批次数据的最开始10个元素。这用于开发和调试。在Python API中，同样的操作叫print()。</li>
<li>saveAsTextFiles(prefix, [suffix])：以text文件形式存储这个DStream的内容。每一批次的存储文件名基于参数中的prefix和suffix。”prefix-Time_IN_MS[.suffix]”. </li>
<li>saveAsObjectFiles(prefix, [suffix])：以Java对象序列化的方式将Stream中的数据保存为 SequenceFiles . 每一批次的存储文件名基于参数中的为”prefix-TIME_IN_MS[.suffix]”. Python中目前不可用。</li>
<li>saveAsHadoopFiles(prefix, [suffix])：将Stream中的数据保存为 Hadoop files. 每一批次的存储文件名基于参数中的为”prefix-TIME_IN_MS[.suffix]”。<br>Python API Python中目前不可用。</li>
<li>foreachRDD(func)：<strong>这是最通用的输出操作</strong>，即将函数 func 用于产生于 stream的每一个RDD。其中参数传入的函数func应该实现将每一个RDD中数据推送到外部系统，如将RDD存入文件或者通过网络将其写入数据库。注意：函数func在运行流应用的驱动中被执行，同时其中一般函数RDD操作从而强制其对于流RDD的运算。</li>
</ol>
<p><font color="red"></font></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Xiangjie</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://awslzhang.top/2020/08/26/spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-SparkStreaming/">https://awslzhang.top/2020/08/26/spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-SparkStreaming/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://awslzhang.top" target="_blank">zxj</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><div class="post_share"><div class="social-share" data-image="https://spark.apache.org/images/spark-logo-trademark.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/08/29/spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-SparkCore%E8%A7%A3%E6%9E%90/"><img class="prev-cover" src="https://spark.apache.org/images/spark-logo-trademark.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">spark学习笔记-SparkCore解析</div></div></a></div><div class="next-post pull-right"><a href="/2020/08/24/spark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-SparkSQL/"><img class="next-cover" src="https://spark.apache.org/images/spark-logo-trademark.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">spark学习笔记-SparkSQL</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/08/23/spark学习笔记-广播变量及累加器/" title="spark学习笔记-广播变量及累加器"><img class="cover" src="https://spark.apache.org/images/spark-logo-trademark.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-23</div><div class="title">spark学习笔记-广播变量及累加器</div></div></a></div><div><a href="/2020/08/24/spark学习笔记-SparkSQL/" title="spark学习笔记-SparkSQL"><img class="cover" src="https://spark.apache.org/images/spark-logo-trademark.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-24</div><div class="title">spark学习笔记-SparkSQL</div></div></a></div><div><a href="/2020/08/19/spark学习笔记-简单了解/" title="spark学习笔记-简单了解"><img class="cover" src="https://spark.apache.org/images/spark-logo-trademark.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-19</div><div class="title">spark学习笔记-简单了解</div></div></a></div><div><a href="/2020/08/29/spark学习笔记-SparkCore解析/" title="spark学习笔记-SparkCore解析"><img class="cover" src="https://spark.apache.org/images/spark-logo-trademark.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-29</div><div class="title">spark学习笔记-SparkCore解析</div></div></a></div><div><a href="/2020/08/22/spark学习笔记-RDD/" title="spark学习笔记-RDD"><img class="cover" src="https://spark.apache.org/images/spark-logo-trademark.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-22</div><div class="title">spark学习笔记-RDD</div></div></a></div></div></div></div><div class="aside_content" id="aside_content"><div class="card-widget card-info"><div class="card-content"><div class="card-info-avatar is-center"><img class="avatar-img" src="/null" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Xiangjie</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">85</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">43</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">25</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/XiangJie-Zhang" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:qluzxj@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div></div><div class="card-widget card-announcement"><div class="card-content"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="card-content"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark-Streaming%E6%A6%82%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">Spark Streaming概述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-Streaming%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">1.1.</span> <span class="toc-text">Spark Streaming是什么</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#DStream%E5%85%A5%E9%97%A8"><span class="toc-number">2.</span> <span class="toc-text">DStream入门</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%A0%E7%8A%B6%E6%80%81%E7%9A%84WordCount%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="toc-number">2.1.</span> <span class="toc-text">无状态的WordCount案例实操</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#DStream%E5%88%9B%E5%BB%BA"><span class="toc-number">3.</span> <span class="toc-text">DStream创建</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="toc-number">3.1.</span> <span class="toc-text">文件数据源</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%A8%E6%B3%95%E5%8F%8A%E8%AF%B4%E6%98%8E"><span class="toc-number">3.1.1.</span> <span class="toc-text">用法及说明</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="toc-number">3.2.</span> <span class="toc-text">自定义数据源</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%A8%E6%B3%95%E5%8F%8A%E8%AF%B4%E6%98%8E-1"><span class="toc-number">3.2.1.</span> <span class="toc-text">用法及说明</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="toc-number">3.2.2.</span> <span class="toc-text">案例实操</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka%E6%95%B0%E6%8D%AE%E6%BA%90%F0%9F%94%BA"><span class="toc-number">3.3.</span> <span class="toc-text">Kafka数据源🔺</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%A8%E6%B3%95%E5%8F%8A%E8%AF%B4%E6%98%8E-2"><span class="toc-number">3.3.1.</span> <span class="toc-text">用法及说明</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#WC%E6%97%A0%E7%8A%B6%E6%80%81%E7%9A%84%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="toc-number">3.3.2.</span> <span class="toc-text">WC无状态的案例实操</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#DStream%E7%9A%84%E8%BD%AC%E6%8D%A2"><span class="toc-number">4.</span> <span class="toc-text">DStream的转换</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%A0%E7%8A%B6%E6%80%81%E7%9A%84%E8%BD%AC%E6%8D%A2"><span class="toc-number">4.1.</span> <span class="toc-text">无状态的转换</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%89%E7%8A%B6%E6%80%81%E7%9A%84%E8%BD%AC%E6%8D%A2%F0%9F%94%BA"><span class="toc-number">4.2.</span> <span class="toc-text">有状态的转换🔺</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#UpdateStateByKey"><span class="toc-number">4.2.1.</span> <span class="toc-text">UpdateStateByKey</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Window-Operations"><span class="toc-number">4.2.2.</span> <span class="toc-text">Window Operations</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E9%87%8D%E8%A6%81%E6%93%8D%E4%BD%9C%F0%9F%94%BA"><span class="toc-number">4.3.</span> <span class="toc-text">其他重要操作🔺</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Transform%F0%9F%94%BA"><span class="toc-number">4.3.1.</span> <span class="toc-text">Transform🔺</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#DStream%E8%BE%93%E5%87%BA"><span class="toc-number">5.</span> <span class="toc-text">DStream输出</span></a></li></ol></div></div></div><div class="card-widget card-recent-post"><div class="card-content"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/01/10/Api%E5%92%8CSQL/" title="Api和SQL"><img src="https://flink.apache.org/img/flink-header-logo.svg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Api和SQL"/></a><div class="content"><a class="title" href="/2021/01/10/Api%E5%92%8CSQL/" title="Api和SQL">Api和SQL</a><time datetime="2021-01-10T03:51:40.000Z" title="发表于 2021-01-10 11:51:40">2021-01-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/01/02/Flink%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86/" title="Flink状态编程和容错机制"><img src="https://flink.apache.org/img/flink-header-logo.svg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Flink状态编程和容错机制"/></a><div class="content"><a class="title" href="/2021/01/02/Flink%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86/" title="Flink状态编程和容错机制">Flink状态编程和容错机制</a><time datetime="2021-01-02T10:06:02.000Z" title="发表于 2021-01-02 18:06:02">2021-01-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/23/Flink%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89%E5%92%8CWaterMark/" title="Flink的时间语义和watermark"><img src="https://flink.apache.org/img/flink-header-logo.svg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Flink的时间语义和watermark"/></a><div class="content"><a class="title" href="/2020/12/23/Flink%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89%E5%92%8CWaterMark/" title="Flink的时间语义和watermark">Flink的时间语义和watermark</a><time datetime="2020-12-23T10:59:07.000Z" title="发表于 2020-12-23 18:59:07">2020-12-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/17/Flink-API/" title="Flink Api学习"><img src="https://flink.apache.org/img/flink-header-logo.svg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Flink Api学习"/></a><div class="content"><a class="title" href="/2020/12/17/Flink-API/" title="Flink Api学习">Flink Api学习</a><time datetime="2020-12-17T12:36:28.000Z" title="发表于 2020-12-17 20:36:28">2020-12-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2020/12/17/ready/" title="ready!"><img src="https://images.pexels.com/photos/924824/pexels-photo-924824.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ready!"/></a><div class="content"><a class="title" href="/2020/12/17/ready/" title="ready!">ready!</a><time datetime="2020-12-16T16:02:06.000Z" title="发表于 2020-12-17 00:02:06">2020-12-17</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Xiangjie</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script></div></body></html>